{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read files into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=\"/home/kirana/Documents/phd\"\n",
    "DATAPATH=\"/home/kirana/Documents/phd/data/aclImdb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai\n",
    "from fastai.text import *\n",
    "from fastai import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdbEr.txt  imdb.vocab  README  \u001b[0m\u001b[01;34mtest\u001b[0m/  \u001b[01;34mtrain\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls {DATAPATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeledBow.feat  \u001b[0m\u001b[01;34mpos\u001b[0m/    unsupBow.feat  urls_pos.txt\r\n",
      "\u001b[01;34mneg\u001b[0m/             \u001b[01;34munsup\u001b[0m/  urls_neg.txt   urls_unsup.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls {DATAPATH}/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeledBow.feat  \u001b[0m\u001b[01;34mneg\u001b[0m/  \u001b[01;34mpos\u001b[0m/  urls_neg.txt  urls_pos.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls {DATAPATH}/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subpaths=['train/pos','train/neg','train/unsup','test/pos','test/neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files_from_folder(DATAPATH,subpaths):\n",
    "    dflist=[]\n",
    "    for subpath in subpaths:\n",
    "        path=f'{DATAPATH}/{subpath}'\n",
    "        files=os.listdir(path)\n",
    "        files=[f for f in files if f.endswith('.txt')]\n",
    "        dstype=str.split(subpath,'/')[0]\n",
    "        label=str.split(subpath,'/')[1]\n",
    "        text=[open(f'{DATAPATH}/{subpath}/{f}','r').readlines()[0] for f in files]\n",
    "        df=pd.DataFrame({'text':text})\n",
    "        df['label']=label\n",
    "        df['dstype']=dstype\n",
    "        dflist.append(df) \n",
    "    df=pd.concat(dflist)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=read_files_from_folder(DATAPATH,subpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>dstype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and possibly closest to the Dickens story line...</td>\n",
       "      <td>pos</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A small pleasure in life is walking down the o...</td>\n",
       "      <td>pos</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Have just seen the Australian premiere of Show...</td>\n",
       "      <td>pos</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thankfully saw this on a plane to Singapore re...</td>\n",
       "      <td>pos</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The movie itself was ok for the kids. But I go...</td>\n",
       "      <td>pos</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label dstype\n",
       "0  and possibly closest to the Dickens story line...   pos  train\n",
       "1  A small pleasure in life is walking down the o...   pos  train\n",
       "2  Have just seen the Australian premiere of Show...   pos  train\n",
       "3  Thankfully saw this on a plane to Singapore re...   pos  train\n",
       "4  The movie itself was ok for the kids. But I go...   pos  train"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>label</th>\n",
       "      <th>neg</th>\n",
       "      <th>pos</th>\n",
       "      <th>unsup</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dstype</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>12500</td>\n",
       "      <td>12500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>12500</td>\n",
       "      <td>12500</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "label     neg    pos  unsup\n",
       "dstype                     \n",
       "test    12500  12500      0\n",
       "train   12500  12500  50000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(df['dstype'],df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "myle=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label']=myle.fit_transform(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    50000\n",
       "1    25000\n",
       "0    25000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=df.loc[df['dstype']=='test']\n",
    "df_train=df.loc[df['dstype']=='train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,df_valid=train_test_split(df_train,train_size=0.9,test_size=0.1,random_state=11,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100000, 3), (67500, 3), (7500, 3), (25000, 3))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape,df_train.shape,df_valid.shape,df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>dstype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and possibly closest to the Dickens story line...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A small pleasure in life is walking down the o...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Have just seen the Australian premiere of Show...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thankfully saw this on a plane to Singapore re...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The movie itself was ok for the kids. But I go...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label dstype\n",
       "0  and possibly closest to the Dickens story line...      1  train\n",
       "1  A small pleasure in life is walking down the o...      1  train\n",
       "2  Have just seen the Australian premiere of Show...      1  train\n",
       "3  Thankfully saw this on a plane to Singapore re...      1  train\n",
       "4  The movie itself was ok for the kids. But I go...      1  train"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df_train.loc[:,['label','text']]\n",
    "df_valid=df_valid.loc[:,['label','text']]\n",
    "df_test=df_test.loc[:,['label','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(f'{PATH}/inter/df_train.csv',index=False)\n",
    "df_valid.to_csv(f'{PATH}/inter/df_valid.csv',index=False)\n",
    "df_test.to_csv(f'{PATH}/inter/df_test.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Numericalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "re1 = re.compile(r'  +')\n",
    "\n",
    "def fixup(x):\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n",
    "        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))\n",
    "\n",
    "def get_texts(df, n_lbls=1):\n",
    "    labels = df.iloc[:, range(n_lbls)].values.astype(np.int64)\n",
    "\n",
    "    texts = f'\\n{BOS} {FLD} 1 ' + df.iloc[:,n_lbls].astype(str)\n",
    "    for i in range(n_lbls + 1, len(df.columns)):\n",
    "        texts += f' {FLD} {i - n_lbls} ' + df[i].astype(str)\n",
    "    texts = texts.apply(fixup).values.astype(str)\n",
    "\n",
    "    tokenizer = Tokenizer(n_cpus=10)\n",
    "    tokop=tokenizer.process_all(texts)\n",
    "    return tokop, list(labels)\n",
    "\n",
    "\n",
    "def get_all(df, n_lbls):\n",
    "    tok, labels = [], []\n",
    "    #import pdb\n",
    "    #pdb.set_trace()\n",
    "    for i, txt in enumerate(df):\n",
    "        tok_, labels_ = get_texts(txt, n_lbls)\n",
    "        tok += tok_\n",
    "        labels += labels_\n",
    "    return tok, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize=24000\n",
    "chunk_train=pd.read_csv(f'{PATH}/inter/df_train.csv',chunksize=chunksize)\n",
    "chunk_valid=pd.read_csv(f'{PATH}/inter/df_valid.csv',chunksize=chunksize)\n",
    "chunk_test=pd.read_csv(f'{PATH}/inter/df_test.csv',chunksize=chunksize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens, train_labels = get_all(chunk_train, 1)\n",
    "valid_tokens, valid_labels = get_all(chunk_valid, 1)\n",
    "test_tokens, test_labels = get_all(chunk_test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['words']=train_tokens\n",
    "df_valid['words']=valid_tokens\n",
    "df_test['words']=test_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('xxmaj', 1704081),\n",
       " ('the', 912744),\n",
       " ('.', 746692),\n",
       " (',', 745869),\n",
       " ('and', 444260),\n",
       " ('a', 440858),\n",
       " ('of', 396150),\n",
       " ('to', 366366),\n",
       " ('is', 297479),\n",
       " ('it', 258089),\n",
       " ('in', 254252),\n",
       " ('i', 231608),\n",
       " ('this', 203711),\n",
       " ('that', 197861),\n",
       " ('\"', 178689),\n",
       " (\"'s\", 167922),\n",
       " ('-', 143037),\n",
       " ('\\n \\n ', 137426),\n",
       " ('was', 136078),\n",
       " ('as', 125813),\n",
       " ('xxup', 121824),\n",
       " ('with', 120233),\n",
       " ('for', 120232),\n",
       " ('movie', 118739),\n",
       " ('but', 114132)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = Counter(p for o in train_tokens for p in o)\n",
    "freq.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab = 60000\n",
    "min_freq = 2\n",
    "\n",
    "itos = [o for o, c in freq.most_common(max_vocab) if c > min_freq]\n",
    "itos.insert(0, '_pad_')\n",
    "itos.insert(0, '_unk_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59972"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi = collections.defaultdict(lambda: 0, { v: k for k, v in enumerate(itos) })\n",
    "len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_lm = np.array([ [stoi[o] for o in p] for p in train_tokens ])\n",
    "val_lm = np.array([ [stoi[o] for o in p] for p in valid_tokens ])\n",
    "test_lm = np.array([ [stoi[o] for o in p] for p in test_tokens ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['tokens']=trn_lm\n",
    "df_valid['tokens']=val_lm\n",
    "df_test['tokens']=test_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump([df_train,df_valid,df_test,itos, train_tokens, valid_tokens, test_tokens, trn_lm, val_lm, test_lm],open(f'{PATH}/inter/dfs_tokens_fastai.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "[df_train,df_valid,df_test,itos, train_tokens, valid_tokens, test_tokens, trn_lm, val_lm, test_lm]=pickle.load(open(f'{PATH}/inter/dfs_tokens_fastai.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=20 # 52 - Jeremey, 20 - default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt= 35 #70 - Jeremey, 35 - default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_lm(tokens,bs):\n",
    "    import itertools\n",
    "    # Collapse into a single large array\n",
    "    tokens=np.asarray(list (itertools.chain(*tokens)))\n",
    "    # How many batches\n",
    "    n_batch=len(tokens)//bs\n",
    "    # Truncate to exclude the ones at the end\n",
    "    tokens=tokens[:bs*n_batch]\n",
    "    # Reshape\n",
    "    tokens=tokens.reshape(bs,-1)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20, 1031291), (20, 114129), (20, 372474))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens=create_data_lm(df_train['tokens'],bs)\n",
    "valid_tokens=create_data_lm(df_valid['tokens'],bs)\n",
    "test_tokens=create_data_lm(df_test['tokens'],bs)\n",
    "train_tokens.shape, valid_tokens.shape, test_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i):\n",
    "    seq_len = min(args.bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 35) (20, 35)\n"
     ]
    }
   ],
   "source": [
    "n_batch=train_tokens.shape[1]\n",
    "for i in range(0,n_batch,bptt):\n",
    "    seq_len=min(bptt,n_batch-1-i)\n",
    "    x=train_tokens[:,i:i+seq_len]\n",
    "    y=train_tokens[:,i+1:i+1+seq_len]\n",
    "    print (x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20, 35), (20, 35))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   41,    42,    43,    40, ...,  1033,     7,   189,    21],\n",
       "        [  103,   278,  3442, 43417, ...,     0,     2,     0,     6],\n",
       "        [    2,  1065,   732,   227, ...,    11,    73,    38,   576],\n",
       "        [    0,    29,    10,  2414, ...,    11,   817,  9572,     6],\n",
       "        ...,\n",
       "        [ 2626,  2008,   849,     4, ...,     6,     3, 21421,    59],\n",
       "        [    5,    35,    10,    59, ...,   182,   200,    61,     5],\n",
       "        [ 3863,  3557,   193,    18, ...,    23,     3,  1504,    28],\n",
       "        [   66,     2,     9,   894, ...,   399,   111,     9,    58]]),\n",
       " array([[   42,    43,    40,    13, ...,     7,   189,    21,    13],\n",
       "        [  278,  3442, 43417,     5, ...,     2,     0,     6,     2],\n",
       "        [ 1065,   732,   227,   166, ...,    73,    38,   576,  1104],\n",
       "        [   29,    10,  2414,     6, ...,   817,  9572,     6,  1800],\n",
       "        ...,\n",
       "        [ 2008,   849,     4,     2, ...,     3, 21421,    59,     3],\n",
       "        [   35,    10,    59,     9, ...,   200,    61,     5,     2],\n",
       "        [ 3557,   193,    18,   144, ...,     3,  1504,    28,    98],\n",
       "        [    2,     9,   894,    23, ...,   111,     9,    58,    79]]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20, 35), (20, 35))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1031291)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-trained AWD-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_AWD_LSTM='/home/kirana/Documents/phd/data/pre-trained/awd_lstm/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bwd_wt103_enc.h5  fwd_wt103_enc.h5  itos_wt103.pkl  \u001b[0m\u001b[01;34mwt103_60002\u001b[0m/\r\n",
      "bwd_wt103.h5      fwd_wt103.h5      \u001b[01;34mwt103_238642\u001b[0m/   \u001b[01;31mwt103_tiny.tgz\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "ls {PATH_AWD_LSTM}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts = torch.load(f'{PATH_AWD_LSTM}/fwd_wt103.h5', map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.encoder.weight',\n",
       "              tensor([[-1.2274e-01,  2.7886e-01, -3.8850e-01,  ..., -1.0404e-01,\n",
       "                        1.9580e-02,  1.8548e-01],\n",
       "                      [ 1.4854e-05, -2.3424e-05,  1.9693e-05,  ...,  2.1349e-05,\n",
       "                        2.1776e-05, -1.2394e-05],\n",
       "                      [ 1.8070e-01,  1.5874e+00, -1.1738e-01,  ..., -4.5935e-02,\n",
       "                       -8.1352e-02,  1.8054e-01],\n",
       "                      ...,\n",
       "                      [-1.8595e-03, -6.8529e-03,  1.6999e-03,  ...,  1.7039e-03,\n",
       "                        4.1632e-03, -1.3171e-03],\n",
       "                      [-2.3120e-03, -6.9001e-03,  1.8772e-03,  ...,  5.0309e-04,\n",
       "                        4.6596e-03, -2.5850e-03],\n",
       "                      [-2.2463e-03, -9.1512e-03,  1.3927e-03,  ...,  1.2296e-03,\n",
       "                        5.8085e-03, -1.8940e-03]])),\n",
       "             ('0.encoder_with_dropout.embed.weight',\n",
       "              tensor([[-1.2274e-01,  2.7886e-01, -3.8850e-01,  ..., -1.0404e-01,\n",
       "                        1.9580e-02,  1.8548e-01],\n",
       "                      [ 1.4854e-05, -2.3424e-05,  1.9693e-05,  ...,  2.1349e-05,\n",
       "                        2.1776e-05, -1.2394e-05],\n",
       "                      [ 1.8070e-01,  1.5874e+00, -1.1738e-01,  ..., -4.5935e-02,\n",
       "                       -8.1352e-02,  1.8054e-01],\n",
       "                      ...,\n",
       "                      [-1.8595e-03, -6.8529e-03,  1.6999e-03,  ...,  1.7039e-03,\n",
       "                        4.1632e-03, -1.3171e-03],\n",
       "                      [-2.3120e-03, -6.9001e-03,  1.8772e-03,  ...,  5.0309e-04,\n",
       "                        4.6596e-03, -2.5850e-03],\n",
       "                      [-2.2463e-03, -9.1512e-03,  1.3927e-03,  ...,  1.2296e-03,\n",
       "                        5.8085e-03, -1.8940e-03]])),\n",
       "             ('0.rnns.0.module.weight_ih_l0',\n",
       "              tensor([[-0.0812, -0.0811, -0.0937,  ..., -0.0259, -0.1403, -0.3247],\n",
       "                      [ 0.1154,  0.1142,  0.0938,  ..., -0.0711,  0.1669, -0.0387],\n",
       "                      [-0.0051,  0.1007,  0.2071,  ..., -0.0860, -0.0288, -0.0894],\n",
       "                      ...,\n",
       "                      [ 0.0055,  0.0157,  0.2990,  ...,  0.0616,  0.1159, -0.4737],\n",
       "                      [ 0.0181,  0.0426,  0.1130,  ...,  0.3529, -0.0114, -0.0125],\n",
       "                      [-0.0167, -0.1328,  0.1741,  ...,  0.0548, -0.0045,  0.1688]])),\n",
       "             ('0.rnns.0.module.bias_ih_l0',\n",
       "              tensor([ 0.1503, -0.4701, -0.1885,  ..., -0.5919, -0.2172, -0.1207])),\n",
       "             ('0.rnns.0.module.bias_hh_l0',\n",
       "              tensor([ 0.1503, -0.4701, -0.1885,  ..., -0.5919, -0.2172, -0.1207])),\n",
       "             ('0.rnns.0.module.weight_hh_l0_raw',\n",
       "              tensor([[-0.1013,  0.1786, -0.0528,  ...,  0.0741,  0.0306,  0.2467],\n",
       "                      [ 0.1780, -0.0853, -0.0243,  ..., -0.1129, -0.1310, -0.1498],\n",
       "                      [ 0.0661, -0.0496,  0.0921,  ...,  0.1829,  0.0533, -0.1525],\n",
       "                      ...,\n",
       "                      [-0.0322, -0.0704,  0.1653,  ...,  0.2142, -0.0558,  0.0315],\n",
       "                      [-0.1651, -0.0290,  0.1748,  ..., -0.0446,  0.5444,  0.0616],\n",
       "                      [ 0.0905, -0.1704, -0.0053,  ..., -0.0057,  0.2269,  0.0328]])),\n",
       "             ('0.rnns.1.module.weight_ih_l0',\n",
       "              tensor([[ 0.3307,  0.0385,  0.0860,  ...,  0.0685, -0.0444,  0.0539],\n",
       "                      [ 0.0720,  0.1607,  0.0562,  ...,  0.0276,  0.0613,  0.1632],\n",
       "                      [-0.1565, -0.1168,  0.1897,  ..., -0.0357,  0.0296,  0.0961],\n",
       "                      ...,\n",
       "                      [-0.0897, -0.1464, -0.0760,  ...,  0.0536,  0.0422, -0.0580],\n",
       "                      [ 0.1166, -0.1534, -0.1784,  ..., -0.0689,  0.2170,  0.1461],\n",
       "                      [-0.0413,  0.0689,  0.0581,  ..., -0.0640, -0.1703, -0.0945]])),\n",
       "             ('0.rnns.1.module.bias_ih_l0',\n",
       "              tensor([-0.8577, -0.6784, -0.7249,  ..., -0.6782,  0.0567, -0.5026])),\n",
       "             ('0.rnns.1.module.bias_hh_l0',\n",
       "              tensor([-0.8577, -0.6784, -0.7249,  ..., -0.6782,  0.0567, -0.5026])),\n",
       "             ('0.rnns.1.module.weight_hh_l0_raw',\n",
       "              tensor([[-0.0273, -0.2277,  0.0782,  ...,  0.1355, -0.1282,  0.1669],\n",
       "                      [ 0.1218,  0.0017, -0.0998,  ..., -0.2085, -0.0686, -0.1389],\n",
       "                      [-0.3878, -0.0498, -0.1748,  ..., -0.4014,  0.1986, -0.4400],\n",
       "                      ...,\n",
       "                      [-0.2097, -0.4298,  0.3551,  ...,  0.0316, -0.1198,  0.1266],\n",
       "                      [ 0.0037, -0.0223,  0.0032,  ..., -0.2672, -0.3093, -0.0361],\n",
       "                      [-0.0464,  0.1664, -0.1348,  ...,  0.1600, -0.1138,  0.0845]])),\n",
       "             ('0.rnns.2.module.weight_ih_l0',\n",
       "              tensor([[-0.0741,  0.0447, -0.0744,  ..., -0.0419,  0.1600, -0.0553],\n",
       "                      [ 0.0270,  0.0118,  0.0449,  ...,  0.1165, -0.1080, -0.0681],\n",
       "                      [-0.1023, -0.1662, -0.0229,  ...,  0.1652, -0.1070,  0.0970],\n",
       "                      ...,\n",
       "                      [-0.0989, -0.4425, -0.0343,  ..., -0.1434,  0.5851, -0.0291],\n",
       "                      [ 0.0802, -0.1067,  0.2789,  ..., -0.0916, -0.2240,  0.1020],\n",
       "                      [-0.4078,  0.7220,  0.1142,  ...,  0.5287,  0.2035, -0.1811]])),\n",
       "             ('0.rnns.2.module.bias_ih_l0',\n",
       "              tensor([-0.3681, -0.9079, -0.1998,  ...,  0.8533,  0.3202,  1.2172])),\n",
       "             ('0.rnns.2.module.bias_hh_l0',\n",
       "              tensor([-0.3681, -0.9079, -0.1998,  ...,  0.8533,  0.3202,  1.2172])),\n",
       "             ('0.rnns.2.module.weight_hh_l0_raw',\n",
       "              tensor([[-0.0966,  0.0236, -0.0152,  ...,  0.0388, -0.0531, -0.0395],\n",
       "                      [-0.0328, -0.2217,  0.0028,  ...,  0.0143, -0.0368, -0.0085],\n",
       "                      [ 0.0167, -0.0081, -0.0561,  ...,  0.0125,  0.0442, -0.0139],\n",
       "                      ...,\n",
       "                      [-0.0212, -0.1034, -0.0106,  ..., -0.0561,  0.0200, -0.0157],\n",
       "                      [ 0.0183,  0.0364, -0.0251,  ..., -0.0240, -0.1150,  0.0046],\n",
       "                      [ 0.0100, -0.1824,  0.1076,  ..., -0.0269,  0.2733,  0.1846]])),\n",
       "             ('1.decoder.weight',\n",
       "              tensor([[-1.2274e-01,  2.7886e-01, -3.8850e-01,  ..., -1.0404e-01,\n",
       "                        1.9580e-02,  1.8548e-01],\n",
       "                      [ 1.4854e-05, -2.3424e-05,  1.9693e-05,  ...,  2.1349e-05,\n",
       "                        2.1776e-05, -1.2394e-05],\n",
       "                      [ 1.8070e-01,  1.5874e+00, -1.1738e-01,  ..., -4.5935e-02,\n",
       "                       -8.1352e-02,  1.8054e-01],\n",
       "                      ...,\n",
       "                      [-1.8595e-03, -6.8529e-03,  1.6999e-03,  ...,  1.7039e-03,\n",
       "                        4.1632e-03, -1.3171e-03],\n",
       "                      [-2.3120e-03, -6.9001e-03,  1.8772e-03,  ...,  5.0309e-04,\n",
       "                        4.6596e-03, -2.5850e-03],\n",
       "                      [-2.2463e-03, -9.1512e-03,  1.3927e-03,  ...,  1.2296e-03,\n",
       "                        5.8085e-03, -1.8940e-03]]))])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts = torch.load(f'{PATH_AWD_LSTM}/fwd_wt103_enc.h5', map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('encoder.weight',\n",
       "              tensor([[-1.2274e-01,  2.7886e-01, -3.8850e-01,  ..., -1.0404e-01,\n",
       "                        1.9580e-02,  1.8548e-01],\n",
       "                      [ 1.4854e-05, -2.3424e-05,  1.9693e-05,  ...,  2.1349e-05,\n",
       "                        2.1776e-05, -1.2394e-05],\n",
       "                      [ 1.8070e-01,  1.5874e+00, -1.1738e-01,  ..., -4.5935e-02,\n",
       "                       -8.1352e-02,  1.8054e-01],\n",
       "                      ...,\n",
       "                      [-1.8595e-03, -6.8529e-03,  1.6999e-03,  ...,  1.7039e-03,\n",
       "                        4.1632e-03, -1.3171e-03],\n",
       "                      [-2.3120e-03, -6.9001e-03,  1.8772e-03,  ...,  5.0309e-04,\n",
       "                        4.6596e-03, -2.5850e-03],\n",
       "                      [-2.2463e-03, -9.1512e-03,  1.3927e-03,  ...,  1.2296e-03,\n",
       "                        5.8085e-03, -1.8940e-03]])),\n",
       "             ('encoder_with_dropout.embed.weight',\n",
       "              tensor([[-1.2274e-01,  2.7886e-01, -3.8850e-01,  ..., -1.0404e-01,\n",
       "                        1.9580e-02,  1.8548e-01],\n",
       "                      [ 1.4854e-05, -2.3424e-05,  1.9693e-05,  ...,  2.1349e-05,\n",
       "                        2.1776e-05, -1.2394e-05],\n",
       "                      [ 1.8070e-01,  1.5874e+00, -1.1738e-01,  ..., -4.5935e-02,\n",
       "                       -8.1352e-02,  1.8054e-01],\n",
       "                      ...,\n",
       "                      [-1.8595e-03, -6.8529e-03,  1.6999e-03,  ...,  1.7039e-03,\n",
       "                        4.1632e-03, -1.3171e-03],\n",
       "                      [-2.3120e-03, -6.9001e-03,  1.8772e-03,  ...,  5.0309e-04,\n",
       "                        4.6596e-03, -2.5850e-03],\n",
       "                      [-2.2463e-03, -9.1512e-03,  1.3927e-03,  ...,  1.2296e-03,\n",
       "                        5.8085e-03, -1.8940e-03]])),\n",
       "             ('rnns.0.module.weight_ih_l0',\n",
       "              tensor([[-0.0812, -0.0811, -0.0937,  ..., -0.0259, -0.1403, -0.3247],\n",
       "                      [ 0.1154,  0.1142,  0.0938,  ..., -0.0711,  0.1669, -0.0387],\n",
       "                      [-0.0051,  0.1007,  0.2071,  ..., -0.0860, -0.0288, -0.0894],\n",
       "                      ...,\n",
       "                      [ 0.0055,  0.0157,  0.2990,  ...,  0.0616,  0.1159, -0.4737],\n",
       "                      [ 0.0181,  0.0426,  0.1130,  ...,  0.3529, -0.0114, -0.0125],\n",
       "                      [-0.0167, -0.1328,  0.1741,  ...,  0.0548, -0.0045,  0.1688]])),\n",
       "             ('rnns.0.module.bias_ih_l0',\n",
       "              tensor([ 0.1503, -0.4701, -0.1885,  ..., -0.5919, -0.2172, -0.1207])),\n",
       "             ('rnns.0.module.bias_hh_l0',\n",
       "              tensor([ 0.1503, -0.4701, -0.1885,  ..., -0.5919, -0.2172, -0.1207])),\n",
       "             ('rnns.0.module.weight_hh_l0_raw',\n",
       "              tensor([[-0.1013,  0.1786, -0.0528,  ...,  0.0741,  0.0306,  0.2467],\n",
       "                      [ 0.1780, -0.0853, -0.0243,  ..., -0.1129, -0.1310, -0.1498],\n",
       "                      [ 0.0661, -0.0496,  0.0921,  ...,  0.1829,  0.0533, -0.1525],\n",
       "                      ...,\n",
       "                      [-0.0322, -0.0704,  0.1653,  ...,  0.2142, -0.0558,  0.0315],\n",
       "                      [-0.1651, -0.0290,  0.1748,  ..., -0.0446,  0.5444,  0.0616],\n",
       "                      [ 0.0905, -0.1704, -0.0053,  ..., -0.0057,  0.2269,  0.0328]])),\n",
       "             ('rnns.1.module.weight_ih_l0',\n",
       "              tensor([[ 0.3307,  0.0385,  0.0860,  ...,  0.0685, -0.0444,  0.0539],\n",
       "                      [ 0.0720,  0.1607,  0.0562,  ...,  0.0276,  0.0613,  0.1632],\n",
       "                      [-0.1565, -0.1168,  0.1897,  ..., -0.0357,  0.0296,  0.0961],\n",
       "                      ...,\n",
       "                      [-0.0897, -0.1464, -0.0760,  ...,  0.0536,  0.0422, -0.0580],\n",
       "                      [ 0.1166, -0.1534, -0.1784,  ..., -0.0689,  0.2170,  0.1461],\n",
       "                      [-0.0413,  0.0689,  0.0581,  ..., -0.0640, -0.1703, -0.0945]])),\n",
       "             ('rnns.1.module.bias_ih_l0',\n",
       "              tensor([-0.8577, -0.6784, -0.7249,  ..., -0.6782,  0.0567, -0.5026])),\n",
       "             ('rnns.1.module.bias_hh_l0',\n",
       "              tensor([-0.8577, -0.6784, -0.7249,  ..., -0.6782,  0.0567, -0.5026])),\n",
       "             ('rnns.1.module.weight_hh_l0_raw',\n",
       "              tensor([[-0.0273, -0.2277,  0.0782,  ...,  0.1355, -0.1282,  0.1669],\n",
       "                      [ 0.1218,  0.0017, -0.0998,  ..., -0.2085, -0.0686, -0.1389],\n",
       "                      [-0.3878, -0.0498, -0.1748,  ..., -0.4014,  0.1986, -0.4400],\n",
       "                      ...,\n",
       "                      [-0.2097, -0.4298,  0.3551,  ...,  0.0316, -0.1198,  0.1266],\n",
       "                      [ 0.0037, -0.0223,  0.0032,  ..., -0.2672, -0.3093, -0.0361],\n",
       "                      [-0.0464,  0.1664, -0.1348,  ...,  0.1600, -0.1138,  0.0845]])),\n",
       "             ('rnns.2.module.weight_ih_l0',\n",
       "              tensor([[-0.0741,  0.0447, -0.0744,  ..., -0.0419,  0.1600, -0.0553],\n",
       "                      [ 0.0270,  0.0118,  0.0449,  ...,  0.1165, -0.1080, -0.0681],\n",
       "                      [-0.1023, -0.1662, -0.0229,  ...,  0.1652, -0.1070,  0.0970],\n",
       "                      ...,\n",
       "                      [-0.0989, -0.4425, -0.0343,  ..., -0.1434,  0.5851, -0.0291],\n",
       "                      [ 0.0802, -0.1067,  0.2789,  ..., -0.0916, -0.2240,  0.1020],\n",
       "                      [-0.4078,  0.7220,  0.1142,  ...,  0.5287,  0.2035, -0.1811]])),\n",
       "             ('rnns.2.module.bias_ih_l0',\n",
       "              tensor([-0.3681, -0.9079, -0.1998,  ...,  0.8533,  0.3202,  1.2172])),\n",
       "             ('rnns.2.module.bias_hh_l0',\n",
       "              tensor([-0.3681, -0.9079, -0.1998,  ...,  0.8533,  0.3202,  1.2172])),\n",
       "             ('rnns.2.module.weight_hh_l0_raw',\n",
       "              tensor([[-0.0966,  0.0236, -0.0152,  ...,  0.0388, -0.0531, -0.0395],\n",
       "                      [-0.0328, -0.2217,  0.0028,  ...,  0.0143, -0.0368, -0.0085],\n",
       "                      [ 0.0167, -0.0081, -0.0561,  ...,  0.0125,  0.0442, -0.0139],\n",
       "                      ...,\n",
       "                      [-0.0212, -0.1034, -0.0106,  ..., -0.0561,  0.0200, -0.0157],\n",
       "                      [ 0.0183,  0.0364, -0.0251,  ..., -0.0240, -0.1150,  0.0046],\n",
       "                      [ 0.0100, -0.1824,  0.1076,  ..., -0.0269,  0.2733,  0.1846]]))])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos2=pickle.load(open(f'{PATH_AWD_LSTM}/itos_wt103.pkl','rb'))\n",
    "stoi2 = collections.defaultdict(lambda: -1, { v: k for k, v in enumerate(itos2) })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([238462, 400])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgts['encoder.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_wgts = wgts['encoder.weight'].numpy() # converts np.ndarray from torch.FloatTensor.output shape: (238462, 400)\n",
    "row_m = enc_wgts.mean(0) # returns the average of the array elements along axis 0. output shape: (400,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((238462, 400), 59972)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_wgts.shape, len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_emb=enc_wgts.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_w = np.zeros((len(itos),n_emb), dtype=np.float32) # shape: (60002, 400)\n",
    "\n",
    "for i, w in enumerate(itos):\n",
    "    r = stoi2[w]\n",
    "    new_w[i] = enc_wgts[r] if r >= 0 else row_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8910"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(itos).difference(set(itos2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from adaptive import *\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inp=len(itos)\n",
    "n_emb=400 #650\n",
    "n_hidden=400 #650\n",
    "n_layers=2\n",
    "dropout=0.5\n",
    "wd=1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class language_model (nn.Module):\n",
    "    def __init__(self,n_inp,n_emb,n_hidden,n_layers,bidirectional,bs,device,dropout_e=0.05,dropout=0.5,\\\n",
    "                 dropout_o=0.5,pretrain_mtx=None,adaptive_log_softmax=True,tie_weights=True):\n",
    "        super().__init__()\n",
    "        self.n_inp,self.n_emb,self.n_hidden,self.n_layers,self.bidirectional,self.bs,self.device,self.pretrain_mtx=\\\n",
    "                            n_inp,n_emb,n_hidden,n_layers,bidirectional,bs,device,pretrain_mtx\n",
    "        self.adaptive_log_softmax,self.tie_weights=adaptive_log_softmax,tie_weights\n",
    "        self.dropout_e,self.dropout,self.dropout_o=dropout_e,dropout,dropout_o\n",
    "        self.gen_hidden()\n",
    "        self.create_architecture()\n",
    "        if pretrain_mtx is not None:\n",
    "            print (\"initializing\")\n",
    "            self.initialize_glove()\n",
    "            \n",
    "        if self.adaptive_log_softmax is False:\n",
    "            self.criterion=nn.CrossEntropyLoss()\n",
    "        \n",
    "    def create_architecture(self):\n",
    "        # Dropout layer\n",
    "        self.dropout_enc=nn.Dropout(self.dropout_e)\n",
    "        # Embedding Layer\n",
    "        self.encoder=nn.Embedding(self.n_inp,self.n_emb)\n",
    "        # LSTM Layer\n",
    "        self.lstm=nn.LSTM(self.n_emb,self.n_hidden,self.n_layers,batch_first=True,dropout=self.dropout,\\\n",
    "                          bidirectional=False)\n",
    "        self.dropout_op=nn.Dropout(self.dropout_o)\n",
    "        \n",
    "        if self.adaptive_log_softmax:\n",
    "            # Adaptive Log Softmax Loss\n",
    "            self.adaptive_softmax=AdaptiveLogSoftmaxWithLoss(self.n_hidden,\n",
    "                                    self.n_inp,\n",
    "                                    cutoffs=[round(self.n_inp/15),3*round(self.n_inp/15)],\n",
    "                                    div_value=4,\n",
    "                                    get_full_prob=True)\n",
    "        else:\n",
    "            self.decoder=nn.Linear(self.n_hidden,self.n_inp)\n",
    "    \n",
    "    def freeze_embedding(self):\n",
    "        self.encoder.weight.requires_grad=False\n",
    "        \n",
    "    \n",
    "    def unfreeze_embedding(self):\n",
    "        self.encoder.weight.requires_grad=True\n",
    "        \n",
    "        \n",
    "    def initialize_glove(self):\n",
    "        self.encoder.weight.data=torch.Tensor(self.pretrain_mtx)\n",
    "        self.decoder.weight=self.encoder.weight\n",
    "    \n",
    "    def gen_hidden(self):\n",
    "        # Initialize hidden\n",
    "        self.hidden=(Variable(torch.zeros(self.n_layers,self.bs,self.n_hidden,requires_grad=False).to(self.device)),\n",
    "                     Variable(torch.zeros(self.n_layers,self.bs,self.n_hidden,requires_grad=False).to(self.device)))\n",
    "    \n",
    "        \n",
    "    def forward(self,Xb,Yb):\n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        embs=self.dropout_enc(self.encoder(Xb))\n",
    "        if Xb.size(0) < self.bs:\n",
    "            self.hidden=(self.hidden[0][:,:Xb.size(0),:].contiguous(),\n",
    "            self.hidden[1][:,:Xb.size(0),:].contiguous())\n",
    "        out,new_hidden=self.lstm(embs,self.hidden)\n",
    "        out=self.dropout_op(out)\n",
    "        out=out.reshape(out.size(0)*out.size(1),out.size(2))        # output is of shape n_batch * n_seq * n_hidden\n",
    "        # Wrap the hidden state in a new tensor without the gradients\n",
    "        self.hidden=(Variable(new_hidden[0].data,requires_grad=False).to(self.device),\\\n",
    "                     Variable(new_hidden[1].data,requires_grad=False).to(self.device))\n",
    "        if self.adaptive_log_softmax:\n",
    "            out=self.adaptive_softmax(out,Yb.view(-1))\n",
    "            loss=out.loss\n",
    "            preds=out.output_full\n",
    "        else:\n",
    "            loss=self.criterion(out.view(-1, self.model.n_inp),Yb)\n",
    "            preds=self.decoder(out)\n",
    "        return preds, loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_multinomial(preds,actual):\n",
    "    preds=preds.max(1)[1]\n",
    "    correct=preds==actual\n",
    "    return correct.float().sum()/len(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing\n"
     ]
    }
   ],
   "source": [
    "device='cuda:0'\n",
    "model=language_model(n_inp,n_emb,n_hidden,n_layers,False,bs,device,0.05,0.5,0.5,new_w,False,True)\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2274e-01,  2.7886e-01, -3.8850e-01,  ..., -1.0404e-01,\n",
       "          1.9580e-02,  1.8548e-01],\n",
       "        [ 1.4854e-05, -2.3424e-05,  1.9693e-05,  ...,  2.1349e-05,\n",
       "          2.1776e-05, -1.2394e-05],\n",
       "        [-1.8296e-02, -1.3826e-01,  1.4381e-02,  ...,  4.8841e-03,\n",
       "          5.7428e-02, -7.5990e-03],\n",
       "        ...,\n",
       "        [-1.8296e-02, -1.3826e-01,  1.4381e-02,  ...,  4.8841e-03,\n",
       "          5.7428e-02, -7.5990e-03],\n",
       "        [-1.8296e-02, -1.3826e-01,  1.4381e-02,  ...,  4.8841e-03,\n",
       "          5.7428e-02, -7.5990e-03],\n",
       "        [-1.8296e-02, -1.3826e-01,  1.4381e-02,  ...,  4.8841e-03,\n",
       "          5.7428e-02, -7.5990e-03]], device='cuda:0')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self,model,optimizer,metric_fn,device,bptt=12,print_every=5,clip_val=None):\n",
    "        self.model,self.optimizer,self.metric_fn,self.device,self.print_every,self.bptt,self.losses,self.clip_val=\\\n",
    "            model,optimizer,metric_fn,device,print_every,bptt,[],clip_val\n",
    "        self.n_epochs=1\n",
    "  \n",
    "        \n",
    "    \n",
    "    def fit (self,Xb,Yb,mode_train=True):\n",
    "        if mode_train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            \n",
    "        preds,loss=self.model(Xb,Yb)\n",
    "        \n",
    "       \n",
    "            \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            acc=self.metric_fn(preds,Yb.view(-1))\n",
    "            acc=acc.item()\n",
    "            del preds\n",
    "        \n",
    "        if mode_train:\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        myloss=loss.item()\n",
    "        del loss\n",
    "        \n",
    "        if self.clip_val is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.25)\n",
    "            if 1==0:\n",
    "                for p in self.model.parameters():\n",
    "                    p.data.add_(self.lr, p.grad.data)\n",
    "        \n",
    "        return myloss, acc\n",
    "    \n",
    "    def lr_find (self,start_lr,end_lr,iterator,n_batch):\n",
    "        losses,lrs=[],[]\n",
    "        ratio=end_lr/start_lr\n",
    "        num_steps=n_batch\n",
    "        lr=start_lr\n",
    "        for i in range(num_steps):            \n",
    "            lr=lr*(end_lr/start_lr)**(1/num_steps)\n",
    "            lrs.append(lr)\n",
    "        self.lrs=lrs\n",
    "        self.run_epoch(iterator,mode_train=True,lrs=lrs)\n",
    "    \n",
    "    def run_epoch(self,iterator,mode_train,lrs=None):\n",
    "        n_batch=iterator.shape[1]\n",
    "        epoch_loss,epoch_acc,i=0,0,0\n",
    "        self.model.gen_hidden()\n",
    "        for k,i in enumerate(range(0,n_batch,self.bptt)):\n",
    "            seq_len=min(bptt,n_batch-1-i)\n",
    "            Xb=train_tokens[:,i:i+seq_len]\n",
    "            Yb=train_tokens[:,i+1:i+1+seq_len]\n",
    "            Xb=torch.LongTensor(Xb)\n",
    "            Yb=torch.LongTensor(Yb)\n",
    "            Xb=Xb.to(self.device)\n",
    "            Yb=Yb.to(self.device)\n",
    "            \n",
    "            if lrs is not None:\n",
    "                lr=lrs[k]\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr']=lr \n",
    "            \n",
    "\n",
    "            loss,acc=self.fit(Xb,Yb,mode_train)\n",
    "            \n",
    "            if lrs is not None:\n",
    "                self.losses.append(loss)\n",
    "            \n",
    "            \n",
    "            epoch_loss+=loss\n",
    "            epoch_acc+=acc\n",
    "            if k%self.print_every == 0:\n",
    "                if k:\n",
    "                    print (f'Batch:{k} {epoch_loss/(k)}  {epoch_acc/(k)}')  \n",
    "                    torch.cuda.empty_cache()\n",
    "        epoch_loss=epoch_loss/k\n",
    "        epoch_acc=epoch_acc/k\n",
    "        \n",
    "        if 1==0:\n",
    "            lr /= 4.0\n",
    "            # Freeze all the layers initially\n",
    "            for param in resnet.parameters():\n",
    "                param.requires_grad=False\n",
    "            torch.save(resnet,'resnet')\n",
    "            torch.save(resnet.state_dict(),'resnet_state_dict')\n",
    "            resnet.load_state_dict(torch.load('resnet_state_dict'))\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr']=lr  \n",
    "            \n",
    "        return epoch_loss,epoch_acc\n",
    "    \n",
    "    def plot_lrs(self, n_roll=1):\n",
    "        import seaborn as sns\n",
    "        ax=sns.lineplot(x=self.lrs,y=pd.Series(self.losses).rolling(n_roll).mean())\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.set_xlabel('Learning Rate')\n",
    "\n",
    "    \n",
    "    def run_epochs(self,dltrain,dlvalid,n_epochs=1):\n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            loss,acc=self.run_epoch(dltrain,True)\n",
    "            print (f'Epoch:{epoch} Loss:{loss}')\n",
    "            lossv,accv=self.run_epoch(dlvalid,mode_train=False)\n",
    "            print (f'Epoch:{epoch} Loss:{loss} Accuracy:{acc} Loss:{lossv} Accuracy:{accv}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(model.parameters(),lr=1e-3,betas=(0.9,0.999), weight_decay=wd)\n",
    "metric_fn=accuracy_multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29465.457142857143"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens.shape[1]/bptt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner=Learner(model,optimizer,accuracy_multinomial,device,bptt,500,0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch=np.int(np.ceil(valid_tokens.shape[1]/bptt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:500 7.944759424209595  0.03532285663112998\n",
      "Batch:1000 7.877773783683777  0.03720142808556557\n",
      "Batch:1500 7.802010853131613  0.03968857092410326\n",
      "Batch:2000 7.722838351964951  0.04160571377538145\n",
      "Batch:2500 7.658665308570862  0.043098285208642485\n"
     ]
    }
   ],
   "source": [
    "learner.lr_find(1e-4,1e-1,valid_tokens,n_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.plot_lrs(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2910"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(learner.lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2910"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(learner.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'language_model' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-55bb90a80737>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-65-74779c440876>\u001b[0m in \u001b[0;36mrun_epochs\u001b[0;34m(self, dltrain, dlvalid, n_epochs)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdltrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch:{epoch} Loss:{loss}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mlossv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdlvalid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-74779c440876>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(self, iterator, mode_train, lrs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mYb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlrs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-74779c440876>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, Xb, Yb, mode_train)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mYb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-61-d1abc3c56951>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, Xb, Yb)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mpreds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_full\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_inp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mYb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mpreds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    537\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 539\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'language_model' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(train_tokens,valid_tokens,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        if rnn_type in ['LSTM', 'GRU']:\n",
    "            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
    "        else:\n",
    "            try:\n",
    "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
    "            except KeyError:\n",
    "                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n",
    "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n",
    "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        # Optionally tie weights as in:\n",
    "        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
    "        # https://arxiv.org/abs/1608.05859\n",
    "        # and\n",
    "        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
    "        # https://arxiv.org/abs/1611.01462\n",
    "        if tie_weights:\n",
    "            if nhid != ninp:\n",
    "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
    "                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
    "        else:\n",
    "            return weight.new_zeros(self.nlayers, bsz, self.nhid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "newmodel=torch.load('/home/kirana/Documents/phd/examples/examples/word_language_model/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (drop): Dropout(p=0.2)\n",
       "  (encoder): Embedding(33278, 200)\n",
       "  (rnn): LSTM(200, 200, num_layers=2, dropout=0.2)\n",
       "  (decoder): Linear(in_features=200, out_features=33278, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-fce809493b06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnewmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "newmodel.encoder.weight.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
