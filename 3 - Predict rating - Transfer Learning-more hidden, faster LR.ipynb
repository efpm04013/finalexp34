{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import os \n",
    "import fastai\n",
    "import fastai\n",
    "from fastai.text import *\n",
    "PATH=\"/home/kirana/Documents/phd/final\"\n",
    "DATAPATH=\"/home/kirana/Documents/phd/data/aclImdb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "[df_train,df_valid,df_test,itos, train_tokens, valid_tokens, test_tokens, trn_lm, val_lm, test_lm]=pickle.load(open(f'{PATH}/inter/dfs_tokens_fastai.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.concat([df_train,df_valid],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid=df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df_train.loc[df_train['label']<2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3218</th>\n",
       "      <td>0</td>\n",
       "      <td>I've noticed how all the other reviews of this...</td>\n",
       "      <td>[ \\n , xxbos, xxfld, 1, i, 've, noticed, how, ...</td>\n",
       "      <td>[41, 42, 43, 40, 13, 161, 2030, 110, 44, 3, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6854</th>\n",
       "      <td>0</td>\n",
       "      <td>I went on a visit to one of my relatives a whi...</td>\n",
       "      <td>[ \\n , xxbos, xxfld, 1, i, went, on, a, visit,...</td>\n",
       "      <td>[41, 42, 43, 40, 13, 436, 30, 7, 2074, 9, 39, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>0</td>\n",
       "      <td>Darcy and her young daughter Pamela are headin...</td>\n",
       "      <td>[ \\n , xxbos, xxfld, 1, xxmaj, darcy, and, her...</td>\n",
       "      <td>[41, 42, 43, 40, 2, 9225, 6, 55, 207, 551, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5747</th>\n",
       "      <td>0</td>\n",
       "      <td>Battlestar Gallactica was so great because it ...</td>\n",
       "      <td>[ \\n , xxbos, xxfld, 1, xxmaj, battlestar, xxm...</td>\n",
       "      <td>[41, 42, 43, 40, 2, 7900, 2, 33242, 20, 52, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8731</th>\n",
       "      <td>0</td>\n",
       "      <td>Any movie with \"National Lampoon\" in the title...</td>\n",
       "      <td>[ \\n , xxbos, xxfld, 1, xxmaj, any, movie, wit...</td>\n",
       "      <td>[41, 42, 43, 40, 2, 120, 25, 23, 16, 2, 2155, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  \\\n",
       "3218      0  I've noticed how all the other reviews of this...   \n",
       "6854      0  I went on a visit to one of my relatives a whi...   \n",
       "2697      0  Darcy and her young daughter Pamela are headin...   \n",
       "5747      0  Battlestar Gallactica was so great because it ...   \n",
       "8731      0  Any movie with \"National Lampoon\" in the title...   \n",
       "\n",
       "                                                  words  \\\n",
       "3218  [ \\n , xxbos, xxfld, 1, i, 've, noticed, how, ...   \n",
       "6854  [ \\n , xxbos, xxfld, 1, i, went, on, a, visit,...   \n",
       "2697  [ \\n , xxbos, xxfld, 1, xxmaj, darcy, and, her...   \n",
       "5747  [ \\n , xxbos, xxfld, 1, xxmaj, battlestar, xxm...   \n",
       "8731  [ \\n , xxbos, xxfld, 1, xxmaj, any, movie, wit...   \n",
       "\n",
       "                                                 tokens  \n",
       "3218  [41, 42, 43, 40, 13, 161, 2030, 110, 44, 3, 10...  \n",
       "6854  [41, 42, 43, 40, 13, 436, 30, 7, 2074, 9, 39, ...  \n",
       "2697  [41, 42, 43, 40, 2, 9225, 6, 55, 207, 551, 2, ...  \n",
       "5747  [41, 42, 43, 40, 2, 7900, 2, 33242, 20, 52, 10...  \n",
       "8731  [41, 42, 43, 40, 2, 120, 25, 23, 16, 2, 2155, ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_train['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=52 # 52 - Jeremey, 20 - default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt= 70 #70 - Jeremey, 35 - default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3218</th>\n",
       "      <td>0</td>\n",
       "      <td>I've noticed how all the other reviews of this...</td>\n",
       "      <td>[ \\n , xxbos, xxfld, 1, i, 've, noticed, how, ...</td>\n",
       "      <td>[41, 42, 43, 40, 13, 161, 2030, 110, 44, 3, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6854</th>\n",
       "      <td>0</td>\n",
       "      <td>I went on a visit to one of my relatives a whi...</td>\n",
       "      <td>[ \\n , xxbos, xxfld, 1, i, went, on, a, visit,...</td>\n",
       "      <td>[41, 42, 43, 40, 13, 436, 30, 7, 2074, 9, 39, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>0</td>\n",
       "      <td>Darcy and her young daughter Pamela are headin...</td>\n",
       "      <td>[ \\n , xxbos, xxfld, 1, xxmaj, darcy, and, her...</td>\n",
       "      <td>[41, 42, 43, 40, 2, 9225, 6, 55, 207, 551, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5747</th>\n",
       "      <td>0</td>\n",
       "      <td>Battlestar Gallactica was so great because it ...</td>\n",
       "      <td>[ \\n , xxbos, xxfld, 1, xxmaj, battlestar, xxm...</td>\n",
       "      <td>[41, 42, 43, 40, 2, 7900, 2, 33242, 20, 52, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8731</th>\n",
       "      <td>0</td>\n",
       "      <td>Any movie with \"National Lampoon\" in the title...</td>\n",
       "      <td>[ \\n , xxbos, xxfld, 1, xxmaj, any, movie, wit...</td>\n",
       "      <td>[41, 42, 43, 40, 2, 120, 25, 23, 16, 2, 2155, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  \\\n",
       "3218      0  I've noticed how all the other reviews of this...   \n",
       "6854      0  I went on a visit to one of my relatives a whi...   \n",
       "2697      0  Darcy and her young daughter Pamela are headin...   \n",
       "5747      0  Battlestar Gallactica was so great because it ...   \n",
       "8731      0  Any movie with \"National Lampoon\" in the title...   \n",
       "\n",
       "                                                  words  \\\n",
       "3218  [ \\n , xxbos, xxfld, 1, i, 've, noticed, how, ...   \n",
       "6854  [ \\n , xxbos, xxfld, 1, i, went, on, a, visit,...   \n",
       "2697  [ \\n , xxbos, xxfld, 1, xxmaj, darcy, and, her...   \n",
       "5747  [ \\n , xxbos, xxfld, 1, xxmaj, battlestar, xxm...   \n",
       "8731  [ \\n , xxbos, xxfld, 1, xxmaj, any, movie, wit...   \n",
       "\n",
       "                                                 tokens  \n",
       "3218  [41, 42, 43, 40, 13, 161, 2030, 110, 44, 3, 10...  \n",
       "6854  [41, 42, 43, 40, 13, 436, 30, 7, 2074, 9, 39, ...  \n",
       "2697  [41, 42, 43, 40, 2, 9225, 6, 55, 207, 551, 2, ...  \n",
       "5747  [41, 42, 43, 40, 2, 7900, 2, 33242, 20, 52, 10...  \n",
       "8731  [41, 42, 43, 40, 2, 120, 25, 23, 16, 2, 2155, ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12500\n",
       "0    12500\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12500\n",
       "0    12500\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_unk_', '_pad_', 'xxmaj', 'the', '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "padlen=1400\n",
    "padding_idx=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['n_tok']=df_train['tokens'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    25000.000000\n",
       "mean       304.945840\n",
       "std        224.981807\n",
       "min         16.000000\n",
       "25%        166.000000\n",
       "50%        228.000000\n",
       "75%        371.000000\n",
       "max       3354.000000\n",
       "Name: n_tok, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['n_tok'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.sort_values(by='n_tok', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid['n_tok']=df_valid['tokens'].apply(len)\n",
    "df_valid.sort_values(by='n_tok', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['n_tok']=df_test['tokens'].apply(len)\n",
    "df_test.sort_values(by='n_tok', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41. 42. 43. 40. ...  1.  1.  1.  1.]\n",
      "0\n",
      "1400\n",
      "[41. 42. 43. 40. ...  1.  1.  1.  1.]\n",
      "0\n",
      "1400\n",
      "[41. 42. 43. 40. ...  1.  1.  1.  1.]\n",
      "1\n",
      "1400\n",
      "[41. 42. 43. 40. ...  1.  1.  1.  1.]\n",
      "0\n",
      "1400\n",
      "[41. 42. 43. 40. ...  1.  1.  1.  1.]\n",
      "0\n",
      "1400\n",
      "[41. 42. 43. 40. ...  1.  1.  1.  1.]\n",
      "0\n",
      "1400\n",
      "[41. 42. 43. 40. ...  1.  1.  1.  1.]\n",
      "1\n",
      "1400\n",
      "[41. 42. 43. 40. ...  1.  1.  1.  1.]\n",
      "0\n",
      "1400\n",
      "[41. 42. 43. 40. ...  1.  1.  1.  1.]\n",
      "1\n",
      "1400\n",
      "[41. 42. 43. 40. ...  1.  1.  1.  1.]\n",
      "0\n",
      "1400\n",
      "[41. 42. 43. 40. ...  1.  1.  1.  1.]\n",
      "0\n",
      "1400\n",
      "[41. 42. 43. 40. ...  1.  1.  1.  1.]\n",
      "0\n",
      "1400\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,20):\n",
    "    x=df_train['tokens'].values[i]\n",
    "    y=df_train['label'].values[i]    \n",
    "    out=np.ones(padlen)\n",
    "    if len(x) < padlen:\n",
    "        out[:len(x)]=x\n",
    "    else:\n",
    "        out=x[:padlen]\n",
    "    print (out)\n",
    "    print (y)\n",
    "    print (len(out))\n",
    "    if i>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 5)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1    12500\n",
       " 0    12500\n",
       " Name: label, dtype: int64, 1    12500\n",
       " 0    12500\n",
       " Name: label, dtype: int64, 1    12500\n",
       " 0    12500\n",
       " Name: label, dtype: int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].value_counts(), df_valid['label'].value_counts(),df_test['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 5), (25000, 5))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1400"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ds_sentiment (Dataset):\n",
    "    def __init__ (self,df,bs,padlen=64,xvar='tokens',yvar='label',len_var='n_tok',padding_idx=1):\n",
    "        self.x,self.y,self.padlen,self.padding_idx,self.len_var,self.bs=\\\n",
    "            df[xvar],df[yvar],padlen,padding_idx,df[len_var],bs\n",
    "        self.len_var=self.len_var.clip(0,padlen)\n",
    "    \n",
    "    def pad (self,x):\n",
    "        out=np.ones(self.padlen)*self.padding_idx\n",
    "        out=out.astype(int)\n",
    "        if len(x)>=self.padlen:\n",
    "            out[:]=x[:self.padlen]\n",
    "        else:\n",
    "            out[:len(x)]=x\n",
    "        return out\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        return self.pad(self.x.iloc[idx]),self.y.iloc[idx],self.len_var.iloc[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#padlen=max(df_train['n_tok'])\n",
    "padlen=1400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1400"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstrain=ds_sentiment(df_train,bs,padlen)\n",
    "dsvalid=ds_sentiment(df_valid,bs,padlen)\n",
    "dstest=ds_sentiment(df_test,bs,padlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dltrain=DataLoader(dstrain,bs,True)\n",
    "dlvalid=DataLoader(dsvalid,bs,False)\n",
    "dltest=DataLoader(dstest,bs,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xb,yb,xlen in dltrain:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[41, 42, 43,  ...,  1,  1,  1],\n",
       "         [41, 42, 43,  ...,  1,  1,  1],\n",
       "         [41, 42, 43,  ...,  1,  1,  1],\n",
       "         ...,\n",
       "         [41, 42, 43,  ...,  1,  1,  1],\n",
       "         [41, 42, 43,  ...,  1,  1,  1],\n",
       "         [41, 42, 43,  ...,  1,  1,  1]]),\n",
       " tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "         1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "         0, 0, 0, 1]),\n",
       " tensor([409, 470, 182,  61, 427, 170, 426, 727, 281, 452,  75, 172, 421, 197,\n",
       "         126, 635, 158, 300, 816, 234, 378, 261, 149, 117, 374, 193, 567, 185,\n",
       "         298, 311,  79, 249, 399, 578, 744, 616, 180, 158, 523, 351, 157, 449,\n",
       "         313, 407, 321, 542, 149, 156, 169, 379, 141, 243]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb, xlen"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cyclical learning rates\n",
    "\tvary learning rate by epoch between two values\n",
    "\t\n",
    "\n",
    "bptt=70\n",
    "n_emb=400\n",
    "n_hid=1150\n",
    "n_layers=3\n",
    "bs=48 # whatever is the max that can fit in memory\n",
    "\n",
    "# shuffle dataset\n",
    "# sort the data by length\n",
    "\n",
    "\n",
    "class ds_sentiment(Dataset):\n",
    "\tdef __init__(self,x,y):\n",
    "\t\tself.x,self.y=x,y\n",
    "\t\n",
    "\tdef __getitem__(self,idx):\n",
    "\t\treturn self.x[idx],self.y[idx]\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 70)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs,bptt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inp=len(itos)\n",
    "n_emb=400 #650\n",
    "n_hidden=256 #256\n",
    "n_layers= 2 # 2\n",
    "dropout=0.5 # 0.5\n",
    "wd=1e-7\n",
    "bidirectional=True\n",
    "dropout_e=0.2 # 0.5\n",
    "dropout_o=0.5 #0.5\n",
    "n_out=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([52, 1400]), torch.Size([52]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape,xlen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12500\n",
       "0    12500\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_unk_', '_pad_', 'xxmaj', 'the', '.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentiment_classifier (nn.Module):\n",
    "    def __init__(self,n_inp,n_emb,n_hidden,n_layers,bidirectional,bs,device,dropout_e=0.05,dropout=0.5,\\\n",
    "                 dropout_o=0.5,pretrain_mtx=None,n_out=1,padding_idx=1):\n",
    "        super().__init__()\n",
    "        self.n_inp,self.n_emb,self.n_hidden,self.n_layers,self.bidirectional,self.bs,self.device,self.pretrain_mtx,self.padding_idx=\\\n",
    "                            n_inp,n_emb,n_hidden,n_layers,bidirectional,bs,device,pretrain_mtx,padding_idx\n",
    "        self.n_out=n_out\n",
    "        self.dropout_e,self.dropout,self.dropout_o=dropout_e,dropout,dropout_o\n",
    "        \n",
    "        self.create_architecture()\n",
    "        if pretrain_mtx is not None:\n",
    "            print (f'initializing glove with {pretrain_mtx.shape}')\n",
    "            self.initialize_glove()\n",
    "        self.init_hidden()\n",
    "        self.criterion=nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def set_dropouts(self, dropout, dropout_o, dropout_e):\n",
    "        model.dropout, model.dropout_o, model.dropout_e = dropout, dropout_o, dropout_e\n",
    "    \n",
    "    def create_architecture(self):\n",
    "        # Dropout layer\n",
    "        self.dropout_enc=nn.Dropout(self.dropout_e)\n",
    "        # Embedding Layer: Embedding layer just maps each word to an index. n_inp to n_emb mapping is all it does\n",
    "            # input to this is of shape n_batch * n_seq\n",
    "        self.encoder=nn.Embedding(self.n_inp,self.n_emb,padding_idx=self.padding_idx)\n",
    "             # embs are going to be of shape n_batch * n_seq * n_emb\n",
    "            # h and c are of shape n_layers * n_batch * n_hidden\n",
    "        # LSTM Layer\n",
    "        self.lstm=nn.LSTM(self.n_emb,self.n_hidden,self.n_layers,batch_first=True,dropout=self.dropout,\\\n",
    "                          bidirectional=self.bidirectional)\n",
    "        \n",
    "                # output is of shape n_batch * n_seq * n_hidden\n",
    "            # output [:,-1,:] is the same as hn[-1]\n",
    "        self.dropout_op=nn.Dropout(self.dropout_o)\n",
    "        self.fc=nn.Linear(self.n_hidden*2,self.n_out)\n",
    "\n",
    "        #self.log_softmax=nn.LogSoftmax()\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "    \n",
    "    def freeze_embedding(self):\n",
    "        self.encoder.weight.requires_grad=False\n",
    "         \n",
    "    def unfreeze_embedding(self):\n",
    "        self.encoder.weight.requires_grad=True\n",
    "\n",
    "    def initialize_glove(self):\n",
    "        self.encoder.weight.data.copy_(torch.Tensor(self.pretrain_mtx))\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # Initialize hidden\n",
    "        self.hidden=(Variable(torch.zeros(self.n_layers,self.bs,self.n_hidden,requires_grad=False).to(self.device)),\n",
    "                     Variable(torch.zeros(self.n_layers,self.bs,self.n_hidden,requires_grad=False).to(self.device)))\n",
    "    \n",
    "        \n",
    "    def forward(self,Xb,Yb,Xb_lengths):\n",
    "        embs=self.dropout_enc(self.encoder(Xb))\n",
    "        \n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        #embs=self.dropout_enc(embs)\n",
    "        if Xb.size(0) < self.bs:\n",
    "            self.hidden=(self.hidden[0][:,:Xb.size(0),:].contiguous(),\n",
    "            self.hidden[1][:,:Xb.size(0),:].contiguous())\n",
    "\n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        # before lstm call\n",
    "            # pack_padded_sequence so that padded items in the sequence won't be shown to the LSTM\n",
    "        packed_embs = pack_padded_sequence(embs,Xb_lengths,batch_first=True, enforce_sorted=False)\n",
    "        lstm_out,(hidden,cell)=self.lstm(packed_embs)\n",
    "\n",
    "        # after lstm call\n",
    "        lstm_out,_=pad_packed_sequence(lstm_out,batch_first=True)\n",
    "        #lstm_out=self.dropout_op(lstm_out)      \n",
    "        \n",
    "        # Wrap the hidden state in a new tensor without the gradients\n",
    "        #self.hidden=(Variable(newhidden[0].data,requires_grad=False).to(self.device),Variable(newhidden[1].data,requires_grad=False).to(self.device))\n",
    "        \n",
    "            # lstm_out has the values that are padded\n",
    "        #out=self.fc (lstm_out[:,-1,:])\n",
    "        hidden = self.dropout_op(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        \n",
    "        preds=self.fc(hidden)\n",
    "        \n",
    "              \n",
    "        \n",
    "        loss=self.criterion(preds,Yb.contiguous().float().view(-1,1))\n",
    "        \n",
    "        return preds.view(-1), loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_binomial(preds,actual, device=\"cpu\", cutoff=0.5):\n",
    "    preds=torch.sigmoid(preds)\n",
    "    zeros=torch.zeros(len(preds)).to(device)\n",
    "    ones = torch.ones(len(preds)).to(device)\n",
    "\n",
    "    preds=torch.where(preds>cutoff,ones,zeros)\n",
    "    correct=torch.round(preds).long()==actual\n",
    "    return correct.float().sum()/len(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_binomial(preds, y, device=\"cpu\", cutoff=0.5):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    y=y.float()\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_lm_weights=pickle.load(open(f'{PATH}/inter/pretrained_lm_weights','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1==0:\n",
    "    model=torch.load (f'{PATH}/inter/model_awd_lstm')\n",
    "    pretrained_lm_weights=model.encoder.weight.data.cpu().numpy()\n",
    "    import pickle\n",
    "    pickle.dump(pretrained_lm_weights,open(f'{PATH}/inter/pretrained_lm_weights','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if model forward works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59972, 400, 256, 2, True, 52, 'cpu', 0.5, 0.5, 0.5, 1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_inp,n_emb,n_hidden,n_layers,bidirectional,bs,device,dropout_e,dropout_o,dropout,n_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing glove with (59972, 400)\n"
     ]
    }
   ],
   "source": [
    "model_sentiment=sentiment_classifier (n_inp,n_emb,n_hidden,n_layers,bidirectional,bs,device,dropout_e=dropout_e,dropout=dropout,\\\n",
    "                 dropout_o=dropout_o,pretrain_mtx=pretrained_lm_weights,n_out=1,padding_idx=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 26,913,857 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "print(f'The model has {count_parameters(model_sentiment):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment_classifier(\n",
       "  (dropout_enc): Dropout(p=0.5)\n",
       "  (encoder): Embedding(59972, 400, padding_idx=1)\n",
       "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "  (dropout_op): Dropout(p=0.5)\n",
       "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (criterion): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0415,  0.0344,  0.0018,  0.0841,  0.0414,  0.0935,  0.0823,  0.0710,\n",
       "          0.1126,  0.0460, -0.0339,  0.0157,  0.0158, -0.0062, -0.0680,  0.0780,\n",
       "          0.0629,  0.0065,  0.0757,  0.0475,  0.0257, -0.0033, -0.0053,  0.0406,\n",
       "          0.0949,  0.0832, -0.0116, -0.0234,  0.0689,  0.0832,  0.0138,  0.0247,\n",
       "          0.0859,  0.0132,  0.0543,  0.0472,  0.0736,  0.0221,  0.0084, -0.0010,\n",
       "          0.0088,  0.0376,  0.0127,  0.1119,  0.0722,  0.0162, -0.0004,  0.0046,\n",
       "          0.0614,  0.0308,  0.0509,  0.0401], grad_fn=<ViewBackward>),\n",
       " tensor(0.6945, grad_fn=<BinaryCrossEntropyWithLogitsBackward>))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sentiment.forward(xb,yb,xlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds,loss=model_sentiment.forward(xb,yb,xlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0315,  0.0344,  0.0411,  0.0182,  0.0574,  0.0405,  0.0401,  0.0143,\n",
       "        -0.0063,  0.0031,  0.0338,  0.0315,  0.0821,  0.0395,  0.0073,  0.0669,\n",
       "         0.0595,  0.0336,  0.0320, -0.0402, -0.0295,  0.0647,  0.0360, -0.0134,\n",
       "         0.0715,  0.0451, -0.0064,  0.0409,  0.0881,  0.0748,  0.0438,  0.0564,\n",
       "         0.0385,  0.0612,  0.0220, -0.0111,  0.0084, -0.0021,  0.0427,  0.1133,\n",
       "         0.0926,  0.0574,  0.0579,  0.0408,  0.0569, -0.0252,  0.0090, -0.0183,\n",
       "         0.0078, -0.0104,  0.0629,  0.0153], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 1])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([52])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([52])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5000)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_binomial(preds.to(device),yb.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4562962962962963"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(yb,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9.038397e-02, -1.795423e-02, -3.457930e-02,  8.671192e-02, ..., -3.003510e-02, -1.899482e-02,  1.426260e-02,\n",
       "        -4.948893e-02],\n",
       "       [-2.453323e-02, -3.018817e-02,  6.663179e-02, -5.793565e-02, ...,  1.840935e-02,  6.371137e-02,  9.835491e-03,\n",
       "        -2.131385e-03],\n",
       "       [ 7.696263e-03, -4.266643e-02, -1.533517e-01,  2.239776e-01, ...,  9.869517e-02,  3.041433e-02,  1.824751e-01,\n",
       "         1.134978e-01],\n",
       "       [ 3.305928e-02,  2.266591e-01, -4.264669e-02,  1.490862e-01, ...,  3.407921e-02, -6.422209e-03,  3.180612e-01,\n",
       "         9.373549e-02],\n",
       "       ...,\n",
       "       [ 6.027538e-02, -5.980809e-02,  1.861691e-01, -3.105092e-02, ..., -2.764457e-02,  1.962678e-02, -2.172215e-03,\n",
       "         6.297247e-02],\n",
       "       [-1.592789e-02, -2.781571e-04,  1.301994e-01,  2.851282e-02, ...,  5.064877e-02,  1.670864e-01,  2.283701e-02,\n",
       "        -8.746398e-03],\n",
       "       [ 4.669700e-02,  3.138980e-02,  1.221957e-02, -3.927753e-02, ..., -1.281436e-01,  1.121320e-01, -3.726090e-03,\n",
       "        -3.777364e-02],\n",
       "       [ 1.677964e-02,  3.287802e-02,  2.598143e-02, -1.484041e-02, ..., -2.587889e-02,  4.438318e-02, -2.407267e-02,\n",
       "        -8.544586e-02]], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_lm_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self,model,optimizer,metric_fn,device,bptt=12,print_every=5,clip_val=None):\n",
    "        self.model,self.optimizer,self.metric_fn,self.device,self.print_every,self.bptt,self.losses,self.clip_val=\\\n",
    "            model,optimizer,metric_fn,device,print_every,bptt,[],clip_val\n",
    "        self.n_epochs=1\n",
    "  \n",
    "        \n",
    "    \n",
    "    def fit (self,Xb,Yb,Xlen,mode_train=True):\n",
    "        if mode_train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            \n",
    "        preds,loss=self.model(Xb,Yb,Xlen)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            acc=self.metric_fn(preds,Yb.view(-1),self.device)\n",
    "            acc=acc.item()\n",
    "            del preds\n",
    "        \n",
    "        if mode_train:\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        myloss=loss.item()\n",
    "        del loss\n",
    "        \n",
    "        if self.clip_val is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip_val)\n",
    "        \n",
    "        return myloss, acc\n",
    "    \n",
    "    def lr_find (self,start_lr,end_lr,iterator,n_batch):\n",
    "        losses,lrs=[],[]\n",
    "        ratio=end_lr/start_lr\n",
    "        num_steps=n_batch\n",
    "        lr=start_lr\n",
    "        for i in range(num_steps):            \n",
    "            lr=lr*(end_lr/start_lr)**(1/num_steps)\n",
    "            lrs.append(lr)\n",
    "        self.lrs=lrs\n",
    "        self.run_epoch(iterator,mode_train=True,lrs=lrs)\n",
    "    \n",
    "    def run_epoch(self,iterator,mode_train,lrs=None):\n",
    "        epoch_loss,epoch_acc,i,k=0,0,0,0\n",
    "        self.model.init_hidden()\n",
    "        for Xb,Yb,Xlen in iterator:\n",
    "            Xb=Xb.to(self.device)\n",
    "            Yb=Yb.to(self.device)\n",
    "            Xlen=Xlen.to(self.device)\n",
    "            \n",
    "            if lrs is not None:\n",
    "                lr=lrs[k]\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr']=lr \n",
    "            \n",
    "\n",
    "            loss,acc=self.fit(Xb,Yb,Xlen,mode_train)\n",
    "            \n",
    "            if lrs is not None:\n",
    "                self.losses.append(loss)\n",
    "            \n",
    "            \n",
    "            epoch_loss+=loss\n",
    "            epoch_acc+=acc\n",
    "            \n",
    "            k=k+1\n",
    "            if k%self.print_every == 0:\n",
    "                if k:\n",
    "                    print (f'Batch:{k} {epoch_loss/(k)}  {epoch_acc/(k)}')  \n",
    "                    torch.cuda.empty_cache()\n",
    "        epoch_loss=epoch_loss/len(iterator)\n",
    "        epoch_acc=epoch_acc/len(iterator)\n",
    "            \n",
    "        return epoch_loss,epoch_acc\n",
    "    \n",
    "    def plot_lrs(self, n_roll=1):\n",
    "        import seaborn as sns\n",
    "        ax=sns.lineplot(x=self.lrs,y=pd.Series(self.losses).rolling(n_roll).mean())\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.set_xlabel('Learning Rate')\n",
    "\n",
    "    \n",
    "    def run_epochs(self,dltrain,dlvalid,n_epochs=1):\n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            loss,acc=self.run_epoch(dltrain,True)\n",
    "            print (f'Epoch:{epoch} Loss:{loss}')\n",
    "            lossv,accv=self.run_epoch(dlvalid,mode_train=False)\n",
    "            print (f'Epoch:{epoch} Loss:{loss} Accuracy:{acc} Loss:{lossv} Accuracy:{accv}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing glove with (59972, 400)\n"
     ]
    }
   ],
   "source": [
    "model_sentiment=sentiment_classifier (n_inp,n_emb,n_hidden,n_layers,bidirectional,bs,device,dropout_e,dropout,\\\n",
    "                 dropout_o,pretrain_mtx=pretrained_lm_weights,n_out=1,padding_idx=1)\n",
    "model_sentiment=model_sentiment.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(model_sentiment.parameters(),lr=5e-3,betas=(0.9,0.999), weight_decay=wd)\n",
    "metric_fn=accuracy_binomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(481, 481)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dltrain),len(dlvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12500\n",
       "0    12500\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner=Learner(model_sentiment,optimizer,accuracy_binomial,device,bptt,50,0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sentiment.freeze_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sentiment.encoder.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:50 0.7049101042747498  0.5215384811162949\n",
      "Batch:100 0.6640003144741058  0.5871154063940048\n",
      "Batch:150 0.6459228873252869  0.6179487411181132\n",
      "Batch:200 0.6539176434278489  0.6095192535221576\n",
      "Batch:250 0.6653938720226288  0.5893846372365952\n",
      "Batch:300 0.6683918231725693  0.5835897647341093\n",
      "Batch:350 0.670021391596113  0.5876373835972377\n",
      "Batch:400 0.673944260776043  0.5757692511379718\n",
      "Batch:450 0.6768298112021552  0.5655983105632993\n",
      "Epoch:0 Loss:0.6769196791361375\n",
      "Batch:50 0.6005768239498138  0.8353846478462219\n",
      "Batch:100 0.600014768242836  0.8265384948253631\n",
      "Batch:150 0.5986782133579254  0.8292308024565379\n",
      "Batch:200 0.5994284456968307  0.826346186697483\n",
      "Batch:250 0.6003772809505462  0.8225384938716889\n",
      "Batch:300 0.6006900773445765  0.8207692623138427\n",
      "Batch:350 0.6010935039179666  0.8186264043194907\n",
      "Batch:400 0.6023291030526161  0.8137981072068214\n",
      "Batch:450 0.6033195376396179  0.8091880640718672\n",
      "Epoch:0 Loss:0.6769196791361375 Accuracy:0.5664961017714717 Loss:0.6036174146152583 Accuracy:0.8079642067332278\n",
      "Batch:50 0.5561044049263001  0.7476923370361328\n",
      "Batch:100 0.4993889524042606  0.7867307996749878\n",
      "Batch:150 0.46756422440210976  0.8028205434481303\n",
      "Batch:200 0.4505623260140419  0.8110577234625816\n",
      "Batch:250 0.4370936608314514  0.8180000305175781\n",
      "Batch:300 0.4266696254412333  0.8226282370090484\n",
      "Batch:350 0.4192057349426406  0.8262088234083993\n",
      "Batch:400 0.41067760962992905  0.8312981100380421\n",
      "Batch:450 0.40780084282159806  0.8323931954966651\n",
      "Epoch:1 Loss:0.4028538362028197\n",
      "Batch:50 0.27574400484561923  0.885000034570694\n",
      "Batch:100 0.2694227170199156  0.8878846502304077\n",
      "Batch:150 0.26533324087659516  0.8897436265150706\n",
      "Batch:200 0.26586551826447247  0.8885577288269997\n",
      "Batch:250 0.2683329341113567  0.8873077285289764\n",
      "Batch:300 0.27186456359922884  0.8854487544298172\n",
      "Batch:350 0.27266055449843407  0.884945091690336\n",
      "Batch:400 0.27355244806036355  0.8848077288269997\n",
      "Batch:450 0.2762659355501334  0.8845726854271359\n",
      "Epoch:1 Loss:0.4028538362028197 Accuracy:0.8345994257877374 Loss:0.2769341230733231 Accuracy:0.8846793897186644\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(dltrain,dlvalid,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:50 0.32560369729995725  0.867692346572876\n",
      "Batch:100 0.3149690940976143  0.8713461899757385\n",
      "Batch:150 0.31616545885801317  0.8715384972095489\n",
      "Batch:200 0.31540725149214266  0.8711538809537888\n",
      "Batch:250 0.3134321436882019  0.8712308046817779\n",
      "Batch:300 0.3125024070839087  0.8723718305428823\n",
      "Batch:350 0.30839487544127875  0.8742308049542563\n",
      "Batch:400 0.307709935195744  0.8755769589543343\n",
      "Batch:450 0.3049015839563476  0.8768803780608707\n",
      "Epoch:0 Loss:0.3044661101952908\n",
      "Batch:50 0.25211650103330613  0.9053846561908722\n",
      "Batch:100 0.24904165856540203  0.9059615749120712\n",
      "Batch:150 0.2429587238530318  0.9084615774949392\n",
      "Batch:200 0.2418766375631094  0.9086538863182068\n",
      "Batch:250 0.2432983710169792  0.9071538860797882\n",
      "Batch:300 0.24523801234861214  0.9048077317078909\n",
      "Batch:350 0.24594865164586477  0.9039560835702078\n",
      "Batch:400 0.24590014100074767  0.9041346554458142\n",
      "Batch:450 0.2474136440621482  0.90337610801061\n",
      "Epoch:0 Loss:0.3044661101952908 Accuracy:0.8770230651645303 Loss:0.24705741808530943 Accuracy:0.9036942664650027\n",
      "Batch:50 0.2671711899340153  0.8938461911678314\n",
      "Batch:100 0.28376097925007343  0.8861538827419281\n",
      "Batch:150 0.2826329895357291  0.8857692658901215\n",
      "Batch:200 0.2795711968466639  0.8873077276349067\n",
      "Batch:250 0.27618470031023024  0.8895384972095489\n",
      "Batch:300 0.27477906475464503  0.8900000353654226\n",
      "Batch:350 0.2765575380197593  0.8881868481636047\n",
      "Batch:400 0.2735982510261238  0.8899519585072995\n",
      "Batch:450 0.27154169673720996  0.8905128558476766\n",
      "Epoch:1 Loss:0.2704156877128349\n",
      "Batch:50 0.22408585920929908  0.9173077309131622\n",
      "Batch:100 0.2204719516262412  0.9153846544027329\n",
      "Batch:150 0.21593221671879292  0.9167949104309082\n",
      "Batch:200 0.2137422042898834  0.9171154230833054\n",
      "Batch:250 0.21413159509003163  0.9163846542835236\n",
      "Batch:300 0.21718288611620665  0.91410260339578\n",
      "Batch:350 0.21790069055344377  0.9136813580989838\n",
      "Batch:400 0.21898031196556986  0.9126442703604698\n",
      "Batch:450 0.22078595591088135  0.9119658513863882\n",
      "Epoch:1 Loss:0.2704156877128349 Accuracy:0.8910403360695948 Loss:0.2205396724047755 Accuracy:0.9125700053950605\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(dltrain,dlvalid,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:50 0.25534069001674653  0.9026923477649689\n",
      "Batch:100 0.2483185590058565  0.9026923459768296\n",
      "Batch:150 0.24761820728580156  0.903461578687032\n",
      "Batch:200 0.25559049379080534  0.9004808086156845\n",
      "Batch:250 0.25234098213911055  0.9020769622325897\n",
      "Batch:300 0.254256089652578  0.9015384995937348\n",
      "Batch:350 0.2554201994410583  0.9002747632775988\n",
      "Batch:400 0.25505498334765436  0.900625037997961\n",
      "Batch:450 0.2536403034130732  0.9006410633193122\n",
      "Epoch:0 Loss:0.25531334081459445\n",
      "Batch:50 0.22183472752571107  0.9138461887836457\n",
      "Batch:100 0.22053509801626206  0.9146154218912125\n",
      "Batch:150 0.2157799701889356  0.9162820887565613\n",
      "Batch:200 0.21453345000743865  0.9173077312111855\n",
      "Batch:250 0.21779062893986703  0.9156154232025147\n",
      "Batch:300 0.22029575732847054  0.9142949110269547\n",
      "Batch:350 0.2213040920453412  0.9135714680807931\n",
      "Batch:400 0.22202144099399448  0.9135577318072319\n",
      "Batch:450 0.22457711691657703  0.9120513210031721\n",
      "Epoch:0 Loss:0.25531334081459445 Accuracy:0.8999200759459434 Loss:0.22522112516504317 Accuracy:0.9122501586925958\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(dltrain,dlvalid,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_group in learner.optimizer.param_groups:\n",
    "    param_group['lr']=5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:50 0.22361971259117128  0.9130769634246826\n",
      "Batch:100 0.2279772774502635  0.9100000381469726\n",
      "Batch:150 0.23540648189683755  0.9078205509980519\n",
      "Batch:200 0.22933210765942932  0.9100000396370888\n",
      "Batch:250 0.22959483601152897  0.9086154239177704\n",
      "Batch:300 0.22942450220386187  0.9090385007858276\n",
      "Batch:350 0.23072864936930793  0.908626412664141\n",
      "Batch:400 0.23218061966821552  0.9079808077216148\n",
      "Batch:450 0.2308107333050834  0.9082906368043687\n",
      "Epoch:0 Loss:0.23166188205787894\n",
      "Batch:50 0.20899285078048707  0.9211538803577423\n",
      "Batch:100 0.20655105393379927  0.9219231122732162\n",
      "Batch:150 0.20229740180075167  0.9221795233090718\n",
      "Batch:200 0.20147294705733657  0.9223077300190926\n",
      "Batch:250 0.20385502518713475  0.9214615762233734\n",
      "Batch:300 0.2055590144669016  0.9197436277071634\n",
      "Batch:350 0.20601352126470635  0.9195604772227151\n",
      "Batch:400 0.20609171100892126  0.9193269607424736\n",
      "Batch:450 0.20773702655401494  0.9180769610404969\n",
      "Epoch:0 Loss:0.23166188205787894 Accuracy:0.9081961046633255 Loss:0.2075896251117985 Accuracy:0.9185391390645826\n",
      "Batch:50 0.22303613841533662  0.9107692694664001\n",
      "Batch:100 0.21150447063148023  0.915000038743019\n",
      "Batch:150 0.21726797555883726  0.9133333718776703\n",
      "Batch:200 0.2208922815695405  0.9131731143593789\n",
      "Batch:250 0.2254516829252243  0.9120000371932984\n",
      "Batch:300 0.22550789932409923  0.9121154220898946\n",
      "Batch:350 0.22536051066858429  0.9106593787670135\n",
      "Batch:400 0.22371988628059625  0.9110096535086631\n",
      "Batch:450 0.22595518388681943  0.910170978307724\n",
      "Epoch:1 Loss:0.22601430141318612\n",
      "Batch:50 0.2078949797153473  0.9180769574642181\n",
      "Batch:100 0.206424116268754  0.91980772793293\n",
      "Batch:150 0.2011963610847791  0.9212820875644684\n",
      "Batch:200 0.200223328769207  0.9220192682743072\n",
      "Batch:250 0.20243693944811822  0.9197692680358887\n",
      "Batch:300 0.20410929836332797  0.9183974742889405\n",
      "Batch:350 0.20392461289252553  0.918626411982945\n",
      "Batch:400 0.2033417024090886  0.9187981151044369\n",
      "Batch:450 0.2049611973762512  0.9179060212771097\n",
      "Epoch:1 Loss:0.22601430141318612 Accuracy:0.9101151828954225 Loss:0.2043248949003195 Accuracy:0.9187390433775412\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(dltrain,dlvalid,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kirana/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type sentiment_classifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model_sentiment.state_dict(),f'{PATH}/inter/sentiment_model_awd_lstm_state_dict')\n",
    "torch.save(optimizer.state_dict(),f'{PATH}/inter/sentiment_learner_awd_lstm_state_dict')\n",
    "torch.save (model_sentiment,f'{PATH}/inter/sentiment_model_awd_lstm')\n",
    "torch.save (optimizer,f'{PATH}/inter/sentiment_optimizer_awd_lstm')\n",
    "torch.save (learner,f'{PATH}/inter/sentiment_learner_awd_lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sentiment.unfreeze_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sentiment.encoder.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:50 0.22543422520160675  0.9038461852073669\n",
      "Batch:100 0.2261739369481802  0.9050000375509262\n",
      "Batch:150 0.22177489136656126  0.9084615770975749\n",
      "Batch:200 0.21774300549179315  0.91182696133852\n",
      "Batch:250 0.21418449717760085  0.9142308073043823\n",
      "Batch:300 0.2120815611630678  0.915512859026591\n",
      "Batch:350 0.21183441687907492  0.9158791598251887\n",
      "Batch:400 0.21211563305929304  0.9160096538066864\n",
      "Batch:450 0.2127698947985967  0.915982944700453\n",
      "Epoch:0 Loss:0.21325347400628603\n",
      "Batch:50 0.20118361279368402  0.9200000381469726\n",
      "Batch:100 0.1982692200690508  0.9226923459768295\n",
      "Batch:150 0.19237979829311372  0.9258974746863047\n",
      "Batch:200 0.19060995291918517  0.9265385004878044\n",
      "Batch:250 0.19231294970214366  0.9246923460960388\n",
      "Batch:300 0.19382860192408163  0.9233974744876225\n",
      "Batch:350 0.19381565663431372  0.9236813570771899\n",
      "Batch:400 0.1936470889952034  0.9236058080196381\n",
      "Batch:450 0.19532251803411377  0.9233761072158814\n",
      "Epoch:0 Loss:0.21325347400628603 Accuracy:0.9155285851623313 Loss:0.19525870475569534 Accuracy:0.9241404511080958\n",
      "Batch:50 0.18648862034082414  0.9261538827419281\n",
      "Batch:100 0.18554193384945392  0.9263461941480636\n",
      "Batch:150 0.18314619809389115  0.9305128602186838\n",
      "Batch:200 0.17982689095661045  0.9317308089137077\n",
      "Batch:250 0.17730803287029268  0.9326154232025147\n",
      "Batch:300 0.17731388573845228  0.9314102939764659\n",
      "Batch:350 0.17689907732818808  0.9312637739522116\n",
      "Batch:400 0.17505811467766763  0.9324519601464272\n",
      "Batch:450 0.1761597459928857  0.9320513192812602\n",
      "Epoch:1 Loss:0.17663420489727832\n",
      "Batch:50 0.200001612752676  0.9242308056354522\n",
      "Batch:100 0.1980579476058483  0.926153883934021\n",
      "Batch:150 0.19166108315189678  0.9275641397635142\n",
      "Batch:200 0.18983667280524968  0.9275000381469727\n",
      "Batch:250 0.19105267161130904  0.9260769612789154\n",
      "Batch:300 0.1926532845447461  0.9246154226859411\n",
      "Batch:350 0.1931928017309734  0.9249450932230269\n",
      "Batch:400 0.1933495093137026  0.9245673462748527\n",
      "Batch:450 0.1949315866496828  0.9240598673290676\n",
      "Epoch:1 Loss:0.17663420489727832 Accuracy:0.9317408019440585 Loss:0.19512012232242157 Accuracy:0.924420316724916\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(dltrain,dlvalid,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_group in learner.optimizer.param_groups:\n",
    "    param_group['lr']=5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:50 0.14470430627465247  0.9461538827419281\n",
      "Batch:100 0.14433286335319281  0.9482692682743072\n",
      "Batch:150 0.1429171841343244  0.9467949100335439\n",
      "Batch:200 0.1421044604294002  0.9464423453807831\n",
      "Batch:250 0.14251119102537632  0.9461538834571839\n",
      "Batch:300 0.14152628174672524  0.9466026014089585\n",
      "Batch:350 0.14243143741573605  0.9463736629486084\n",
      "Batch:400 0.14166418856941163  0.9473558057844639\n",
      "Batch:450 0.14269747604098584  0.9475214038954841\n",
      "Epoch:0 Loss:0.1429498392755549\n",
      "Batch:50 0.202112774848938  0.9234615743160248\n",
      "Batch:100 0.20160498678684236  0.9244231134653091\n",
      "Batch:150 0.19512223991254965  0.9262820871671041\n",
      "Batch:200 0.19318435352295638  0.9266346520185471\n",
      "Batch:250 0.1949907910823822  0.9255384986400604\n",
      "Batch:300 0.19681865897029638  0.9241026016076406\n",
      "Batch:350 0.1977276628677334  0.9246154223169599\n",
      "Batch:400 0.19798897867091  0.9239423455297947\n",
      "Batch:450 0.19988020989629957  0.9234615763028463\n",
      "Epoch:0 Loss:0.1429498392755549 Accuracy:0.9472693470064667 Loss:0.20022502381997395 Accuracy:0.9238206041577948\n",
      "Batch:50 0.15703003279864788  0.9434615755081177\n",
      "Batch:100 0.1454189272597432  0.9478846514225006\n",
      "Batch:150 0.1429340380926927  0.948589779138565\n",
      "Batch:200 0.14386728819459677  0.9462500366568566\n",
      "Batch:250 0.14265739355236293  0.9467692677974701\n",
      "Batch:300 0.1388451432622969  0.9482051648696264\n",
      "Batch:350 0.1398877106553742  0.9477472899641309\n",
      "Batch:400 0.13954351146239788  0.9479327295720578\n",
      "Batch:450 0.13900717231548496  0.9483761050966051\n",
      "Epoch:1 Loss:0.13838099550819818\n",
      "Batch:50 0.20439906552433967  0.9253846502304077\n",
      "Batch:100 0.20441678665578367  0.9250000363588333\n",
      "Batch:150 0.19766689449548722  0.9261538827419281\n",
      "Batch:200 0.1957772189565003  0.9261538833379745\n",
      "Batch:250 0.19781020577251912  0.9250769605636596\n",
      "Batch:300 0.19962555479258298  0.9237179869413376\n",
      "Batch:350 0.20071166226906437  0.9241209173202515\n",
      "Batch:400 0.20102396958507598  0.9236538845300675\n",
      "Batch:450 0.2030836992048555  0.9234188417593638\n",
      "Epoch:1 Loss:0.13838099550819818 Accuracy:0.9483368354626852 Loss:0.20366664045720734 Accuracy:0.923660681178317\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(dltrain,dlvalid,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 0.5, 0.5)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.model.dropout, learner.model.dropout_o, learner.model.dropout_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to literal (<ipython-input-89-1eb3fb98cfd5>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-89-1eb3fb98cfd5>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    learner.model.dropout=0.75, learner.model.dropout_o=0.75, learner.model.dropout_e=0.2\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't assign to literal\n"
     ]
    }
   ],
   "source": [
    "learner.model.dropout=0.75, learner.model.dropout_o=0.75, learner.model.dropout_e=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
