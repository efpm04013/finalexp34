{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import os \n",
    "import fastai\n",
    "import fastai\n",
    "from fastai.text import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=\"/home/kirana/Documents/phd\"\n",
    "DATAPATH=\"/home/kirana/Documents/phd/data/experiment/cellphoneaccessory\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "[df_train,df_valid,itos, train_tokens, valid_tokens, trn_lm, val_lm]=pickle.load(open(f'{DATAPATH}/inter/dfs_tokens_fastai.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=52 # 52 - Jeremey, 20 - default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt= 70 #70 - Jeremey, 35 - default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_lm(tokens,bs):\n",
    "    import itertools\n",
    "    # Collapse into a single large array\n",
    "    tokens=np.asarray(list (itertools.chain(*tokens)))\n",
    "    # How many batches\n",
    "    n_batch=len(tokens)//bs\n",
    "    # Truncate to exclude the ones at the end\n",
    "    tokens=tokens[:bs*n_batch]\n",
    "    # Reshape\n",
    "    tokens=tokens.reshape(bs,-1)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52, 349220), (52, 38606))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens=create_data_lm(df_train['tokens'],bs)\n",
    "valid_tokens=create_data_lm(df_valid['tokens'],bs)\n",
    "train_tokens.shape, valid_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i):\n",
    "    seq_len = min(args.bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52, 70) (52, 70)\n"
     ]
    }
   ],
   "source": [
    "n_batch=train_tokens.shape[1]\n",
    "for i in range(0,n_batch,bptt):\n",
    "    seq_len=min(bptt,n_batch-1-i)\n",
    "    x=train_tokens[:,i:i+seq_len]\n",
    "    y=train_tokens[:,i+1:i+1+seq_len]\n",
    "    print (x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52, 70), (52, 70))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  18,   19,   20,   17, ...,   13,  128,   13,   10],\n",
       "        [ 109,   10,   53,    6, ...,   45,  501,  257,    3],\n",
       "        [   3,    2,   46,   26, ...,   44,   33, 2451,   10],\n",
       "        [   6,    5,  142,   53, ...,    2,  604,    6,   25],\n",
       "        ...,\n",
       "        [   2,   35,  746,    5, ...,  534,    3,    5,   56],\n",
       "        [ 113,   22,    9, 7475, ...,   38,    2,   70,   12],\n",
       "        [   7,    8,   26,   40, ...,    3,   18,   19,   20],\n",
       "        [   3,    2,  473,  134, ...,  515,  162,   80,   12]]),\n",
       " array([[  19,   20,   17,    2, ...,  128,   13,   10,   13],\n",
       "        [  10,   53,    6,  159, ...,  501,  257,    3,    2],\n",
       "        [   2,   46,   26,  167, ...,   33, 2451,   10,   56],\n",
       "        [   5,  142,   53,    9, ...,  604,    6,   25,   72],\n",
       "        ...,\n",
       "        [  35,  746,    5,  128, ...,    3,    5,   56,  422],\n",
       "        [  22,    9, 7475,  612, ...,    2,   70,   12,   23],\n",
       "        [   8,   26,   40,  547, ...,   18,   19,   20,   17],\n",
       "        [   2,  473,  134,   76, ...,  162,   80,   12,   11]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52, 70), (52, 70))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 349220)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-trained AWD-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_AWD_LSTM='/home/kirana/Documents/phd/data/pre-trained/awd_lstm/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bwd_wt103_enc.h5  fwd_wt103_enc.h5  itos_wt103.pkl  \u001b[0m\u001b[01;34mwt103_60002\u001b[0m/\r\n",
      "bwd_wt103.h5      fwd_wt103.h5      \u001b[01;34mwt103_238642\u001b[0m/   \u001b[01;31mwt103_tiny.tgz\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "ls {PATH_AWD_LSTM}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts = torch.load(f'{PATH_AWD_LSTM}/fwd_wt103.h5', map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.encoder.weight',\n",
       "              tensor([[-1.2274e-01,  2.7886e-01, -3.8850e-01,  ..., -1.0404e-01,\n",
       "                        1.9580e-02,  1.8548e-01],\n",
       "                      [ 1.4854e-05, -2.3424e-05,  1.9693e-05,  ...,  2.1349e-05,\n",
       "                        2.1776e-05, -1.2394e-05],\n",
       "                      [ 1.8070e-01,  1.5874e+00, -1.1738e-01,  ..., -4.5935e-02,\n",
       "                       -8.1352e-02,  1.8054e-01],\n",
       "                      ...,\n",
       "                      [-1.8595e-03, -6.8529e-03,  1.6999e-03,  ...,  1.7039e-03,\n",
       "                        4.1632e-03, -1.3171e-03],\n",
       "                      [-2.3120e-03, -6.9001e-03,  1.8772e-03,  ...,  5.0309e-04,\n",
       "                        4.6596e-03, -2.5850e-03],\n",
       "                      [-2.2463e-03, -9.1512e-03,  1.3927e-03,  ...,  1.2296e-03,\n",
       "                        5.8085e-03, -1.8940e-03]])),\n",
       "             ('0.encoder_with_dropout.embed.weight',\n",
       "              tensor([[-1.2274e-01,  2.7886e-01, -3.8850e-01,  ..., -1.0404e-01,\n",
       "                        1.9580e-02,  1.8548e-01],\n",
       "                      [ 1.4854e-05, -2.3424e-05,  1.9693e-05,  ...,  2.1349e-05,\n",
       "                        2.1776e-05, -1.2394e-05],\n",
       "                      [ 1.8070e-01,  1.5874e+00, -1.1738e-01,  ..., -4.5935e-02,\n",
       "                       -8.1352e-02,  1.8054e-01],\n",
       "                      ...,\n",
       "                      [-1.8595e-03, -6.8529e-03,  1.6999e-03,  ...,  1.7039e-03,\n",
       "                        4.1632e-03, -1.3171e-03],\n",
       "                      [-2.3120e-03, -6.9001e-03,  1.8772e-03,  ...,  5.0309e-04,\n",
       "                        4.6596e-03, -2.5850e-03],\n",
       "                      [-2.2463e-03, -9.1512e-03,  1.3927e-03,  ...,  1.2296e-03,\n",
       "                        5.8085e-03, -1.8940e-03]])),\n",
       "             ('0.rnns.0.module.weight_ih_l0',\n",
       "              tensor([[-0.0812, -0.0811, -0.0937,  ..., -0.0259, -0.1403, -0.3247],\n",
       "                      [ 0.1154,  0.1142,  0.0938,  ..., -0.0711,  0.1669, -0.0387],\n",
       "                      [-0.0051,  0.1007,  0.2071,  ..., -0.0860, -0.0288, -0.0894],\n",
       "                      ...,\n",
       "                      [ 0.0055,  0.0157,  0.2990,  ...,  0.0616,  0.1159, -0.4737],\n",
       "                      [ 0.0181,  0.0426,  0.1130,  ...,  0.3529, -0.0114, -0.0125],\n",
       "                      [-0.0167, -0.1328,  0.1741,  ...,  0.0548, -0.0045,  0.1688]])),\n",
       "             ('0.rnns.0.module.bias_ih_l0',\n",
       "              tensor([ 0.1503, -0.4701, -0.1885,  ..., -0.5919, -0.2172, -0.1207])),\n",
       "             ('0.rnns.0.module.bias_hh_l0',\n",
       "              tensor([ 0.1503, -0.4701, -0.1885,  ..., -0.5919, -0.2172, -0.1207])),\n",
       "             ('0.rnns.0.module.weight_hh_l0_raw',\n",
       "              tensor([[-0.1013,  0.1786, -0.0528,  ...,  0.0741,  0.0306,  0.2467],\n",
       "                      [ 0.1780, -0.0853, -0.0243,  ..., -0.1129, -0.1310, -0.1498],\n",
       "                      [ 0.0661, -0.0496,  0.0921,  ...,  0.1829,  0.0533, -0.1525],\n",
       "                      ...,\n",
       "                      [-0.0322, -0.0704,  0.1653,  ...,  0.2142, -0.0558,  0.0315],\n",
       "                      [-0.1651, -0.0290,  0.1748,  ..., -0.0446,  0.5444,  0.0616],\n",
       "                      [ 0.0905, -0.1704, -0.0053,  ..., -0.0057,  0.2269,  0.0328]])),\n",
       "             ('0.rnns.1.module.weight_ih_l0',\n",
       "              tensor([[ 0.3307,  0.0385,  0.0860,  ...,  0.0685, -0.0444,  0.0539],\n",
       "                      [ 0.0720,  0.1607,  0.0562,  ...,  0.0276,  0.0613,  0.1632],\n",
       "                      [-0.1565, -0.1168,  0.1897,  ..., -0.0357,  0.0296,  0.0961],\n",
       "                      ...,\n",
       "                      [-0.0897, -0.1464, -0.0760,  ...,  0.0536,  0.0422, -0.0580],\n",
       "                      [ 0.1166, -0.1534, -0.1784,  ..., -0.0689,  0.2170,  0.1461],\n",
       "                      [-0.0413,  0.0689,  0.0581,  ..., -0.0640, -0.1703, -0.0945]])),\n",
       "             ('0.rnns.1.module.bias_ih_l0',\n",
       "              tensor([-0.8577, -0.6784, -0.7249,  ..., -0.6782,  0.0567, -0.5026])),\n",
       "             ('0.rnns.1.module.bias_hh_l0',\n",
       "              tensor([-0.8577, -0.6784, -0.7249,  ..., -0.6782,  0.0567, -0.5026])),\n",
       "             ('0.rnns.1.module.weight_hh_l0_raw',\n",
       "              tensor([[-0.0273, -0.2277,  0.0782,  ...,  0.1355, -0.1282,  0.1669],\n",
       "                      [ 0.1218,  0.0017, -0.0998,  ..., -0.2085, -0.0686, -0.1389],\n",
       "                      [-0.3878, -0.0498, -0.1748,  ..., -0.4014,  0.1986, -0.4400],\n",
       "                      ...,\n",
       "                      [-0.2097, -0.4298,  0.3551,  ...,  0.0316, -0.1198,  0.1266],\n",
       "                      [ 0.0037, -0.0223,  0.0032,  ..., -0.2672, -0.3093, -0.0361],\n",
       "                      [-0.0464,  0.1664, -0.1348,  ...,  0.1600, -0.1138,  0.0845]])),\n",
       "             ('0.rnns.2.module.weight_ih_l0',\n",
       "              tensor([[-0.0741,  0.0447, -0.0744,  ..., -0.0419,  0.1600, -0.0553],\n",
       "                      [ 0.0270,  0.0118,  0.0449,  ...,  0.1165, -0.1080, -0.0681],\n",
       "                      [-0.1023, -0.1662, -0.0229,  ...,  0.1652, -0.1070,  0.0970],\n",
       "                      ...,\n",
       "                      [-0.0989, -0.4425, -0.0343,  ..., -0.1434,  0.5851, -0.0291],\n",
       "                      [ 0.0802, -0.1067,  0.2789,  ..., -0.0916, -0.2240,  0.1020],\n",
       "                      [-0.4078,  0.7220,  0.1142,  ...,  0.5287,  0.2035, -0.1811]])),\n",
       "             ('0.rnns.2.module.bias_ih_l0',\n",
       "              tensor([-0.3681, -0.9079, -0.1998,  ...,  0.8533,  0.3202,  1.2172])),\n",
       "             ('0.rnns.2.module.bias_hh_l0',\n",
       "              tensor([-0.3681, -0.9079, -0.1998,  ...,  0.8533,  0.3202,  1.2172])),\n",
       "             ('0.rnns.2.module.weight_hh_l0_raw',\n",
       "              tensor([[-0.0966,  0.0236, -0.0152,  ...,  0.0388, -0.0531, -0.0395],\n",
       "                      [-0.0328, -0.2217,  0.0028,  ...,  0.0143, -0.0368, -0.0085],\n",
       "                      [ 0.0167, -0.0081, -0.0561,  ...,  0.0125,  0.0442, -0.0139],\n",
       "                      ...,\n",
       "                      [-0.0212, -0.1034, -0.0106,  ..., -0.0561,  0.0200, -0.0157],\n",
       "                      [ 0.0183,  0.0364, -0.0251,  ..., -0.0240, -0.1150,  0.0046],\n",
       "                      [ 0.0100, -0.1824,  0.1076,  ..., -0.0269,  0.2733,  0.1846]])),\n",
       "             ('1.decoder.weight',\n",
       "              tensor([[-1.2274e-01,  2.7886e-01, -3.8850e-01,  ..., -1.0404e-01,\n",
       "                        1.9580e-02,  1.8548e-01],\n",
       "                      [ 1.4854e-05, -2.3424e-05,  1.9693e-05,  ...,  2.1349e-05,\n",
       "                        2.1776e-05, -1.2394e-05],\n",
       "                      [ 1.8070e-01,  1.5874e+00, -1.1738e-01,  ..., -4.5935e-02,\n",
       "                       -8.1352e-02,  1.8054e-01],\n",
       "                      ...,\n",
       "                      [-1.8595e-03, -6.8529e-03,  1.6999e-03,  ...,  1.7039e-03,\n",
       "                        4.1632e-03, -1.3171e-03],\n",
       "                      [-2.3120e-03, -6.9001e-03,  1.8772e-03,  ...,  5.0309e-04,\n",
       "                        4.6596e-03, -2.5850e-03],\n",
       "                      [-2.2463e-03, -9.1512e-03,  1.3927e-03,  ...,  1.2296e-03,\n",
       "                        5.8085e-03, -1.8940e-03]]))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts = torch.load(f'{PATH_AWD_LSTM}/fwd_wt103_enc.h5', map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('encoder.weight',\n",
       "              tensor([[-1.2274e-01,  2.7886e-01, -3.8850e-01,  ..., -1.0404e-01,\n",
       "                        1.9580e-02,  1.8548e-01],\n",
       "                      [ 1.4854e-05, -2.3424e-05,  1.9693e-05,  ...,  2.1349e-05,\n",
       "                        2.1776e-05, -1.2394e-05],\n",
       "                      [ 1.8070e-01,  1.5874e+00, -1.1738e-01,  ..., -4.5935e-02,\n",
       "                       -8.1352e-02,  1.8054e-01],\n",
       "                      ...,\n",
       "                      [-1.8595e-03, -6.8529e-03,  1.6999e-03,  ...,  1.7039e-03,\n",
       "                        4.1632e-03, -1.3171e-03],\n",
       "                      [-2.3120e-03, -6.9001e-03,  1.8772e-03,  ...,  5.0309e-04,\n",
       "                        4.6596e-03, -2.5850e-03],\n",
       "                      [-2.2463e-03, -9.1512e-03,  1.3927e-03,  ...,  1.2296e-03,\n",
       "                        5.8085e-03, -1.8940e-03]])),\n",
       "             ('encoder_with_dropout.embed.weight',\n",
       "              tensor([[-1.2274e-01,  2.7886e-01, -3.8850e-01,  ..., -1.0404e-01,\n",
       "                        1.9580e-02,  1.8548e-01],\n",
       "                      [ 1.4854e-05, -2.3424e-05,  1.9693e-05,  ...,  2.1349e-05,\n",
       "                        2.1776e-05, -1.2394e-05],\n",
       "                      [ 1.8070e-01,  1.5874e+00, -1.1738e-01,  ..., -4.5935e-02,\n",
       "                       -8.1352e-02,  1.8054e-01],\n",
       "                      ...,\n",
       "                      [-1.8595e-03, -6.8529e-03,  1.6999e-03,  ...,  1.7039e-03,\n",
       "                        4.1632e-03, -1.3171e-03],\n",
       "                      [-2.3120e-03, -6.9001e-03,  1.8772e-03,  ...,  5.0309e-04,\n",
       "                        4.6596e-03, -2.5850e-03],\n",
       "                      [-2.2463e-03, -9.1512e-03,  1.3927e-03,  ...,  1.2296e-03,\n",
       "                        5.8085e-03, -1.8940e-03]])),\n",
       "             ('rnns.0.module.weight_ih_l0',\n",
       "              tensor([[-0.0812, -0.0811, -0.0937,  ..., -0.0259, -0.1403, -0.3247],\n",
       "                      [ 0.1154,  0.1142,  0.0938,  ..., -0.0711,  0.1669, -0.0387],\n",
       "                      [-0.0051,  0.1007,  0.2071,  ..., -0.0860, -0.0288, -0.0894],\n",
       "                      ...,\n",
       "                      [ 0.0055,  0.0157,  0.2990,  ...,  0.0616,  0.1159, -0.4737],\n",
       "                      [ 0.0181,  0.0426,  0.1130,  ...,  0.3529, -0.0114, -0.0125],\n",
       "                      [-0.0167, -0.1328,  0.1741,  ...,  0.0548, -0.0045,  0.1688]])),\n",
       "             ('rnns.0.module.bias_ih_l0',\n",
       "              tensor([ 0.1503, -0.4701, -0.1885,  ..., -0.5919, -0.2172, -0.1207])),\n",
       "             ('rnns.0.module.bias_hh_l0',\n",
       "              tensor([ 0.1503, -0.4701, -0.1885,  ..., -0.5919, -0.2172, -0.1207])),\n",
       "             ('rnns.0.module.weight_hh_l0_raw',\n",
       "              tensor([[-0.1013,  0.1786, -0.0528,  ...,  0.0741,  0.0306,  0.2467],\n",
       "                      [ 0.1780, -0.0853, -0.0243,  ..., -0.1129, -0.1310, -0.1498],\n",
       "                      [ 0.0661, -0.0496,  0.0921,  ...,  0.1829,  0.0533, -0.1525],\n",
       "                      ...,\n",
       "                      [-0.0322, -0.0704,  0.1653,  ...,  0.2142, -0.0558,  0.0315],\n",
       "                      [-0.1651, -0.0290,  0.1748,  ..., -0.0446,  0.5444,  0.0616],\n",
       "                      [ 0.0905, -0.1704, -0.0053,  ..., -0.0057,  0.2269,  0.0328]])),\n",
       "             ('rnns.1.module.weight_ih_l0',\n",
       "              tensor([[ 0.3307,  0.0385,  0.0860,  ...,  0.0685, -0.0444,  0.0539],\n",
       "                      [ 0.0720,  0.1607,  0.0562,  ...,  0.0276,  0.0613,  0.1632],\n",
       "                      [-0.1565, -0.1168,  0.1897,  ..., -0.0357,  0.0296,  0.0961],\n",
       "                      ...,\n",
       "                      [-0.0897, -0.1464, -0.0760,  ...,  0.0536,  0.0422, -0.0580],\n",
       "                      [ 0.1166, -0.1534, -0.1784,  ..., -0.0689,  0.2170,  0.1461],\n",
       "                      [-0.0413,  0.0689,  0.0581,  ..., -0.0640, -0.1703, -0.0945]])),\n",
       "             ('rnns.1.module.bias_ih_l0',\n",
       "              tensor([-0.8577, -0.6784, -0.7249,  ..., -0.6782,  0.0567, -0.5026])),\n",
       "             ('rnns.1.module.bias_hh_l0',\n",
       "              tensor([-0.8577, -0.6784, -0.7249,  ..., -0.6782,  0.0567, -0.5026])),\n",
       "             ('rnns.1.module.weight_hh_l0_raw',\n",
       "              tensor([[-0.0273, -0.2277,  0.0782,  ...,  0.1355, -0.1282,  0.1669],\n",
       "                      [ 0.1218,  0.0017, -0.0998,  ..., -0.2085, -0.0686, -0.1389],\n",
       "                      [-0.3878, -0.0498, -0.1748,  ..., -0.4014,  0.1986, -0.4400],\n",
       "                      ...,\n",
       "                      [-0.2097, -0.4298,  0.3551,  ...,  0.0316, -0.1198,  0.1266],\n",
       "                      [ 0.0037, -0.0223,  0.0032,  ..., -0.2672, -0.3093, -0.0361],\n",
       "                      [-0.0464,  0.1664, -0.1348,  ...,  0.1600, -0.1138,  0.0845]])),\n",
       "             ('rnns.2.module.weight_ih_l0',\n",
       "              tensor([[-0.0741,  0.0447, -0.0744,  ..., -0.0419,  0.1600, -0.0553],\n",
       "                      [ 0.0270,  0.0118,  0.0449,  ...,  0.1165, -0.1080, -0.0681],\n",
       "                      [-0.1023, -0.1662, -0.0229,  ...,  0.1652, -0.1070,  0.0970],\n",
       "                      ...,\n",
       "                      [-0.0989, -0.4425, -0.0343,  ..., -0.1434,  0.5851, -0.0291],\n",
       "                      [ 0.0802, -0.1067,  0.2789,  ..., -0.0916, -0.2240,  0.1020],\n",
       "                      [-0.4078,  0.7220,  0.1142,  ...,  0.5287,  0.2035, -0.1811]])),\n",
       "             ('rnns.2.module.bias_ih_l0',\n",
       "              tensor([-0.3681, -0.9079, -0.1998,  ...,  0.8533,  0.3202,  1.2172])),\n",
       "             ('rnns.2.module.bias_hh_l0',\n",
       "              tensor([-0.3681, -0.9079, -0.1998,  ...,  0.8533,  0.3202,  1.2172])),\n",
       "             ('rnns.2.module.weight_hh_l0_raw',\n",
       "              tensor([[-0.0966,  0.0236, -0.0152,  ...,  0.0388, -0.0531, -0.0395],\n",
       "                      [-0.0328, -0.2217,  0.0028,  ...,  0.0143, -0.0368, -0.0085],\n",
       "                      [ 0.0167, -0.0081, -0.0561,  ...,  0.0125,  0.0442, -0.0139],\n",
       "                      ...,\n",
       "                      [-0.0212, -0.1034, -0.0106,  ..., -0.0561,  0.0200, -0.0157],\n",
       "                      [ 0.0183,  0.0364, -0.0251,  ..., -0.0240, -0.1150,  0.0046],\n",
       "                      [ 0.0100, -0.1824,  0.1076,  ..., -0.0269,  0.2733,  0.1846]]))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos2=pickle.load(open(f'{PATH_AWD_LSTM}/itos_wt103.pkl','rb'))\n",
    "stoi2 = collections.defaultdict(lambda: -1, { v: k for k, v in enumerate(itos2) })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([238462, 400])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgts['encoder.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_wgts = wgts['encoder.weight'].numpy() # converts np.ndarray from torch.FloatTensor.output shape: (238462, 400)\n",
    "row_m = enc_wgts.mean(0) # returns the average of the array elements along axis 0. output shape: (400,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((238462, 400), 30238)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_wgts.shape, len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_emb=enc_wgts.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_w = np.zeros((len(itos),n_emb), dtype=np.float32) # shape: (60002, 400)\n",
    "\n",
    "for i, w in enumerate(itos):\n",
    "    r = stoi2[w]\n",
    "    new_w[i] = enc_wgts[r] if r >= 0 else row_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9402"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(itos).difference(set(itos2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inp=len(itos)\n",
    "n_emb=400 #650\n",
    "n_hidden=400 #650\n",
    "n_layers=2\n",
    "dropout=0.5\n",
    "wd=1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class language_model (nn.Module):\n",
    "    def __init__(self,n_inp,n_emb,n_hidden,n_layers,bidirectional,bs,device,dropout_e=0.05,dropout=0.5,\\\n",
    "                 dropout_o=0.5,pretrain_mtx=None,adaptive_log_softmax=True,tie_weights=True):\n",
    "        super().__init__()\n",
    "        self.n_inp,self.n_emb,self.n_hidden,self.n_layers,self.bidirectional,self.bs,self.device,self.pretrain_mtx=\\\n",
    "                            n_inp,n_emb,n_hidden,n_layers,bidirectional,bs,device,pretrain_mtx\n",
    "        self.adaptive_log_softmax,self.tie_weights=adaptive_log_softmax,tie_weights\n",
    "        self.dropout_e,self.dropout,self.dropout_o=dropout_e,dropout,dropout_o\n",
    "        self.gen_hidden()\n",
    "        self.create_architecture()\n",
    "        if pretrain_mtx is not None:\n",
    "            print (\"initializing\")\n",
    "            self.initialize_glove()\n",
    "            \n",
    "        if self.adaptive_log_softmax is False:\n",
    "            self.criterion=nn.CrossEntropyLoss()\n",
    "        \n",
    "    def create_architecture(self):\n",
    "        # Dropout layer\n",
    "        self.dropout_enc=nn.Dropout(self.dropout_e)\n",
    "        # Embedding Layer\n",
    "        self.encoder=nn.Embedding(self.n_inp,self.n_emb)\n",
    "        # LSTM Layer\n",
    "        self.lstm=nn.LSTM(self.n_emb,self.n_hidden,self.n_layers,batch_first=True,dropout=self.dropout,\\\n",
    "                          bidirectional=False)\n",
    "        self.dropout_op=nn.Dropout(self.dropout_o)\n",
    "        \n",
    "        if self.adaptive_log_softmax:\n",
    "            # Adaptive Log Softmax Loss\n",
    "            self.adaptive_softmax=AdaptiveLogSoftmaxWithLoss(self.n_hidden,\n",
    "                                    self.n_inp,\n",
    "                                    cutoffs=[round(self.n_inp/15),3*round(self.n_inp/15)],\n",
    "                                    div_value=4,\n",
    "                                    get_full_prob=True)\n",
    "        else:\n",
    "            self.decoder=nn.Linear(self.n_hidden,self.n_inp)\n",
    "    \n",
    "    def freeze_embedding(self):\n",
    "        self.encoder.weight.requires_grad=False\n",
    "        if self.tie_weights:\n",
    "            self.decoder.weight.requires_grad=False\n",
    "        \n",
    "    \n",
    "    def unfreeze_embedding(self):\n",
    "        self.encoder.weight.requires_grad=True\n",
    "        if self.tie_weights:\n",
    "            self.decoder.weight.requires_grad=True\n",
    "        \n",
    "    def initialize_glove(self):\n",
    "        self.encoder.weight.data=torch.Tensor(self.pretrain_mtx)\n",
    "        if self.tie_weights:\n",
    "            self.decoder.weight=self.encoder.weight\n",
    "    \n",
    "    def gen_hidden(self):\n",
    "        # Initialize hidden\n",
    "        self.hidden=(Variable(torch.zeros(self.n_layers,self.bs,self.n_hidden,requires_grad=False).to(self.device)),\n",
    "                     Variable(torch.zeros(self.n_layers,self.bs,self.n_hidden,requires_grad=False).to(self.device)))\n",
    "    \n",
    "        \n",
    "    def forward(self,Xb,Yb):\n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        embs=self.dropout_enc(self.encoder(Xb))\n",
    "        if Xb.size(0) < self.bs:\n",
    "            self.hidden=(self.hidden[0][:,:Xb.size(0),:].contiguous(),\n",
    "            self.hidden[1][:,:Xb.size(0),:].contiguous())\n",
    "        out,new_hidden=self.lstm(embs,self.hidden)\n",
    "        out=self.dropout_op(out)\n",
    "         # Wrap the hidden state in a new tensor without the gradients\n",
    "        self.hidden=(Variable(new_hidden[0].data,requires_grad=False).to(self.device),\\\n",
    "                     Variable(new_hidden[1].data,requires_grad=False).to(self.device))\n",
    "        if self.adaptive_log_softmax:\n",
    "            out=out.reshape(out.size(0)*out.size(1),out.size(2))        # output is of shape n_batch * n_seq * n_hidden\n",
    "      \n",
    "            out=self.adaptive_softmax(out,Yb.view(-1))\n",
    "            loss=out.loss\n",
    "            preds=out.output_full\n",
    "        else:\n",
    "            #import pdb\n",
    "            #pdb.set_trace()\n",
    "            preds=self.decoder(out.contiguous().view(out.size(0)*out.size(1), out.size(2)))\n",
    "            loss=self.criterion(preds,Yb.contiguous().view(-1))\n",
    "\n",
    "        return preds, loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_multinomial(preds,actual):\n",
    "    preds=preds.max(1)[1]\n",
    "    correct=preds==actual\n",
    "    return correct.float().sum()/len(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing\n"
     ]
    }
   ],
   "source": [
    "device=\"cuda:0\"\n",
    "model=language_model(n_inp,n_emb,n_hidden,n_layers,False,bs,device,0.05,0.5,0.5,new_w,False,True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30238, 400)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([30238, 400]), torch.Size([30238, 400]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.weight.data.shape,model.decoder.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([30238, 400]), 400, 30238)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Linear(n_hidden,n_inp).weight.data.shape, n_hidden, n_inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test if model forward works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([30238, 400]), torch.Size([30238, 400]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder.weight.shape,model.encoder.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 70)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1==0:\n",
    "    model.forward(torch.LongTensor(x),torch.LongTensor(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self,model,optimizer,metric_fn,device,bptt=12,print_every=5,clip_val=None):\n",
    "        self.model,self.optimizer,self.metric_fn,self.device,self.print_every,self.bptt,self.losses,self.clip_val=\\\n",
    "            model,optimizer,metric_fn,device,print_every,bptt,[],clip_val\n",
    "        self.n_epochs=1\n",
    "\n",
    "    \n",
    "    def fit (self,Xb,Yb,mode_train=True):\n",
    "        if mode_train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            \n",
    "        preds,loss=self.model(Xb,Yb)\n",
    "        \n",
    "       \n",
    "            \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            acc=self.metric_fn(preds,Yb.view(-1))\n",
    "            acc=acc.item()\n",
    "            del preds\n",
    "        \n",
    "        if mode_train:\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        myloss=loss.item()\n",
    "        del loss\n",
    "        \n",
    "        if self.clip_val is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.25)\n",
    "            if 1==0:\n",
    "                for p in self.model.parameters():\n",
    "                    p.data.add_(self.lr, p.grad.data)\n",
    "        \n",
    "        return myloss, acc\n",
    "    \n",
    "    def lr_find (self,start_lr,end_lr,iterator,n_batch):\n",
    "        losses,lrs=[],[]\n",
    "        ratio=end_lr/start_lr\n",
    "        num_steps=n_batch\n",
    "        lr=start_lr\n",
    "        for i in range(num_steps):            \n",
    "            lr=lr*(end_lr/start_lr)**(1/num_steps)\n",
    "            lrs.append(lr)\n",
    "        self.lrs=lrs\n",
    "        self.run_epoch(iterator,mode_train=True,lrs=lrs)\n",
    "    \n",
    "    def run_epoch(self,iterator,mode_train,lrs=None):\n",
    "        n_batch=iterator.shape[1]\n",
    "        epoch_loss,epoch_acc,i,k=0,0,0,0\n",
    "        self.model.gen_hidden()\n",
    "        #for k,i in enumerate(range(0,n_batch,self.bptt)):\n",
    "        n_batch=iterator.shape[1]\n",
    "        while i<n_batch-bptt:\n",
    "            if mode_train:\n",
    "                cust_bptt=self.bptt if np.random.random() < 0.95 else self.bptt//np.random.randint (2,4)\n",
    "            else:\n",
    "                cust_bptt=bptt\n",
    "            seq_len=min(cust_bptt,n_batch-1-i)\n",
    "            Xb=train_tokens[:,i:i+seq_len]\n",
    "            Yb=train_tokens[:,i+1:i+1+seq_len]\n",
    "            Xb=torch.LongTensor(Xb)\n",
    "            Yb=torch.LongTensor(Yb)\n",
    "            Xb=Xb.to(self.device)\n",
    "            Yb=Yb.to(self.device)\n",
    "            \n",
    "            if lrs is not None:\n",
    "                lr=lrs[k]\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr']=lr \n",
    "            \n",
    "\n",
    "            loss,acc=self.fit(Xb,Yb,mode_train)\n",
    "            \n",
    "            if lrs is not None:\n",
    "                self.losses.append(loss)\n",
    "            \n",
    "            \n",
    "            epoch_loss+=loss\n",
    "            epoch_acc+=acc\n",
    "            if k%self.print_every == 0:\n",
    "                if k:\n",
    "                    print (f'Batch:{k} {epoch_loss/(k)}  {epoch_acc/(k)}')  \n",
    "                    torch.cuda.empty_cache()\n",
    "            k=k+1\n",
    "            i=i+seq_len\n",
    "        epoch_loss=epoch_loss/k\n",
    "        epoch_acc=epoch_acc/k\n",
    "        \n",
    "        if 1==0:\n",
    "            lr /= 4.0\n",
    "            # Freeze all the layers initially\n",
    "            for param in resnet.parameters():\n",
    "                param.requires_grad=False\n",
    "            torch.save(resnet,'resnet')\n",
    "            torch.save(resnet.state_dict(),'resnet_state_dict')\n",
    "            resnet.load_state_dict(torch.load('resnet_state_dict'))\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr']=lr  \n",
    "            \n",
    "        return epoch_loss,epoch_acc\n",
    "    \n",
    "    def plot_lrs(self, n_roll=1):\n",
    "        import seaborn as sns\n",
    "        ax=sns.lineplot(x=self.lrs,y=pd.Series(self.losses).rolling(n_roll).mean())\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.set_xlabel('Learning Rate')\n",
    "\n",
    "    \n",
    "    def run_epochs(self,dltrain,dlvalid,n_epochs=1):\n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            loss,acc=self.run_epoch(dltrain,True)\n",
    "            print (f'Epoch:{epoch} Loss:{loss}')\n",
    "            lossv,accv=self.run_epoch(dlvalid,mode_train=False)\n",
    "            print (f'Epoch:{epoch} Loss:{loss} Accuracy:{acc} Loss:{lossv} Accuracy:{accv}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(model.parameters(),lr=1e-3,betas=(0.9,0.999), weight_decay=wd)\n",
    "metric_fn=accuracy_multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4989"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_batch=np.int(np.ceil(train_tokens.shape[1]/bptt))\n",
    "n_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.weight.requires_grad, model.decoder.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner=Learner(model,optimizer,accuracy_multinomial,device,bptt,10000,0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30238"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 52)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tokens), len(valid_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155700, 5)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Loss:5.044906149873117\n",
      "Epoch:0 Loss:5.044906149873117 Accuracy:0.18903325404318533 Loss:4.282061222893357 Accuracy:0.23383709174427061\n",
      "Epoch:1 Loss:4.526315976247609\n",
      "Epoch:1 Loss:4.526315976247609 Accuracy:0.22384560171433512 Loss:4.143852847890282 Accuracy:0.26221107503571656\n",
      "Epoch:2 Loss:4.4497637319444285\n",
      "Epoch:2 Loss:4.4497637319444285 Accuracy:0.2324531795089402 Loss:4.102019507309486 Accuracy:0.2675963884689414\n",
      "Epoch:3 Loss:4.414959174364445\n",
      "Epoch:3 Loss:4.414959174364445 Accuracy:0.23626207181826414 Loss:4.077250600942899 Accuracy:0.2702838100506476\n",
      "Epoch:4 Loss:4.393212403801917\n",
      "Epoch:4 Loss:4.393212403801917 Accuracy:0.23870510033140763 Loss:4.058779211961639 Accuracy:0.27272293223118826\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(train_tokens,valid_tokens,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n"
     ]
    }
   ],
   "source": [
    "for param_group in optimizer.param_groups:\n",
    "    print(param_group['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr']=5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Loss:4.360720880731833\n",
      "Epoch:0 Loss:4.360720880731833 Accuracy:0.24227506387270908 Loss:4.0287062161198115 Accuracy:0.27611087903569703\n",
      "Epoch:1 Loss:4.348983817733462\n",
      "Epoch:1 Loss:4.348983817733462 Accuracy:0.24362821396731738 Loss:4.016934303969091 Accuracy:0.27768244782290313\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(train_tokens,valid_tokens,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr']=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Loss:4.330928662887443\n",
      "Epoch:0 Loss:4.330928662887443 Accuracy:0.24560643849868333 Loss:4.002791845214345 Accuracy:0.2794270278537771\n",
      "Epoch:1 Loss:4.324945534839763\n",
      "Epoch:1 Loss:4.324945534839763 Accuracy:0.24615321052405312 Loss:3.9981312448878903 Accuracy:0.27978651466257126\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(train_tokens,valid_tokens,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),f'{DATAPATH}/inter/varybptt_model_state_dict')\n",
    "torch.save(optimizer.state_dict(),f'{DATAPATH}/inter/varybptt_learner_state_dict')\n",
    "torch.save (model,f'{DATAPATH}/inter/varybptt_model_awd_lstm')\n",
    "torch.save (optimizer,f'{DATAPATH}/inter/varybptt_optimizer_awd_lstm')\n",
    "torch.save (learner,f'{DATAPATH}/inter/varybptt_learner_awd_lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_lm_weights=model.encoder.weight.data.cpu().numpy()\n",
    "import pickle\n",
    "pickle.dump(pretrained_lm_weights,open(f'{DATAPATH}/inter/varybpttpretrained_lm_weights_freeze','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.unfreeze_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.weight.requires_grad, model.decoder.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr']=5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Loss:3.98885220368331\n",
      "Epoch:0 Loss:3.98885220368331 Accuracy:0.28449018054425856 Loss:3.5798200098442727 Accuracy:0.32061139024969887\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(train_tokens,valid_tokens,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.51659315162847"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(3.57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Loss:3.817732463857879\n",
      "Epoch:0 Loss:3.817732463857879 Accuracy:0.29453458813161904 Loss:3.4959105774625026 Accuracy:0.32534454251806877\n",
      "Epoch:1 Loss:3.748959439836508\n",
      "Epoch:1 Loss:3.748959439836508 Accuracy:0.2986992632972024 Loss:3.4480200891269748 Accuracy:0.32821594527542264\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(train_tokens,valid_tokens,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_lm_weights=model.encoder.weight.data.cpu().numpy()\n",
    "import pickle\n",
    "pickle.dump(pretrained_lm_weights,open(f'{DATAPATH}/inter/varybpttpretrained_lm_weights_freeze','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Loss:3.7068574866299864\n",
      "Epoch:0 Loss:3.7068574866299864 Accuracy:0.3014067806663943 Loss:3.4144628008999973 Accuracy:0.33059024902740103\n",
      "Epoch:1 Loss:3.676902432195632\n",
      "Epoch:1 Loss:3.676902432195632 Accuracy:0.30347215821703544 Loss:3.3901200458921235 Accuracy:0.3325896112217877\n",
      "Epoch:2 Loss:3.654093524802672\n",
      "Epoch:2 Loss:3.654093524802672 Accuracy:0.30511261368181575 Loss:3.369099552099155 Accuracy:0.3340689390627312\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(train_tokens,valid_tokens,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_lm_weights=model.encoder.weight.data.cpu().numpy()\n",
    "import pickle\n",
    "pickle.dump(pretrained_lm_weights,open(f'{DATAPATH}/inter/varybpttpretrained_lm_weights_unfreeze','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr']=7e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),f'{DATAPATH}/inter/varybptt_model_state_dict_nofreeze')\n",
    "torch.save(optimizer.state_dict(),f'{DATAPATH}/inter/varybptt_learner_state_dict_nofreeze')\n",
    "torch.save (model,f'{DATAPATH}/inter/varybptt_model_awd_lstm_nofreeze')\n",
    "torch.save (optimizer,f'{DATAPATH}/inter/varybptt_optimizer_awd_lstm_nofreeze')\n",
    "torch.save (learner,f'{DATAPATH}/inter/varybptt_learner_awd_lstm_nofreeze')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Loss:3.6463532880753693\n",
      "Epoch:0 Loss:3.6463532880753693 Accuracy:0.30517356850167615 Loss:3.3609841917041425 Accuracy:0.3342683771236406\n",
      "Epoch:1 Loss:3.6297835546240056\n",
      "Epoch:1 Loss:3.6297835546240056 Accuracy:0.30645713239550265 Loss:3.345677702915863 Accuracy:0.3354450587568612\n",
      "Epoch:2 Loss:3.614953364363164\n",
      "Epoch:2 Loss:3.614953364363164 Accuracy:0.3076373416726818 Loss:3.332489339063475 Accuracy:0.3365594171352698\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(train_tokens,valid_tokens,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36.966052814822504, 27.938341703236507)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(3.61),np.exp(3.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),f'{DATAPATH}/inter/varybptt_model_state_dict_unfreeze')\n",
    "torch.save(optimizer.state_dict(),f'{DATAPATH}/inter/varybptt_learner_state_dict_unfreeze')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save (model,f'{DATAPATH}/inter/varybptt_model_awd_lstm')\n",
    "torch.save (optimizer,f'{DATAPATH}/inter/varybptt_optimizer_awd_lstm')\n",
    "torch.save (learner,f'{DATAPATH}/inter/varybptt_learner_awd_lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_lm_weights=model.encoder.weight.data.cpu().numpy()\n",
    "import pickle\n",
    "pickle.dump(pretrained_lm_weights,open(f'{DATAPATH}/inter/varybpttpretrained_lm_weights_unfreeze','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
