{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import os \n",
    "import fastai\n",
    "import fastai\n",
    "from fastai.text import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=\"/home/kirana/Documents/phd\"\n",
    "DATAPATH=\"/home/kirana/Documents/phd/data/experiment/SMSS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "[df_train,df_valid,itos, train_tokens, valid_tokens, trn_lm, val_lm]=pickle.load(open(f'{DATAPATH}/inter/dfs_tokens_fastai.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=52 # 52 - Jeremey, 20 - default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt= 70 #70 - Jeremey, 35 - default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_lm(tokens,bs):\n",
    "    import itertools\n",
    "    # Collapse into a single large array\n",
    "    tokens=np.asarray(list (itertools.chain(*tokens)))\n",
    "    # How many batches\n",
    "    n_batch=len(tokens)//bs\n",
    "    # Truncate to exclude the ones at the end\n",
    "    tokens=tokens[:bs*n_batch]\n",
    "    # Reshape\n",
    "    tokens=tokens.reshape(bs,-1)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52, 2460), (52, 274))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens=create_data_lm(df_train['tokens'],bs)\n",
    "valid_tokens=create_data_lm(df_valid['tokens'],bs)\n",
    "train_tokens.shape, valid_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i):\n",
    "    seq_len = min(args.bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52, 70) (52, 70)\n"
     ]
    }
   ],
   "source": [
    "n_batch=train_tokens.shape[1]\n",
    "for i in range(0,n_batch,bptt):\n",
    "    seq_len=min(bptt,n_batch-1-i)\n",
    "    x=train_tokens[:,i:i+seq_len]\n",
    "    y=train_tokens[:,i+1:i+1+seq_len]\n",
    "    print (x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52, 70), (52, 70))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   4,    5,    6,    3, ...,    7,    2,    0,   15],\n",
       "        [  32,   22,   17,    4, ...,   52,   87,   46,    4],\n",
       "        [   2,   63,   34,   11, ...,  145,  430,    4,    5],\n",
       "        [   0,    0,    0,  846, ...,  256,  159,   40,  275],\n",
       "        ...,\n",
       "        [  50,   18,   65,  204, ...,    3,    2,   29, 1036],\n",
       "        [  85,   68,  106, 1940, ...,    0,    2,    0,   45],\n",
       "        [  17,    2,    0,  252, ...,   16, 1088,   12,    8],\n",
       "        [  27,   59,    0,   18, ...,    9,  582,   15, 1215]]),\n",
       " array([[   5,    6,    3,    2, ...,    2,    0,   15,  887],\n",
       "        [  22,   17,    4,    5, ...,   87,   46,    4,    5],\n",
       "        [  63,   34,   11,  187, ...,  430,    4,    5,    6],\n",
       "        [   0,    0,  846,   62, ...,  159,   40,  275,   19],\n",
       "        ...,\n",
       "        [  18,   65,  204,   74, ...,    2,   29, 1036,    0],\n",
       "        [  68,  106, 1940,  395, ...,    2,    0,   45,    2],\n",
       "        [   2,    0,  252,  254, ..., 1088,   12,    8,  682],\n",
       "        [  59,    0,   18,   42, ...,  582,   15, 1215,    0]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52, 70), (52, 70))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 2460)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-trained AWD-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_AWD_LSTM='/home/kirana/Documents/phd/data/pre-trained/awd_lstm/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bwd_wt103_enc.h5  fwd_wt103_enc.h5  itos_wt103.pkl  \u001b[0m\u001b[01;34mwt103_60002\u001b[0m/\r\n",
      "bwd_wt103.h5      fwd_wt103.h5      \u001b[01;34mwt103_238642\u001b[0m/   \u001b[01;31mwt103_tiny.tgz\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "ls {PATH_AWD_LSTM}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts = torch.load(f'{PATH_AWD_LSTM}/fwd_wt103.h5', map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.encoder.weight',\n",
       "              tensor([[-1.2274e-01,  2.7886e-01, -3.8850e-01,  ..., -1.0404e-01,\n",
       "                        1.9580e-02,  1.8548e-01],\n",
       "                      [ 1.4854e-05, -2.3424e-05,  1.9693e-05,  ...,  2.1349e-05,\n",
       "                        2.1776e-05, -1.2394e-05],\n",
       "                      [ 1.8070e-01,  1.5874e+00, -1.1738e-01,  ..., -4.5935e-02,\n",
       "                       -8.1352e-02,  1.8054e-01],\n",
       "                      ...,\n",
       "                      [-1.8595e-03, -6.8529e-03,  1.6999e-03,  ...,  1.7039e-03,\n",
       "                        4.1632e-03, -1.3171e-03],\n",
       "                      [-2.3120e-03, -6.9001e-03,  1.8772e-03,  ...,  5.0309e-04,\n",
       "                        4.6596e-03, -2.5850e-03],\n",
       "                      [-2.2463e-03, -9.1512e-03,  1.3927e-03,  ...,  1.2296e-03,\n",
       "                        5.8085e-03, -1.8940e-03]])),\n",
       "             ('0.encoder_with_dropout.embed.weight',\n",
       "              tensor([[-1.2274e-01,  2.7886e-01, -3.8850e-01,  ..., -1.0404e-01,\n",
       "                        1.9580e-02,  1.8548e-01],\n",
       "                      [ 1.4854e-05, -2.3424e-05,  1.9693e-05,  ...,  2.1349e-05,\n",
       "                        2.1776e-05, -1.2394e-05],\n",
       "                      [ 1.8070e-01,  1.5874e+00, -1.1738e-01,  ..., -4.5935e-02,\n",
       "                       -8.1352e-02,  1.8054e-01],\n",
       "                      ...,\n",
       "                      [-1.8595e-03, -6.8529e-03,  1.6999e-03,  ...,  1.7039e-03,\n",
       "                        4.1632e-03, -1.3171e-03],\n",
       "                      [-2.3120e-03, -6.9001e-03,  1.8772e-03,  ...,  5.0309e-04,\n",
       "                        4.6596e-03, -2.5850e-03],\n",
       "                      [-2.2463e-03, -9.1512e-03,  1.3927e-03,  ...,  1.2296e-03,\n",
       "                        5.8085e-03, -1.8940e-03]])),\n",
       "             ('0.rnns.0.module.weight_ih_l0',\n",
       "              tensor([[-0.0812, -0.0811, -0.0937,  ..., -0.0259, -0.1403, -0.3247],\n",
       "                      [ 0.1154,  0.1142,  0.0938,  ..., -0.0711,  0.1669, -0.0387],\n",
       "                      [-0.0051,  0.1007,  0.2071,  ..., -0.0860, -0.0288, -0.0894],\n",
       "                      ...,\n",
       "                      [ 0.0055,  0.0157,  0.2990,  ...,  0.0616,  0.1159, -0.4737],\n",
       "                      [ 0.0181,  0.0426,  0.1130,  ...,  0.3529, -0.0114, -0.0125],\n",
       "                      [-0.0167, -0.1328,  0.1741,  ...,  0.0548, -0.0045,  0.1688]])),\n",
       "             ('0.rnns.0.module.bias_ih_l0',\n",
       "              tensor([ 0.1503, -0.4701, -0.1885,  ..., -0.5919, -0.2172, -0.1207])),\n",
       "             ('0.rnns.0.module.bias_hh_l0',\n",
       "              tensor([ 0.1503, -0.4701, -0.1885,  ..., -0.5919, -0.2172, -0.1207])),\n",
       "             ('0.rnns.0.module.weight_hh_l0_raw',\n",
       "              tensor([[-0.1013,  0.1786, -0.0528,  ...,  0.0741,  0.0306,  0.2467],\n",
       "                      [ 0.1780, -0.0853, -0.0243,  ..., -0.1129, -0.1310, -0.1498],\n",
       "                      [ 0.0661, -0.0496,  0.0921,  ...,  0.1829,  0.0533, -0.1525],\n",
       "                      ...,\n",
       "                      [-0.0322, -0.0704,  0.1653,  ...,  0.2142, -0.0558,  0.0315],\n",
       "                      [-0.1651, -0.0290,  0.1748,  ..., -0.0446,  0.5444,  0.0616],\n",
       "                      [ 0.0905, -0.1704, -0.0053,  ..., -0.0057,  0.2269,  0.0328]])),\n",
       "             ('0.rnns.1.module.weight_ih_l0',\n",
       "              tensor([[ 0.3307,  0.0385,  0.0860,  ...,  0.0685, -0.0444,  0.0539],\n",
       "                      [ 0.0720,  0.1607,  0.0562,  ...,  0.0276,  0.0613,  0.1632],\n",
       "                      [-0.1565, -0.1168,  0.1897,  ..., -0.0357,  0.0296,  0.0961],\n",
       "                      ...,\n",
       "                      [-0.0897, -0.1464, -0.0760,  ...,  0.0536,  0.0422, -0.0580],\n",
       "                      [ 0.1166, -0.1534, -0.1784,  ..., -0.0689,  0.2170,  0.1461],\n",
       "                      [-0.0413,  0.0689,  0.0581,  ..., -0.0640, -0.1703, -0.0945]])),\n",
       "             ('0.rnns.1.module.bias_ih_l0',\n",
       "              tensor([-0.8577, -0.6784, -0.7249,  ..., -0.6782,  0.0567, -0.5026])),\n",
       "             ('0.rnns.1.module.bias_hh_l0',\n",
       "              tensor([-0.8577, -0.6784, -0.7249,  ..., -0.6782,  0.0567, -0.5026])),\n",
       "             ('0.rnns.1.module.weight_hh_l0_raw',\n",
       "              tensor([[-0.0273, -0.2277,  0.0782,  ...,  0.1355, -0.1282,  0.1669],\n",
       "                      [ 0.1218,  0.0017, -0.0998,  ..., -0.2085, -0.0686, -0.1389],\n",
       "                      [-0.3878, -0.0498, -0.1748,  ..., -0.4014,  0.1986, -0.4400],\n",
       "                      ...,\n",
       "                      [-0.2097, -0.4298,  0.3551,  ...,  0.0316, -0.1198,  0.1266],\n",
       "                      [ 0.0037, -0.0223,  0.0032,  ..., -0.2672, -0.3093, -0.0361],\n",
       "                      [-0.0464,  0.1664, -0.1348,  ...,  0.1600, -0.1138,  0.0845]])),\n",
       "             ('0.rnns.2.module.weight_ih_l0',\n",
       "              tensor([[-0.0741,  0.0447, -0.0744,  ..., -0.0419,  0.1600, -0.0553],\n",
       "                      [ 0.0270,  0.0118,  0.0449,  ...,  0.1165, -0.1080, -0.0681],\n",
       "                      [-0.1023, -0.1662, -0.0229,  ...,  0.1652, -0.1070,  0.0970],\n",
       "                      ...,\n",
       "                      [-0.0989, -0.4425, -0.0343,  ..., -0.1434,  0.5851, -0.0291],\n",
       "                      [ 0.0802, -0.1067,  0.2789,  ..., -0.0916, -0.2240,  0.1020],\n",
       "                      [-0.4078,  0.7220,  0.1142,  ...,  0.5287,  0.2035, -0.1811]])),\n",
       "             ('0.rnns.2.module.bias_ih_l0',\n",
       "              tensor([-0.3681, -0.9079, -0.1998,  ...,  0.8533,  0.3202,  1.2172])),\n",
       "             ('0.rnns.2.module.bias_hh_l0',\n",
       "              tensor([-0.3681, -0.9079, -0.1998,  ...,  0.8533,  0.3202,  1.2172])),\n",
       "             ('0.rnns.2.module.weight_hh_l0_raw',\n",
       "              tensor([[-0.0966,  0.0236, -0.0152,  ...,  0.0388, -0.0531, -0.0395],\n",
       "                      [-0.0328, -0.2217,  0.0028,  ...,  0.0143, -0.0368, -0.0085],\n",
       "                      [ 0.0167, -0.0081, -0.0561,  ...,  0.0125,  0.0442, -0.0139],\n",
       "                      ...,\n",
       "                      [-0.0212, -0.1034, -0.0106,  ..., -0.0561,  0.0200, -0.0157],\n",
       "                      [ 0.0183,  0.0364, -0.0251,  ..., -0.0240, -0.1150,  0.0046],\n",
       "                      [ 0.0100, -0.1824,  0.1076,  ..., -0.0269,  0.2733,  0.1846]])),\n",
       "             ('1.decoder.weight',\n",
       "              tensor([[-1.2274e-01,  2.7886e-01, -3.8850e-01,  ..., -1.0404e-01,\n",
       "                        1.9580e-02,  1.8548e-01],\n",
       "                      [ 1.4854e-05, -2.3424e-05,  1.9693e-05,  ...,  2.1349e-05,\n",
       "                        2.1776e-05, -1.2394e-05],\n",
       "                      [ 1.8070e-01,  1.5874e+00, -1.1738e-01,  ..., -4.5935e-02,\n",
       "                       -8.1352e-02,  1.8054e-01],\n",
       "                      ...,\n",
       "                      [-1.8595e-03, -6.8529e-03,  1.6999e-03,  ...,  1.7039e-03,\n",
       "                        4.1632e-03, -1.3171e-03],\n",
       "                      [-2.3120e-03, -6.9001e-03,  1.8772e-03,  ...,  5.0309e-04,\n",
       "                        4.6596e-03, -2.5850e-03],\n",
       "                      [-2.2463e-03, -9.1512e-03,  1.3927e-03,  ...,  1.2296e-03,\n",
       "                        5.8085e-03, -1.8940e-03]]))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts = torch.load(f'{PATH_AWD_LSTM}/fwd_wt103_enc.h5', map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('encoder.weight',\n",
       "              tensor([[-1.2274e-01,  2.7886e-01, -3.8850e-01,  ..., -1.0404e-01,\n",
       "                        1.9580e-02,  1.8548e-01],\n",
       "                      [ 1.4854e-05, -2.3424e-05,  1.9693e-05,  ...,  2.1349e-05,\n",
       "                        2.1776e-05, -1.2394e-05],\n",
       "                      [ 1.8070e-01,  1.5874e+00, -1.1738e-01,  ..., -4.5935e-02,\n",
       "                       -8.1352e-02,  1.8054e-01],\n",
       "                      ...,\n",
       "                      [-1.8595e-03, -6.8529e-03,  1.6999e-03,  ...,  1.7039e-03,\n",
       "                        4.1632e-03, -1.3171e-03],\n",
       "                      [-2.3120e-03, -6.9001e-03,  1.8772e-03,  ...,  5.0309e-04,\n",
       "                        4.6596e-03, -2.5850e-03],\n",
       "                      [-2.2463e-03, -9.1512e-03,  1.3927e-03,  ...,  1.2296e-03,\n",
       "                        5.8085e-03, -1.8940e-03]])),\n",
       "             ('encoder_with_dropout.embed.weight',\n",
       "              tensor([[-1.2274e-01,  2.7886e-01, -3.8850e-01,  ..., -1.0404e-01,\n",
       "                        1.9580e-02,  1.8548e-01],\n",
       "                      [ 1.4854e-05, -2.3424e-05,  1.9693e-05,  ...,  2.1349e-05,\n",
       "                        2.1776e-05, -1.2394e-05],\n",
       "                      [ 1.8070e-01,  1.5874e+00, -1.1738e-01,  ..., -4.5935e-02,\n",
       "                       -8.1352e-02,  1.8054e-01],\n",
       "                      ...,\n",
       "                      [-1.8595e-03, -6.8529e-03,  1.6999e-03,  ...,  1.7039e-03,\n",
       "                        4.1632e-03, -1.3171e-03],\n",
       "                      [-2.3120e-03, -6.9001e-03,  1.8772e-03,  ...,  5.0309e-04,\n",
       "                        4.6596e-03, -2.5850e-03],\n",
       "                      [-2.2463e-03, -9.1512e-03,  1.3927e-03,  ...,  1.2296e-03,\n",
       "                        5.8085e-03, -1.8940e-03]])),\n",
       "             ('rnns.0.module.weight_ih_l0',\n",
       "              tensor([[-0.0812, -0.0811, -0.0937,  ..., -0.0259, -0.1403, -0.3247],\n",
       "                      [ 0.1154,  0.1142,  0.0938,  ..., -0.0711,  0.1669, -0.0387],\n",
       "                      [-0.0051,  0.1007,  0.2071,  ..., -0.0860, -0.0288, -0.0894],\n",
       "                      ...,\n",
       "                      [ 0.0055,  0.0157,  0.2990,  ...,  0.0616,  0.1159, -0.4737],\n",
       "                      [ 0.0181,  0.0426,  0.1130,  ...,  0.3529, -0.0114, -0.0125],\n",
       "                      [-0.0167, -0.1328,  0.1741,  ...,  0.0548, -0.0045,  0.1688]])),\n",
       "             ('rnns.0.module.bias_ih_l0',\n",
       "              tensor([ 0.1503, -0.4701, -0.1885,  ..., -0.5919, -0.2172, -0.1207])),\n",
       "             ('rnns.0.module.bias_hh_l0',\n",
       "              tensor([ 0.1503, -0.4701, -0.1885,  ..., -0.5919, -0.2172, -0.1207])),\n",
       "             ('rnns.0.module.weight_hh_l0_raw',\n",
       "              tensor([[-0.1013,  0.1786, -0.0528,  ...,  0.0741,  0.0306,  0.2467],\n",
       "                      [ 0.1780, -0.0853, -0.0243,  ..., -0.1129, -0.1310, -0.1498],\n",
       "                      [ 0.0661, -0.0496,  0.0921,  ...,  0.1829,  0.0533, -0.1525],\n",
       "                      ...,\n",
       "                      [-0.0322, -0.0704,  0.1653,  ...,  0.2142, -0.0558,  0.0315],\n",
       "                      [-0.1651, -0.0290,  0.1748,  ..., -0.0446,  0.5444,  0.0616],\n",
       "                      [ 0.0905, -0.1704, -0.0053,  ..., -0.0057,  0.2269,  0.0328]])),\n",
       "             ('rnns.1.module.weight_ih_l0',\n",
       "              tensor([[ 0.3307,  0.0385,  0.0860,  ...,  0.0685, -0.0444,  0.0539],\n",
       "                      [ 0.0720,  0.1607,  0.0562,  ...,  0.0276,  0.0613,  0.1632],\n",
       "                      [-0.1565, -0.1168,  0.1897,  ..., -0.0357,  0.0296,  0.0961],\n",
       "                      ...,\n",
       "                      [-0.0897, -0.1464, -0.0760,  ...,  0.0536,  0.0422, -0.0580],\n",
       "                      [ 0.1166, -0.1534, -0.1784,  ..., -0.0689,  0.2170,  0.1461],\n",
       "                      [-0.0413,  0.0689,  0.0581,  ..., -0.0640, -0.1703, -0.0945]])),\n",
       "             ('rnns.1.module.bias_ih_l0',\n",
       "              tensor([-0.8577, -0.6784, -0.7249,  ..., -0.6782,  0.0567, -0.5026])),\n",
       "             ('rnns.1.module.bias_hh_l0',\n",
       "              tensor([-0.8577, -0.6784, -0.7249,  ..., -0.6782,  0.0567, -0.5026])),\n",
       "             ('rnns.1.module.weight_hh_l0_raw',\n",
       "              tensor([[-0.0273, -0.2277,  0.0782,  ...,  0.1355, -0.1282,  0.1669],\n",
       "                      [ 0.1218,  0.0017, -0.0998,  ..., -0.2085, -0.0686, -0.1389],\n",
       "                      [-0.3878, -0.0498, -0.1748,  ..., -0.4014,  0.1986, -0.4400],\n",
       "                      ...,\n",
       "                      [-0.2097, -0.4298,  0.3551,  ...,  0.0316, -0.1198,  0.1266],\n",
       "                      [ 0.0037, -0.0223,  0.0032,  ..., -0.2672, -0.3093, -0.0361],\n",
       "                      [-0.0464,  0.1664, -0.1348,  ...,  0.1600, -0.1138,  0.0845]])),\n",
       "             ('rnns.2.module.weight_ih_l0',\n",
       "              tensor([[-0.0741,  0.0447, -0.0744,  ..., -0.0419,  0.1600, -0.0553],\n",
       "                      [ 0.0270,  0.0118,  0.0449,  ...,  0.1165, -0.1080, -0.0681],\n",
       "                      [-0.1023, -0.1662, -0.0229,  ...,  0.1652, -0.1070,  0.0970],\n",
       "                      ...,\n",
       "                      [-0.0989, -0.4425, -0.0343,  ..., -0.1434,  0.5851, -0.0291],\n",
       "                      [ 0.0802, -0.1067,  0.2789,  ..., -0.0916, -0.2240,  0.1020],\n",
       "                      [-0.4078,  0.7220,  0.1142,  ...,  0.5287,  0.2035, -0.1811]])),\n",
       "             ('rnns.2.module.bias_ih_l0',\n",
       "              tensor([-0.3681, -0.9079, -0.1998,  ...,  0.8533,  0.3202,  1.2172])),\n",
       "             ('rnns.2.module.bias_hh_l0',\n",
       "              tensor([-0.3681, -0.9079, -0.1998,  ...,  0.8533,  0.3202,  1.2172])),\n",
       "             ('rnns.2.module.weight_hh_l0_raw',\n",
       "              tensor([[-0.0966,  0.0236, -0.0152,  ...,  0.0388, -0.0531, -0.0395],\n",
       "                      [-0.0328, -0.2217,  0.0028,  ...,  0.0143, -0.0368, -0.0085],\n",
       "                      [ 0.0167, -0.0081, -0.0561,  ...,  0.0125,  0.0442, -0.0139],\n",
       "                      ...,\n",
       "                      [-0.0212, -0.1034, -0.0106,  ..., -0.0561,  0.0200, -0.0157],\n",
       "                      [ 0.0183,  0.0364, -0.0251,  ..., -0.0240, -0.1150,  0.0046],\n",
       "                      [ 0.0100, -0.1824,  0.1076,  ..., -0.0269,  0.2733,  0.1846]]))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos2=pickle.load(open(f'{PATH_AWD_LSTM}/itos_wt103.pkl','rb'))\n",
    "stoi2 = collections.defaultdict(lambda: -1, { v: k for k, v in enumerate(itos2) })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([238462, 400])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgts['encoder.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_wgts = wgts['encoder.weight'].numpy() # converts np.ndarray from torch.FloatTensor.output shape: (238462, 400)\n",
    "row_m = enc_wgts.mean(0) # returns the average of the array elements along axis 0. output shape: (400,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((238462, 400), 2750)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_wgts.shape, len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_emb=enc_wgts.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_w = np.zeros((len(itos),n_emb), dtype=np.float32) # shape: (60002, 400)\n",
    "\n",
    "for i, w in enumerate(itos):\n",
    "    r = stoi2[w]\n",
    "    new_w[i] = enc_wgts[r] if r >= 0 else row_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "394"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(itos).difference(set(itos2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inp=len(itos)\n",
    "n_emb=400 #650\n",
    "n_hidden=400 #650\n",
    "n_layers=2\n",
    "dropout=0.5\n",
    "wd=1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class language_model (nn.Module):\n",
    "    def __init__(self,n_inp,n_emb,n_hidden,n_layers,bidirectional,bs,device,dropout_e=0.05,dropout=0.5,\\\n",
    "                 dropout_o=0.5,pretrain_mtx=None,adaptive_log_softmax=True,tie_weights=True):\n",
    "        super().__init__()\n",
    "        self.n_inp,self.n_emb,self.n_hidden,self.n_layers,self.bidirectional,self.bs,self.device,self.pretrain_mtx=\\\n",
    "                            n_inp,n_emb,n_hidden,n_layers,bidirectional,bs,device,pretrain_mtx\n",
    "        self.adaptive_log_softmax,self.tie_weights=adaptive_log_softmax,tie_weights\n",
    "        self.dropout_e,self.dropout,self.dropout_o=dropout_e,dropout,dropout_o\n",
    "        self.gen_hidden()\n",
    "        self.create_architecture()\n",
    "        if pretrain_mtx is not None:\n",
    "            print (\"initializing\")\n",
    "            self.initialize_glove()\n",
    "            \n",
    "        if self.adaptive_log_softmax is False:\n",
    "            self.criterion=nn.CrossEntropyLoss()\n",
    "        \n",
    "    def create_architecture(self):\n",
    "        # Dropout layer\n",
    "        self.dropout_enc=nn.Dropout(self.dropout_e)\n",
    "        # Embedding Layer\n",
    "        self.encoder=nn.Embedding(self.n_inp,self.n_emb)\n",
    "        # LSTM Layer\n",
    "        self.lstm=nn.LSTM(self.n_emb,self.n_hidden,self.n_layers,batch_first=True,dropout=self.dropout,\\\n",
    "                          bidirectional=False)\n",
    "        self.dropout_op=nn.Dropout(self.dropout_o)\n",
    "        \n",
    "        if self.adaptive_log_softmax:\n",
    "            # Adaptive Log Softmax Loss\n",
    "            self.adaptive_softmax=AdaptiveLogSoftmaxWithLoss(self.n_hidden,\n",
    "                                    self.n_inp,\n",
    "                                    cutoffs=[round(self.n_inp/15),3*round(self.n_inp/15)],\n",
    "                                    div_value=4,\n",
    "                                    get_full_prob=True)\n",
    "        else:\n",
    "            self.decoder=nn.Linear(self.n_hidden,self.n_inp)\n",
    "    \n",
    "    def freeze_embedding(self):\n",
    "        self.encoder.weight.requires_grad=False\n",
    "        if self.tie_weights:\n",
    "            self.decoder.weight.requires_grad=False\n",
    "        \n",
    "    \n",
    "    def unfreeze_embedding(self):\n",
    "        self.encoder.weight.requires_grad=True\n",
    "        if self.tie_weights:\n",
    "            self.decoder.weight.requires_grad=True\n",
    "        \n",
    "    def initialize_glove(self):\n",
    "        self.encoder.weight.data=torch.Tensor(self.pretrain_mtx)\n",
    "        if self.tie_weights:\n",
    "            self.decoder.weight=self.encoder.weight\n",
    "    \n",
    "    def gen_hidden(self):\n",
    "        # Initialize hidden\n",
    "        self.hidden=(Variable(torch.zeros(self.n_layers,self.bs,self.n_hidden,requires_grad=False).to(self.device)),\n",
    "                     Variable(torch.zeros(self.n_layers,self.bs,self.n_hidden,requires_grad=False).to(self.device)))\n",
    "    \n",
    "        \n",
    "    def forward(self,Xb,Yb):\n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        embs=self.dropout_enc(self.encoder(Xb))\n",
    "        if Xb.size(0) < self.bs:\n",
    "            self.hidden=(self.hidden[0][:,:Xb.size(0),:].contiguous(),\n",
    "            self.hidden[1][:,:Xb.size(0),:].contiguous())\n",
    "        out,new_hidden=self.lstm(embs,self.hidden)\n",
    "        out=self.dropout_op(out)\n",
    "         # Wrap the hidden state in a new tensor without the gradients\n",
    "        self.hidden=(Variable(new_hidden[0].data,requires_grad=False).to(self.device),\\\n",
    "                     Variable(new_hidden[1].data,requires_grad=False).to(self.device))\n",
    "        if self.adaptive_log_softmax:\n",
    "            out=out.reshape(out.size(0)*out.size(1),out.size(2))        # output is of shape n_batch * n_seq * n_hidden\n",
    "      \n",
    "            out=self.adaptive_softmax(out,Yb.view(-1))\n",
    "            loss=out.loss\n",
    "            preds=out.output_full\n",
    "        else:\n",
    "            #import pdb\n",
    "            #pdb.set_trace()\n",
    "            preds=self.decoder(out.contiguous().view(out.size(0)*out.size(1), out.size(2)))\n",
    "            loss=self.criterion(preds,Yb.contiguous().view(-1))\n",
    "\n",
    "        return preds, loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_multinomial(preds,actual):\n",
    "    preds=preds.max(1)[1]\n",
    "    correct=preds==actual\n",
    "    return correct.float().sum()/len(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing\n"
     ]
    }
   ],
   "source": [
    "device=\"cuda:1\"\n",
    "model=language_model(n_inp,n_emb,n_hidden,n_layers,False,bs,device,0.05,0.5,0.5,new_w,False,True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2750, 400)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2750, 400]), torch.Size([2750, 400]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.weight.data.shape,model.decoder.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2750, 400]), 400, 2750)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Linear(n_hidden,n_inp).weight.data.shape, n_hidden, n_inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test if model forward works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2750, 400]), torch.Size([2750, 400]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder.weight.shape,model.encoder.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 70)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1==0:\n",
    "    model.forward(torch.LongTensor(x),torch.LongTensor(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self,model,optimizer,metric_fn,device,bptt=12,print_every=5,clip_val=None):\n",
    "        self.model,self.optimizer,self.metric_fn,self.device,self.print_every,self.bptt,self.losses,self.clip_val=\\\n",
    "            model,optimizer,metric_fn,device,print_every,bptt,[],clip_val\n",
    "        self.n_epochs=1\n",
    "\n",
    "    \n",
    "    def fit (self,Xb,Yb,mode_train=True):\n",
    "        if mode_train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            \n",
    "        preds,loss=self.model(Xb,Yb)\n",
    "        \n",
    "       \n",
    "            \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            acc=self.metric_fn(preds,Yb.view(-1))\n",
    "            acc=acc.item()\n",
    "            del preds\n",
    "        \n",
    "        if mode_train:\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        myloss=loss.item()\n",
    "        del loss\n",
    "        \n",
    "        if self.clip_val is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.25)\n",
    "            if 1==0:\n",
    "                for p in self.model.parameters():\n",
    "                    p.data.add_(self.lr, p.grad.data)\n",
    "        \n",
    "        return myloss, acc\n",
    "    \n",
    "    def lr_find (self,start_lr,end_lr,iterator,n_batch):\n",
    "        losses,lrs=[],[]\n",
    "        ratio=end_lr/start_lr\n",
    "        num_steps=n_batch\n",
    "        lr=start_lr\n",
    "        for i in range(num_steps):            \n",
    "            lr=lr*(end_lr/start_lr)**(1/num_steps)\n",
    "            lrs.append(lr)\n",
    "        self.lrs=lrs\n",
    "        self.run_epoch(iterator,mode_train=True,lrs=lrs)\n",
    "    \n",
    "    def run_epoch(self,iterator,mode_train,lrs=None):\n",
    "        n_batch=iterator.shape[1]\n",
    "        epoch_loss,epoch_acc,i,k=0,0,0,0\n",
    "        self.model.gen_hidden()\n",
    "        #for k,i in enumerate(range(0,n_batch,self.bptt)):\n",
    "        n_batch=iterator.shape[1]\n",
    "        while i<n_batch-bptt:\n",
    "            if mode_train:\n",
    "                cust_bptt=self.bptt if np.random.random() < 0.95 else self.bptt//np.random.randint (2,4)\n",
    "            else:\n",
    "                cust_bptt=bptt\n",
    "            seq_len=min(cust_bptt,n_batch-1-i)\n",
    "            Xb=train_tokens[:,i:i+seq_len]\n",
    "            Yb=train_tokens[:,i+1:i+1+seq_len]\n",
    "            Xb=torch.LongTensor(Xb)\n",
    "            Yb=torch.LongTensor(Yb)\n",
    "            Xb=Xb.to(self.device)\n",
    "            Yb=Yb.to(self.device)\n",
    "            \n",
    "            if lrs is not None:\n",
    "                lr=lrs[k]\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr']=lr \n",
    "            \n",
    "\n",
    "            loss,acc=self.fit(Xb,Yb,mode_train)\n",
    "            \n",
    "            if lrs is not None:\n",
    "                self.losses.append(loss)\n",
    "            \n",
    "            \n",
    "            epoch_loss+=loss\n",
    "            epoch_acc+=acc\n",
    "            if k%self.print_every == 0:\n",
    "                if k:\n",
    "                    print (f'Batch:{k} {epoch_loss/(k)}  {epoch_acc/(k)}')  \n",
    "                    torch.cuda.empty_cache()\n",
    "            k=k+1\n",
    "            i=i+seq_len\n",
    "        epoch_loss=epoch_loss/k\n",
    "        epoch_acc=epoch_acc/k\n",
    "        \n",
    "        if 1==0:\n",
    "            lr /= 4.0\n",
    "            # Freeze all the layers initially\n",
    "            for param in resnet.parameters():\n",
    "                param.requires_grad=False\n",
    "            torch.save(resnet,'resnet')\n",
    "            torch.save(resnet.state_dict(),'resnet_state_dict')\n",
    "            resnet.load_state_dict(torch.load('resnet_state_dict'))\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr']=lr  \n",
    "            \n",
    "        return epoch_loss,epoch_acc\n",
    "    \n",
    "    def plot_lrs(self, n_roll=1):\n",
    "        import seaborn as sns\n",
    "        ax=sns.lineplot(x=self.lrs,y=pd.Series(self.losses).rolling(n_roll).mean())\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.set_xlabel('Learning Rate')\n",
    "\n",
    "    \n",
    "    def run_epochs(self,dltrain,dlvalid,n_epochs=1):\n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            loss,acc=self.run_epoch(dltrain,True)\n",
    "            print (f'Epoch:{epoch} Loss:{loss}')\n",
    "            lossv,accv=self.run_epoch(dlvalid,mode_train=False)\n",
    "            print (f'Epoch:{epoch} Loss:{loss} Accuracy:{acc} Loss:{lossv} Accuracy:{accv}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(model.parameters(),lr=1e-3,betas=(0.9,0.999), weight_decay=wd)\n",
    "metric_fn=accuracy_multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_batch=np.int(np.ceil(train_tokens.shape[1]/bptt))\n",
    "n_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.weight.requires_grad, model.decoder.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner=Learner(model,optimizer,accuracy_multinomial,device,bptt,500,0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2750"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Loss:6.686410917176141\n",
      "Epoch:0 Loss:6.686410917176141 Accuracy:0.07455407163878489 Loss:6.119425614674886 Accuracy:0.11840660125017166\n",
      "Epoch:1 Loss:6.178734493255615\n",
      "Epoch:1 Loss:6.178734493255615 Accuracy:0.11459184395415442 Loss:5.8683403333028155 Accuracy:0.13672161599000296\n",
      "Epoch:2 Loss:5.991630363464355\n",
      "Epoch:2 Loss:5.991630363464355 Accuracy:0.12529350072145462 Loss:5.711137612660726 Accuracy:0.14423077801863352\n",
      "Epoch:3 Loss:5.8656050409589495\n",
      "Epoch:3 Loss:5.8656050409589495 Accuracy:0.13382261033569062 Loss:5.580183188120524 Accuracy:0.15430404245853424\n",
      "Epoch:4 Loss:5.767781337102254\n",
      "Epoch:4 Loss:5.767781337102254 Accuracy:0.13939819774693912 Loss:5.486290613810222 Accuracy:0.1597985476255417\n",
      "Epoch:5 Loss:5.676023142678397\n",
      "Epoch:5 Loss:5.676023142678397 Accuracy:0.14689168759754725 Loss:5.371978759765625 Accuracy:0.16895605623722076\n",
      "Epoch:6 Loss:5.599775159681165\n",
      "Epoch:6 Loss:5.599775159681165 Accuracy:0.15089811102764025 Loss:5.292594114939372 Accuracy:0.17820513745148978\n",
      "Epoch:7 Loss:5.528252227886303\n",
      "Epoch:7 Loss:5.528252227886303 Accuracy:0.15521914250141866 Loss:5.1991508801778155 Accuracy:0.18388278782367706\n",
      "Epoch:8 Loss:5.458889697727404\n",
      "Epoch:8 Loss:5.458889697727404 Accuracy:0.16063539644605235 Loss:5.116944789886475 Accuracy:0.1913003772497177\n",
      "Epoch:9 Loss:5.386648337046306\n",
      "Epoch:9 Loss:5.386648337046306 Accuracy:0.16607707697484228 Loss:5.038697878519694 Accuracy:0.19652015467484793\n",
      "Epoch:10 Loss:5.313773454938616\n",
      "Epoch:10 Loss:5.313773454938616 Accuracy:0.1723861928497042 Loss:4.942681312561035 Accuracy:0.20668499171733856\n",
      "Epoch:11 Loss:5.2522131072150335\n",
      "Epoch:11 Loss:5.2522131072150335 Accuracy:0.1758928642504745 Loss:4.86714506149292 Accuracy:0.2128205249706904\n",
      "Epoch:12 Loss:5.185761083875383\n",
      "Epoch:12 Loss:5.185761083875383 Accuracy:0.18084465989044735 Loss:4.802485466003418 Accuracy:0.21703297893206278\n",
      "Epoch:13 Loss:5.134782546275371\n",
      "Epoch:13 Loss:5.134782546275371 Accuracy:0.18457181026806702 Loss:4.740814208984375 Accuracy:0.22298535704612732\n",
      "Epoch:14 Loss:5.068227876935686\n",
      "Epoch:14 Loss:5.068227876935686 Accuracy:0.19010000016008105 Loss:4.624984582265218 Accuracy:0.238003671169281\n",
      "Epoch:15 Loss:5.008254355854458\n",
      "Epoch:15 Loss:5.008254355854458 Accuracy:0.193869784888294 Loss:4.5645988782246905 Accuracy:0.2433150311311086\n",
      "Epoch:16 Loss:4.963724793614568\n",
      "Epoch:16 Loss:4.963724793614568 Accuracy:0.19739221519715078 Loss:4.496590614318848 Accuracy:0.2536630183458328\n",
      "Epoch:17 Loss:4.893139613999261\n",
      "Epoch:17 Loss:4.893139613999261 Accuracy:0.20559272666772208 Loss:4.439971446990967 Accuracy:0.25540293753147125\n",
      "Epoch:18 Loss:4.845068005153111\n",
      "Epoch:18 Loss:4.845068005153111 Accuracy:0.20931712005819594 Loss:4.377320925394694 Accuracy:0.26419414579868317\n",
      "Epoch:19 Loss:4.791273633639018\n",
      "Epoch:19 Loss:4.791273633639018 Accuracy:0.2127170032925076 Loss:4.316479841868083 Accuracy:0.2680403143167496\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(train_tokens,valid_tokens,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Loss:4.74673030183122\n",
      "Epoch:0 Loss:4.74673030183122 Accuracy:0.21590082508486672 Loss:4.282040437062581 Accuracy:0.26849818229675293\n",
      "Epoch:1 Loss:4.695068822588239\n",
      "Epoch:1 Loss:4.695068822588239 Accuracy:0.22003140790121897 Loss:4.214593410491943 Accuracy:0.2809523940086365\n",
      "Epoch:2 Loss:4.637629359109061\n",
      "Epoch:2 Loss:4.637629359109061 Accuracy:0.22521296739578248 Loss:4.150295416514079 Accuracy:0.2853480080763499\n",
      "Epoch:3 Loss:4.596357173389858\n",
      "Epoch:3 Loss:4.596357173389858 Accuracy:0.2285561780962679 Loss:4.087162494659424 Accuracy:0.29230770468711853\n",
      "Epoch:4 Loss:4.540509069288099\n",
      "Epoch:4 Loss:4.540509069288099 Accuracy:0.23214448344063116 Loss:4.053236246109009 Accuracy:0.2906593481699626\n",
      "Epoch:5 Loss:4.493198394775391\n",
      "Epoch:5 Loss:4.493198394775391 Accuracy:0.23721794741494315 Loss:3.975783348083496 Accuracy:0.3032967249552409\n",
      "Epoch:6 Loss:4.451648480551583\n",
      "Epoch:6 Loss:4.451648480551583 Accuracy:0.2400785037449428 Loss:3.926325956980387 Accuracy:0.3084249297777812\n",
      "Epoch:7 Loss:4.407064465114049\n",
      "Epoch:7 Loss:4.407064465114049 Accuracy:0.24281790895121438 Loss:3.873294989267985 Accuracy:0.3131868243217468\n",
      "Epoch:8 Loss:4.3619601385934015\n",
      "Epoch:8 Loss:4.3619601385934015 Accuracy:0.24705652637141093 Loss:3.80810546875 Accuracy:0.31684982776641846\n",
      "Epoch:9 Loss:4.311864402559069\n",
      "Epoch:9 Loss:4.311864402559069 Accuracy:0.2511301019953357 Loss:3.770731290181478 Accuracy:0.3224359154701233\n",
      "Epoch:10 Loss:4.265288631121318\n",
      "Epoch:10 Loss:4.265288631121318 Accuracy:0.254605973760287 Loss:3.704015096028646 Accuracy:0.33067766825358075\n",
      "Epoch:11 Loss:4.229646334777007\n",
      "Epoch:11 Loss:4.229646334777007 Accuracy:0.2576645568415925 Loss:3.65628973642985 Accuracy:0.33250916997591656\n",
      "Epoch:12 Loss:4.188276516066657\n",
      "Epoch:12 Loss:4.188276516066657 Accuracy:0.260511238541868 Loss:3.6189564069112143 Accuracy:0.33543957273165387\n",
      "Epoch:13 Loss:4.163224302019391\n",
      "Epoch:13 Loss:4.163224302019391 Accuracy:0.26343015219484056 Loss:3.5946290493011475 Accuracy:0.337454229593277\n",
      "Epoch:14 Loss:4.133783433172438\n",
      "Epoch:14 Loss:4.133783433172438 Accuracy:0.26772742552889717 Loss:3.537855943044027 Accuracy:0.344505508740743\n",
      "Epoch:15 Loss:4.08894111088344\n",
      "Epoch:15 Loss:4.08894111088344 Accuracy:0.27212717022214616 Loss:3.4774304231007895 Accuracy:0.3532967269420624\n",
      "Epoch:16 Loss:4.0412447588784355\n",
      "Epoch:16 Loss:4.0412447588784355 Accuracy:0.27770801782608034 Loss:3.4156912167867026 Accuracy:0.360256423552831\n",
      "Epoch:17 Loss:4.002171715100606\n",
      "Epoch:17 Loss:4.002171715100606 Accuracy:0.28297899208135074 Loss:3.3476715087890625 Accuracy:0.3700549602508545\n",
      "Epoch:18 Loss:3.954531709353129\n",
      "Epoch:18 Loss:3.954531709353129 Accuracy:0.2888454364405738 Loss:3.299083153406779 Accuracy:0.38159342606862384\n",
      "Epoch:19 Loss:3.929855717553033\n",
      "Epoch:19 Loss:3.929855717553033 Accuracy:0.29230869975354934 Loss:3.2499258518218994 Accuracy:0.3911172350247701\n",
      "Epoch:20 Loss:3.8902830805097306\n",
      "Epoch:20 Loss:3.8902830805097306 Accuracy:0.29908164739608767 Loss:3.210247755050659 Accuracy:0.40347986419995624\n",
      "Epoch:21 Loss:3.851576042175293\n",
      "Epoch:21 Loss:3.851576042175293 Accuracy:0.30672003626823424 Loss:3.1720130443573 Accuracy:0.41346155603726703\n",
      "Epoch:22 Loss:3.8105979851314\n",
      "Epoch:22 Loss:3.8105979851314 Accuracy:0.31165416836738585 Loss:3.1264094511667886 Accuracy:0.4196886618932088\n",
      "Epoch:23 Loss:3.781520666394915\n",
      "Epoch:23 Loss:3.781520666394915 Accuracy:0.3172684601375035 Loss:3.098780711491903 Accuracy:0.42820515235265094\n",
      "Epoch:24 Loss:3.751833955446879\n",
      "Epoch:24 Loss:3.751833955446879 Accuracy:0.3153783256808917 Loss:3.06112003326416 Accuracy:0.4293956259886424\n",
      "Epoch:25 Loss:3.7133965764726913\n",
      "Epoch:25 Loss:3.7133965764726913 Accuracy:0.3187905388219016 Loss:3.029851277669271 Accuracy:0.42912089824676514\n",
      "Epoch:26 Loss:3.681430435180664\n",
      "Epoch:26 Loss:3.681430435180664 Accuracy:0.32521979468209405 Loss:2.995558818181356 Accuracy:0.43470698595046997\n",
      "Epoch:27 Loss:3.6714656551678977\n",
      "Epoch:27 Loss:3.6714656551678977 Accuracy:0.327545203268528 Loss:2.9694798787434897 Accuracy:0.44285716613133747\n",
      "Epoch:28 Loss:3.6374706029891968\n",
      "Epoch:28 Loss:3.6374706029891968 Accuracy:0.3299374340309037 Loss:2.933338244756063 Accuracy:0.446062296628952\n",
      "Epoch:29 Loss:3.606570952279227\n",
      "Epoch:29 Loss:3.606570952279227 Accuracy:0.33532968418938774 Loss:2.9056264559427896 Accuracy:0.46263738473256427\n",
      "Epoch:30 Loss:3.569110257284982\n",
      "Epoch:30 Loss:3.569110257284982 Accuracy:0.33961540034839083 Loss:2.8469560146331787 Accuracy:0.46346155802408856\n",
      "Epoch:31 Loss:3.5477337638537088\n",
      "Epoch:31 Loss:3.5477337638537088 Accuracy:0.3435973914133178 Loss:2.832088311513265 Accuracy:0.45970698197682697\n",
      "Epoch:32 Loss:3.5148436069488525\n",
      "Epoch:32 Loss:3.5148436069488525 Accuracy:0.348029842547008 Loss:2.7892688115437827 Accuracy:0.4741758406162262\n",
      "Epoch:33 Loss:3.4829291820526125\n",
      "Epoch:33 Loss:3.4829291820526125 Accuracy:0.35063819885253905 Loss:2.7685041427612305 Accuracy:0.4821428755919139\n",
      "Epoch:34 Loss:3.464264876133687\n",
      "Epoch:34 Loss:3.464264876133687 Accuracy:0.35333771399549535 Loss:2.742755572001139 Accuracy:0.47875460982322693\n",
      "Epoch:35 Loss:3.4209079676204257\n",
      "Epoch:35 Loss:3.4209079676204257 Accuracy:0.35898997137943905 Loss:2.6916497548421225 Accuracy:0.49349819620450336\n",
      "Epoch:36 Loss:3.3971734841664634\n",
      "Epoch:36 Loss:3.3971734841664634 Accuracy:0.3617674145433638 Loss:2.6560038725535073 Accuracy:0.4969780544439952\n",
      "Epoch:37 Loss:3.367516895135244\n",
      "Epoch:37 Loss:3.367516895135244 Accuracy:0.3644011989235878 Loss:2.6426997979482016 Accuracy:0.4969780544439952\n",
      "Epoch:38 Loss:3.347587701252529\n",
      "Epoch:38 Loss:3.347587701252529 Accuracy:0.3688697210380009 Loss:2.61783504486084 Accuracy:0.5012820760409037\n",
      "Epoch:39 Loss:3.317598233904157\n",
      "Epoch:39 Loss:3.317598233904157 Accuracy:0.37218212400163925 Loss:2.5784270763397217 Accuracy:0.5099816918373108\n",
      "Epoch:40 Loss:3.2936340059552873\n",
      "Epoch:40 Loss:3.2936340059552873 Accuracy:0.3758832326957158 Loss:2.5334623654683432 Accuracy:0.5158425172170004\n",
      "Epoch:41 Loss:3.2821375550450504\n",
      "Epoch:41 Loss:3.2821375550450504 Accuracy:0.37725689523928874 Loss:2.5121776262919107 Accuracy:0.5143772959709167\n",
      "Epoch:42 Loss:3.261482825150361\n",
      "Epoch:42 Loss:3.261482825150361 Accuracy:0.3798579082295701 Loss:2.4941889444986978 Accuracy:0.5199634035428365\n",
      "Epoch:43 Loss:3.2413945462968616\n",
      "Epoch:43 Loss:3.2413945462968616 Accuracy:0.38221122572819394 Loss:2.478187402089437 Accuracy:0.529212474822998\n",
      "Epoch:44 Loss:3.219084713194105\n",
      "Epoch:44 Loss:3.219084713194105 Accuracy:0.38319598883390427 Loss:2.4555636246999106 Accuracy:0.5236264069875082\n",
      "Epoch:45 Loss:3.1938988453633077\n",
      "Epoch:45 Loss:3.1938988453633077 Accuracy:0.3856655544525868 Loss:2.4232714970906577 Accuracy:0.526007354259491\n",
      "Epoch:46 Loss:3.1652973175048826\n",
      "Epoch:46 Loss:3.1652973175048826 Accuracy:0.3895031239305224 Loss:2.3998374938964844 Accuracy:0.5276557008425394\n",
      "Epoch:47 Loss:3.139514790640937\n",
      "Epoch:47 Loss:3.139514790640937 Accuracy:0.39434824387232464 Loss:2.379923423131307 Accuracy:0.5312271316846212\n",
      "Epoch:48 Loss:3.123363528932844\n",
      "Epoch:48 Loss:3.123363528932844 Accuracy:0.39627945252827235 Loss:2.357194741566976 Accuracy:0.5348901549975077\n",
      "Epoch:49 Loss:3.096161106654576\n",
      "Epoch:49 Loss:3.096161106654576 Accuracy:0.3986096697194236 Loss:2.3370985190073648 Accuracy:0.5419414242108663\n",
      "Epoch:50 Loss:3.085068424542745\n",
      "Epoch:50 Loss:3.085068424542745 Accuracy:0.40016724169254303 Loss:2.3164499600728354 Accuracy:0.5403846303621928\n",
      "Epoch:51 Loss:3.089606417549981\n",
      "Epoch:51 Loss:3.089606417549981 Accuracy:0.3990079520477189 Loss:2.307084798812866 Accuracy:0.5424908598264059\n",
      "Epoch:52 Loss:3.051241261618478\n",
      "Epoch:52 Loss:3.051241261618478 Accuracy:0.40338306086403986 Loss:2.2845492362976074 Accuracy:0.5437729159990946\n",
      "Epoch:53 Loss:3.037412888294942\n",
      "Epoch:53 Loss:3.037412888294942 Accuracy:0.40553616833042455 Loss:2.259378751118978 Accuracy:0.5464286009470621\n",
      "Epoch:54 Loss:3.0207393441881454\n",
      "Epoch:54 Loss:3.0207393441881454 Accuracy:0.4072455976690565 Loss:2.244751532872518 Accuracy:0.5447802543640137\n",
      "Epoch:55 Loss:3.014179790342176\n",
      "Epoch:55 Loss:3.014179790342176 Accuracy:0.4073804761912372 Loss:2.2325706481933594 Accuracy:0.5448718269666036\n",
      "Epoch:56 Loss:2.992542934417725\n",
      "Epoch:56 Loss:2.992542934417725 Accuracy:0.41004711389541626 Loss:2.218380848566691 Accuracy:0.5472527543703715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:57 Loss:2.9927041265699597\n",
      "Epoch:57 Loss:2.9927041265699597 Accuracy:0.409083205792639 Loss:2.19516658782959 Accuracy:0.5544872085253397\n",
      "Epoch:58 Loss:2.9593699177106223\n",
      "Epoch:58 Loss:2.9593699177106223 Accuracy:0.4121290718515714 Loss:2.184446175893148 Accuracy:0.5667582750320435\n",
      "Epoch:59 Loss:2.961078323636736\n",
      "Epoch:59 Loss:2.961078323636736 Accuracy:0.41188384890556334 Loss:2.166207234064738 Accuracy:0.5653846462567648\n",
      "Epoch:60 Loss:2.9281384195600237\n",
      "Epoch:60 Loss:2.9281384195600237 Accuracy:0.4169333330222538 Loss:2.1568281650543213 Accuracy:0.5665751298268636\n",
      "Epoch:61 Loss:2.9188434150483875\n",
      "Epoch:61 Loss:2.9188434150483875 Accuracy:0.41771216690540314 Loss:2.1315931479136148 Accuracy:0.5690476497014364\n",
      "Epoch:62 Loss:2.898180709944831\n",
      "Epoch:62 Loss:2.898180709944831 Accuracy:0.4197029297550519 Loss:2.1141017278035483 Accuracy:0.5728022456169128\n",
      "Epoch:63 Loss:2.8877879892076765\n",
      "Epoch:63 Loss:2.8877879892076765 Accuracy:0.42110677191189355 Loss:2.0983612537384033 Accuracy:0.5768315394719442\n",
      "Epoch:64 Loss:2.862934698377337\n",
      "Epoch:64 Loss:2.862934698377337 Accuracy:0.42724696482930863 Loss:2.0868682066599527 Accuracy:0.5773809750874838\n",
      "Epoch:65 Loss:2.8645107017623053\n",
      "Epoch:65 Loss:2.8645107017623053 Accuracy:0.4252113716469871 Loss:2.0722645123799643 Accuracy:0.5806776881217957\n",
      "Epoch:66 Loss:2.8300273145948136\n",
      "Epoch:66 Loss:2.8300273145948136 Accuracy:0.42882467338017055 Loss:2.0551720460255942 Accuracy:0.5852564374605814\n",
      "Epoch:67 Loss:2.8336849013964334\n",
      "Epoch:67 Loss:2.8336849013964334 Accuracy:0.4293601206607289 Loss:2.0301751295725503 Accuracy:0.5867216388384501\n",
      "Epoch:68 Loss:2.8261085309480367\n",
      "Epoch:68 Loss:2.8261085309480367 Accuracy:0.4293940522168812 Loss:2.0408018430074057 Accuracy:0.5813186963399252\n",
      "Epoch:69 Loss:2.8107116089926825\n",
      "Epoch:69 Loss:2.8107116089926825 Accuracy:0.43060169451766545 Loss:2.0151650508244834 Accuracy:0.5858059128125509\n",
      "Epoch:70 Loss:2.7805359636034286\n",
      "Epoch:70 Loss:2.7805359636034286 Accuracy:0.43449049932616096 Loss:2.0060879786809287 Accuracy:0.582509179910024\n",
      "Epoch:71 Loss:2.783284001880222\n",
      "Epoch:71 Loss:2.783284001880222 Accuracy:0.43472987827327514 Loss:1.9887757301330566 Accuracy:0.5847985744476318\n",
      "Epoch:72 Loss:2.778147877873601\n",
      "Epoch:72 Loss:2.778147877873601 Accuracy:0.4346183087374713 Loss:1.9686501423517864 Accuracy:0.5863553285598755\n",
      "Epoch:73 Loss:2.7488114635149636\n",
      "Epoch:73 Loss:2.7488114635149636 Accuracy:0.4382784085141288 Loss:1.9544802904129028 Accuracy:0.5899267792701721\n",
      "Epoch:74 Loss:2.7480493204934255\n",
      "Epoch:74 Loss:2.7480493204934255 Accuracy:0.43890897120748246 Loss:1.94402809937795 Accuracy:0.5947802464167277\n",
      "Epoch:75 Loss:2.7249574926164417\n",
      "Epoch:75 Loss:2.7249574926164417 Accuracy:0.4421149401201142 Loss:1.9481106599171956 Accuracy:0.5918498237927755\n",
      "Epoch:76 Loss:2.733730203575558\n",
      "Epoch:76 Loss:2.733730203575558 Accuracy:0.43980864104297424 Loss:1.9269886414210002 Accuracy:0.596611738204956\n",
      "Epoch:77 Loss:2.7068399838038855\n",
      "Epoch:77 Loss:2.7068399838038855 Accuracy:0.4435629163469587 Loss:1.9175703128178914 Accuracy:0.6001831690470377\n",
      "Epoch:78 Loss:2.7195143506333634\n",
      "Epoch:78 Loss:2.7195143506333634 Accuracy:0.4416352639327178 Loss:1.8971785704294841 Accuracy:0.6034798622131348\n",
      "Epoch:79 Loss:2.6770172119140625\n",
      "Epoch:79 Loss:2.6770172119140625 Accuracy:0.44871431837479275 Loss:1.8784736394882202 Accuracy:0.6055860916773478\n",
      "Epoch:80 Loss:2.6719423002666898\n",
      "Epoch:80 Loss:2.6719423002666898 Accuracy:0.4480610216657321 Loss:1.8790222803751628 Accuracy:0.6036630471547445\n",
      "Epoch:81 Loss:2.6722723046938577\n",
      "Epoch:81 Loss:2.6722723046938577 Accuracy:0.4459013707107968 Loss:1.8653428554534912 Accuracy:0.6068681677182516\n",
      "Epoch:82 Loss:2.6604890757136874\n",
      "Epoch:82 Loss:2.6604890757136874 Accuracy:0.4477750112613042 Loss:1.8609208663304646 Accuracy:0.6073260505994161\n",
      "Epoch:83 Loss:2.6417493070874896\n",
      "Epoch:83 Loss:2.6417493070874896 Accuracy:0.4500187916415078 Loss:1.8484301169713337 Accuracy:0.6140110095342001\n",
      "Epoch:84 Loss:2.6424888270241875\n",
      "Epoch:84 Loss:2.6424888270241875 Accuracy:0.4501727070127215 Loss:1.830069859822591 Accuracy:0.6141025821367899\n",
      "Epoch:85 Loss:2.63824364874098\n",
      "Epoch:85 Loss:2.63824364874098 Accuracy:0.45101796338955563 Loss:1.8146364291508992 Accuracy:0.6172161300977071\n",
      "Epoch:86 Loss:2.61553966658456\n",
      "Epoch:86 Loss:2.61553966658456 Accuracy:0.4549136783395495 Loss:1.8058621088663738 Accuracy:0.6187729239463806\n",
      "Epoch:87 Loss:2.6201388222830637\n",
      "Epoch:87 Loss:2.6201388222830637 Accuracy:0.45539248500551494 Loss:1.794529398282369 Accuracy:0.6170329848925272\n",
      "Epoch:88 Loss:2.598569995827145\n",
      "Epoch:88 Loss:2.598569995827145 Accuracy:0.45743750780820847 Loss:1.7937231461207073 Accuracy:0.6183150410652161\n",
      "Epoch:89 Loss:2.6047207764216833\n",
      "Epoch:89 Loss:2.6047207764216833 Accuracy:0.45620096581322805 Loss:1.7778221368789673 Accuracy:0.6178571581840515\n",
      "Epoch:90 Loss:2.5957795143127442\n",
      "Epoch:90 Loss:2.5957795143127442 Accuracy:0.45625590682029726 Loss:1.7675747871398926 Accuracy:0.622710645198822\n",
      "Epoch:91 Loss:2.5796181575672046\n",
      "Epoch:91 Loss:2.5796181575672046 Accuracy:0.4591773305390332 Loss:1.763712207476298 Accuracy:0.6200549602508545\n",
      "Epoch:92 Loss:2.586525645520952\n",
      "Epoch:92 Loss:2.586525645520952 Accuracy:0.4576541731754939 Loss:1.7499175071716309 Accuracy:0.6262820760409037\n",
      "Epoch:93 Loss:2.577044323512486\n",
      "Epoch:93 Loss:2.577044323512486 Accuracy:0.45863424454416546 Loss:1.7355339924494426 Accuracy:0.6286630233128866\n",
      "Epoch:94 Loss:2.5628517468770347\n",
      "Epoch:94 Loss:2.5628517468770347 Accuracy:0.46154279592964387 Loss:1.7468465169270833 Accuracy:0.6243589917818705\n",
      "Epoch:95 Loss:2.5630817148420544\n",
      "Epoch:95 Loss:2.5630817148420544 Accuracy:0.46176045056846404 Loss:1.7360052267710369 Accuracy:0.6289377609888712\n",
      "Epoch:96 Loss:2.5493209634508407\n",
      "Epoch:96 Loss:2.5493209634508407 Accuracy:0.46142074125153676 Loss:1.7234376271565754 Accuracy:0.6288461685180664\n",
      "Epoch:97 Loss:2.5406149162186518\n",
      "Epoch:97 Loss:2.5406149162186518 Accuracy:0.4649941134783957 Loss:1.7210591634114583 Accuracy:0.6296703616778055\n",
      "Epoch:98 Loss:2.533443430491856\n",
      "Epoch:98 Loss:2.533443430491856 Accuracy:0.4644741186073848 Loss:1.7153956095377605 Accuracy:0.6331502000490824\n",
      "Epoch:99 Loss:2.5338687488010954\n",
      "Epoch:99 Loss:2.5338687488010954 Accuracy:0.4635400533676147 Loss:1.7094149986902873 Accuracy:0.6327838897705078\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(train_tokens,valid_tokens,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),f'{DATAPATH}/inter/varybptt_model_state_dict')\n",
    "torch.save(optimizer.state_dict(),f'{DATAPATH}/inter/varybptt_learner_state_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.unfreeze_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.weight.requires_grad, model.decoder.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Loss:2.455076994214739\n",
      "Epoch:0 Loss:2.455076994214739 Accuracy:0.4894116648605892 Loss:1.574017842610677 Accuracy:0.6905677914619446\n",
      "Epoch:1 Loss:2.362198153057614\n",
      "Epoch:1 Loss:2.362198153057614 Accuracy:0.5339623262753358 Loss:1.4725878238677979 Accuracy:0.7192308108011881\n",
      "Epoch:2 Loss:2.2712207356014766\n",
      "Epoch:2 Loss:2.2712207356014766 Accuracy:0.5521716805728706 Loss:1.3900961875915527 Accuracy:0.7353480259577433\n",
      "Epoch:3 Loss:2.1909215859004427\n",
      "Epoch:3 Loss:2.1909215859004427 Accuracy:0.5655832631247384 Loss:1.3382569551467896 Accuracy:0.7434982061386108\n",
      "Epoch:4 Loss:2.1503394874366553\n",
      "Epoch:4 Loss:2.1503394874366553 Accuracy:0.5730291685542545 Loss:1.2933137814203899 Accuracy:0.747619092464447\n",
      "Epoch:5 Loss:2.1151228646437326\n",
      "Epoch:5 Loss:2.1151228646437326 Accuracy:0.5771593401829401 Loss:1.2713415225346882 Accuracy:0.7546703616778055\n",
      "Epoch:6 Loss:2.0871008430208478\n",
      "Epoch:6 Loss:2.0871008430208478 Accuracy:0.5828100732394628 Loss:1.2303366263707478 Accuracy:0.7603480021158854\n",
      "Epoch:7 Loss:2.049787415398492\n",
      "Epoch:7 Loss:2.049787415398492 Accuracy:0.5878686490986083 Loss:1.2014473676681519 Accuracy:0.7624542514483134\n",
      "Epoch:8 Loss:2.0105047290389604\n",
      "Epoch:8 Loss:2.0105047290389604 Accuracy:0.5927373944102107 Loss:1.1757161617279053 Accuracy:0.7658425172170004\n",
      "Epoch:9 Loss:2.0002735687626734\n",
      "Epoch:9 Loss:2.0002735687626734 Accuracy:0.5935552600357268 Loss:1.1490439176559448 Accuracy:0.769505520661672\n",
      "Epoch:10 Loss:1.9624593257904053\n",
      "Epoch:10 Loss:1.9624593257904053 Accuracy:0.5986927230107156 Loss:1.118380864461263 Accuracy:0.7742674152056376\n",
      "Epoch:11 Loss:1.93988898396492\n",
      "Epoch:11 Loss:1.93988898396492 Accuracy:0.5997661103804907 Loss:1.1025839646657307 Accuracy:0.7743590275446574\n",
      "Epoch:12 Loss:1.9222989579041798\n",
      "Epoch:12 Loss:1.9222989579041798 Accuracy:0.6029154989454482 Loss:1.0795907974243164 Accuracy:0.7793040672938029\n",
      "Epoch:13 Loss:1.8930424054463704\n",
      "Epoch:13 Loss:1.8930424054463704 Accuracy:0.6075712260272768 Loss:1.067053000132243 Accuracy:0.7815934618314108\n",
      "Epoch:14 Loss:1.8972607089413538\n",
      "Epoch:14 Loss:1.8972607089413538 Accuracy:0.6069520993365182 Loss:1.0264872511227925 Accuracy:0.7886447310447693\n",
      "Epoch:15 Loss:1.84985171424018\n",
      "Epoch:15 Loss:1.84985171424018 Accuracy:0.6121854914559258 Loss:1.0155606071154277 Accuracy:0.7915751139322916\n",
      "Epoch:16 Loss:1.8313815924856398\n",
      "Epoch:16 Loss:1.8313815924856398 Accuracy:0.6158172984917959 Loss:0.9951638976732889 Accuracy:0.7942308187484741\n",
      "Epoch:17 Loss:1.8362487885687087\n",
      "Epoch:17 Loss:1.8362487885687087 Accuracy:0.6147409627834955 Loss:0.9769243200620016 Accuracy:0.7954212625821432\n",
      "Epoch:18 Loss:1.804358227385415\n",
      "Epoch:18 Loss:1.804358227385415 Accuracy:0.6182440453105502 Loss:0.9640217026074728 Accuracy:0.8003663420677185\n",
      "Epoch:19 Loss:1.782251216269828\n",
      "Epoch:19 Loss:1.782251216269828 Accuracy:0.6216929893235903 Loss:0.9513073563575745 Accuracy:0.8021978338559469\n",
      "Epoch:20 Loss:1.7739205701010567\n",
      "Epoch:20 Loss:1.7739205701010567 Accuracy:0.6226452384676252 Loss:0.9297193884849548 Accuracy:0.8076007763544718\n",
      "Epoch:21 Loss:1.753879223551069\n",
      "Epoch:21 Loss:1.753879223551069 Accuracy:0.6255494832992554 Loss:0.9041716853777567 Accuracy:0.8117216428120931\n",
      "Epoch:22 Loss:1.7346980383521633\n",
      "Epoch:22 Loss:1.7346980383521633 Accuracy:0.6288153748763236 Loss:0.8859140674273173 Accuracy:0.8145604928334554\n",
      "Epoch:23 Loss:1.7075832298823765\n",
      "Epoch:23 Loss:1.7075832298823765 Accuracy:0.6316248348781041 Loss:0.8699632485707601 Accuracy:0.8169414202372233\n",
      "Epoch:24 Loss:1.7091237620303505\n",
      "Epoch:24 Loss:1.7091237620303505 Accuracy:0.6317814008185738 Loss:0.8488061428070068 Accuracy:0.8201465606689453\n",
      "Epoch:25 Loss:1.6867163624082293\n",
      "Epoch:25 Loss:1.6867163624082293 Accuracy:0.6359183975628444 Loss:0.8370538353919983 Accuracy:0.8217949072519938\n",
      "Epoch:26 Loss:1.6663733141762869\n",
      "Epoch:26 Loss:1.6663733141762869 Accuracy:0.640071702003479 Loss:0.8187755544980367 Accuracy:0.8323260347048441\n",
      "Epoch:27 Loss:1.656964888443818\n",
      "Epoch:27 Loss:1.656964888443818 Accuracy:0.6399026625865215 Loss:0.8107272585233053 Accuracy:0.8279304305712382\n",
      "Epoch:28 Loss:1.6315841061728342\n",
      "Epoch:28 Loss:1.6315841061728342 Accuracy:0.6426995073046003 Loss:0.7970619996388754 Accuracy:0.8311355511347452\n",
      "Epoch:29 Loss:1.6379104091061487\n",
      "Epoch:29 Loss:1.6379104091061487 Accuracy:0.6428601609336005 Loss:0.7846249143282572 Accuracy:0.8325091997782389\n",
      "Epoch:30 Loss:1.6194271802902223\n",
      "Epoch:30 Loss:1.6194271802902223 Accuracy:0.6459105474608285 Loss:0.7624580264091492 Accuracy:0.8373626669247946\n",
      "Epoch:31 Loss:1.5880581481116158\n",
      "Epoch:31 Loss:1.5880581481116158 Accuracy:0.6510228259222848 Loss:0.7559120059013367 Accuracy:0.841117262840271\n",
      "Epoch:32 Loss:1.5810842249128554\n",
      "Epoch:32 Loss:1.5810842249128554 Accuracy:0.6497637927532196 Loss:0.7513285676638285 Accuracy:0.8394688963890076\n",
      "Epoch:33 Loss:1.5842366797583445\n",
      "Epoch:33 Loss:1.5842366797583445 Accuracy:0.6500863722392491 Loss:0.7312378287315369 Accuracy:0.8467033306757609\n",
      "Epoch:34 Loss:1.5609818895657857\n",
      "Epoch:34 Loss:1.5609818895657857 Accuracy:0.6555708514319526 Loss:0.7155296007792155 Accuracy:0.8502747615178426\n",
      "Epoch:35 Loss:1.5429258721215384\n",
      "Epoch:35 Loss:1.5429258721215384 Accuracy:0.657374267918723 Loss:0.7082926432291666 Accuracy:0.8475275238355001\n",
      "Epoch:36 Loss:1.5297872953944736\n",
      "Epoch:36 Loss:1.5297872953944736 Accuracy:0.6584823379913965 Loss:0.7038761178652445 Accuracy:0.8475275238355001\n",
      "Epoch:37 Loss:1.5367610189649794\n",
      "Epoch:37 Loss:1.5367610189649794 Accuracy:0.6564248866505094 Loss:0.7054798007011414 Accuracy:0.8451465566953024\n",
      "Epoch:38 Loss:1.5255574882030487\n",
      "Epoch:38 Loss:1.5255574882030487 Accuracy:0.659569631020228 Loss:0.67805415391922 Accuracy:0.8532051841417948\n",
      "Epoch:39 Loss:1.5028531449181692\n",
      "Epoch:39 Loss:1.5028531449181692 Accuracy:0.663233939238957 Loss:0.6736268599828085 Accuracy:0.8556776841481527\n",
      "Epoch:40 Loss:1.5063838263352711\n",
      "Epoch:40 Loss:1.5063838263352711 Accuracy:0.6622843643029531 Loss:0.663176159063975 Accuracy:0.8591575423876444\n",
      "Epoch:41 Loss:1.4847167375925425\n",
      "Epoch:41 Loss:1.4847167375925425 Accuracy:0.6666585174766747 Loss:0.6650002400080363 Accuracy:0.8547619382540385\n",
      "Epoch:42 Loss:1.4835407866372003\n",
      "Epoch:42 Loss:1.4835407866372003 Accuracy:0.6646085729201635 Loss:0.643807570139567 Accuracy:0.8587912519772848\n",
      "Epoch:43 Loss:1.461147665977478\n",
      "Epoch:43 Loss:1.461147665977478 Accuracy:0.6674804074423654 Loss:0.6387837727864584 Accuracy:0.8610806067784628\n",
      "Epoch:44 Loss:1.4472778605090246\n",
      "Epoch:44 Loss:1.4472778605090246 Accuracy:0.671344992187288 Loss:0.6303781668345133 Accuracy:0.8641026020050049\n",
      "Epoch:45 Loss:1.4444188731057304\n",
      "Epoch:45 Loss:1.4444188731057304 Accuracy:0.6722449285643441 Loss:0.6240981817245483 Accuracy:0.8645604848861694\n",
      "Epoch:46 Loss:1.4402674879346575\n",
      "Epoch:46 Loss:1.4402674879346575 Accuracy:0.671232715674809 Loss:0.6067822178204855 Accuracy:0.8686813513437907\n",
      "Epoch:47 Loss:1.4192199773258634\n",
      "Epoch:47 Loss:1.4192199773258634 Accuracy:0.6751845015419854 Loss:0.5936687390009562 Accuracy:0.8695055445035299\n",
      "Epoch:48 Loss:1.4161393063408987\n",
      "Epoch:48 Loss:1.4161393063408987 Accuracy:0.6769490429333278 Loss:0.5854109128316244 Accuracy:0.8752747774124146\n",
      "Epoch:49 Loss:1.397547431894251\n",
      "Epoch:49 Loss:1.397547431894251 Accuracy:0.6787093023996096 Loss:0.5846731066703796 Accuracy:0.8724359472592672\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(train_tokens,valid_tokens,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Loss:1.4005129079560976\n",
      "Epoch:0 Loss:1.4005129079560976 Accuracy:0.6779703530105384 Loss:0.5738116105397543 Accuracy:0.8760989507039388\n",
      "Epoch:1 Loss:1.384764916367001\n",
      "Epoch:1 Loss:1.384764916367001 Accuracy:0.6801707049210867 Loss:0.5655213793118795 Accuracy:0.8769231041272482\n",
      "Epoch:2 Loss:1.3963093314852033\n",
      "Epoch:2 Loss:1.3963093314852033 Accuracy:0.6796232666288103 Loss:0.5642988483111063 Accuracy:0.8788461883862814\n",
      "Epoch:3 Loss:1.3806347846984863\n",
      "Epoch:3 Loss:1.3806347846984863 Accuracy:0.6823077235903059 Loss:0.551474134127299 Accuracy:0.8780220150947571\n",
      "Epoch:4 Loss:1.3543897458485195\n",
      "Epoch:4 Loss:1.3543897458485195 Accuracy:0.6872370788029262 Loss:0.5427469611167908 Accuracy:0.8802197972933451\n",
      "Epoch:5 Loss:1.3410670586994715\n",
      "Epoch:5 Loss:1.3410670586994715 Accuracy:0.6885008113724845 Loss:0.5266106923421224 Accuracy:0.8869963884353638\n",
      "Epoch:6 Loss:1.3506292036601475\n",
      "Epoch:6 Loss:1.3506292036601475 Accuracy:0.6883438331740243 Loss:0.5227194825808207 Accuracy:0.8872710863749186\n",
      "Epoch:7 Loss:1.3496377672467912\n",
      "Epoch:7 Loss:1.3496377672467912 Accuracy:0.6878650222505842 Loss:0.5299135645230612 Accuracy:0.8821429212888082\n",
      "Epoch:8 Loss:1.3235021216528757\n",
      "Epoch:8 Loss:1.3235021216528757 Accuracy:0.6915412170546396 Loss:0.5142259895801544 Accuracy:0.8863553603490194\n",
      "Epoch:9 Loss:1.328170643533979\n",
      "Epoch:9 Loss:1.328170643533979 Accuracy:0.6913265636989049 Loss:0.512482633193334 Accuracy:0.8887363076210022\n",
      "Epoch:10 Loss:1.3186415566338434\n",
      "Epoch:10 Loss:1.3186415566338434 Accuracy:0.6913690782255597 Loss:0.5033677617708842 Accuracy:0.8891026178995768\n",
      "Epoch:11 Loss:1.3138748952320645\n",
      "Epoch:11 Loss:1.3138748952320645 Accuracy:0.6936656526156835 Loss:0.49693363904953003 Accuracy:0.892216165860494\n",
      "Epoch:12 Loss:1.2876069341387069\n",
      "Epoch:12 Loss:1.2876069341387069 Accuracy:0.6972944106374468 Loss:0.4910852015018463 Accuracy:0.8917582829793295\n",
      "Epoch:13 Loss:1.2802318062101092\n",
      "Epoch:13 Loss:1.2802318062101092 Accuracy:0.6991000890731811 Loss:0.48255101839701336 Accuracy:0.8962454597155253\n",
      "Epoch:14 Loss:1.297577828168869\n",
      "Epoch:14 Loss:1.297577828168869 Accuracy:0.6957142535183165 Loss:0.47212571899096173 Accuracy:0.8985348343849182\n",
      "Epoch:15 Loss:1.2843823364802769\n",
      "Epoch:15 Loss:1.2843823364802769 Accuracy:0.6971428871154786 Loss:0.4595799744129181 Accuracy:0.9035714666048685\n",
      "Epoch:16 Loss:1.2778519325786166\n",
      "Epoch:16 Loss:1.2778519325786166 Accuracy:0.6989917076296277 Loss:0.45382678508758545 Accuracy:0.900915781656901\n",
      "Epoch:17 Loss:1.2602326222828457\n",
      "Epoch:17 Loss:1.2602326222828457 Accuracy:0.7012490953717913 Loss:0.45058390498161316 Accuracy:0.9022893905639648\n",
      "Epoch:18 Loss:1.258663472202089\n",
      "Epoch:18 Loss:1.258663472202089 Accuracy:0.7007707903782526 Loss:0.4407772322495778 Accuracy:0.9054029782613119\n",
      "Epoch:19 Loss:1.2524375551276736\n",
      "Epoch:19 Loss:1.2524375551276736 Accuracy:0.70228178302447 Loss:0.4306013186772664 Accuracy:0.9054945508639017\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(train_tokens,valid_tokens,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Loss:1.2286284617015293\n",
      "Epoch:0 Loss:1.2286284617015293 Accuracy:0.7059439931597028 Loss:0.4248349368572235 Accuracy:0.906501849492391\n",
      "Epoch:1 Loss:1.228726397241865\n",
      "Epoch:1 Loss:1.228726397241865 Accuracy:0.7059327346937997 Loss:0.42250380913416546 Accuracy:0.9094322721163431\n",
      "Epoch:2 Loss:1.243957143080862\n",
      "Epoch:2 Loss:1.243957143080862 Accuracy:0.7039552519195958 Loss:0.4171498119831085 Accuracy:0.9095238447189331\n",
      "Epoch:3 Loss:1.2298704896654402\n",
      "Epoch:3 Loss:1.2298704896654402 Accuracy:0.7064207536833627 Loss:0.407338410615921 Accuracy:0.9103480180104574\n",
      "Epoch:4 Loss:1.2144546045197382\n",
      "Epoch:4 Loss:1.2144546045197382 Accuracy:0.7080510126219856 Loss:0.4001307984193166 Accuracy:0.9136447509129842\n",
      "Epoch:5 Loss:1.2084093425008986\n",
      "Epoch:5 Loss:1.2084093425008986 Accuracy:0.7102856387694677 Loss:0.40115875999132794 Accuracy:0.9130952755610148\n",
      "Epoch:6 Loss:1.206751935822623\n",
      "Epoch:6 Loss:1.206751935822623 Accuracy:0.7094348788261413 Loss:0.389671911795934 Accuracy:0.9185897707939148\n",
      "Epoch:7 Loss:1.1838096545802221\n",
      "Epoch:7 Loss:1.1838096545802221 Accuracy:0.7150634742445416 Loss:0.38970668117205304 Accuracy:0.9162088433901469\n",
      "Epoch:8 Loss:1.1964853500065051\n",
      "Epoch:8 Loss:1.1964853500065051 Accuracy:0.7126279630159077 Loss:0.3863823513189952 Accuracy:0.9189561009407043\n",
      "Epoch:9 Loss:1.194244452885219\n",
      "Epoch:9 Loss:1.194244452885219 Accuracy:0.7130534103938512 Loss:0.3774067560831706 Accuracy:0.9219780762990316\n",
      "Epoch:10 Loss:1.1852065971919468\n",
      "Epoch:10 Loss:1.1852065971919468 Accuracy:0.7143799407141549 Loss:0.37314552068710327 Accuracy:0.9207875927289327\n",
      "Epoch:11 Loss:1.1604336533281538\n",
      "Epoch:11 Loss:1.1604336533281538 Accuracy:0.7180539286798902 Loss:0.3701561192671458 Accuracy:0.920421282450358\n",
      "Epoch:12 Loss:1.1653702769960677\n",
      "Epoch:12 Loss:1.1653702769960677 Accuracy:0.7169616733278547 Loss:0.37052029371261597 Accuracy:0.9197802742322286\n",
      "Epoch:13 Loss:1.1772339795086835\n",
      "Epoch:13 Loss:1.1772339795086835 Accuracy:0.7153029764020765 Loss:0.3646429379781087 Accuracy:0.9221612215042114\n",
      "Epoch:14 Loss:1.1512002638408116\n",
      "Epoch:14 Loss:1.1512002638408116 Accuracy:0.7204883916037423 Loss:0.3597835997740428 Accuracy:0.9215201934178671\n",
      "Epoch:15 Loss:1.1525204082330067\n",
      "Epoch:15 Loss:1.1525204082330067 Accuracy:0.7193804366721047 Loss:0.35134804248809814 Accuracy:0.9239011406898499\n",
      "Epoch:16 Loss:1.1442707551492226\n",
      "Epoch:16 Loss:1.1442707551492226 Accuracy:0.7211516177332079 Loss:0.35278178254763287 Accuracy:0.9235348304112753\n",
      "Epoch:17 Loss:1.142245670727321\n",
      "Epoch:17 Loss:1.142245670727321 Accuracy:0.7221507396016802 Loss:0.359426607688268 Accuracy:0.9201465646425883\n",
      "Epoch:18 Loss:1.137427853213416\n",
      "Epoch:18 Loss:1.137427853213416 Accuracy:0.7221403039164014 Loss:0.34530360500017804 Accuracy:0.9235348304112753\n",
      "Epoch:19 Loss:1.119051295850012\n",
      "Epoch:19 Loss:1.119051295850012 Accuracy:0.7251908166540993 Loss:0.34305981794993085 Accuracy:0.9250000317891439\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(train_tokens,valid_tokens,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.034358394435676, 1.4049475905635938)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(1.11),np.exp(0.34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),f'{DATAPATH}/inter/varybptt_model_state_dict_unfreeze')\n",
    "torch.save(optimizer.state_dict(),f'{DATAPATH}/inter/varybptt_learner_state_dict_unfreeze')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kirana/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type language_model. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save (model,f'{DATAPATH}/inter/varybptt_model_awd_lstm')\n",
    "torch.save (optimizer,f'{DATAPATH}/inter/varybptt_optimizer_awd_lstm')\n",
    "torch.save (learner,f'{DATAPATH}/inter/varybptt_learner_awd_lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_lm_weights=model.encoder.weight.data.cpu().numpy()\n",
    "import pickle\n",
    "pickle.dump(pretrained_lm_weights,open(f'{DATAPATH}/inter/varybpttpretrained_lm_weights','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
