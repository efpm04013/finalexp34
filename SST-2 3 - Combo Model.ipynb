{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import os \n",
    "import fastai\n",
    "import fastai\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=\"/home/kirana/Documents/phd\"\n",
    "DATAPATH=\"/home/kirana/Documents/phd/data/experiment/SST_2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "[df_train,df_valid,itos, train_tokens, valid_tokens, trn_lm, val_lm]=pickle.load(open(f'{DATAPATH}/inter/dfs_tokens_fastai.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>dstype</th>\n",
       "      <th>words</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55236</th>\n",
       "      <td>exactly wrong</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>[ \\n , xxbos, xxfld, 1, exactly, wrong]</td>\n",
       "      <td>[3, 4, 5, 2, 628, 459]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100531</th>\n",
       "      <td>the brain</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>[ \\n , xxbos, xxfld, 1, the, brain]</td>\n",
       "      <td>[3, 4, 5, 2, 7, 808]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90848</th>\n",
       "      <td>routine action and jokes like this</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>[ \\n , xxbos, xxfld, 1, routine, action, and, ...</td>\n",
       "      <td>[3, 4, 5, 2, 1036, 117, 10, 389, 37, 27]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17512</th>\n",
       "      <td>If you like quirky , odd movies and\\/or the ir...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>[ \\n , xxbos, xxfld, 1, xxmaj, if, you, like, ...</td>\n",
       "      <td>[3, 4, 5, 2, 6, 65, 29, 37, 382, 8, 1037, 111,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107105</th>\n",
       "      <td>to accomplish what few sequels can</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>[ \\n , xxbos, xxfld, 1, to, accomplish, what, ...</td>\n",
       "      <td>[3, 4, 5, 2, 13, 7436, 64, 186, 1456, 72]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label dstype  \\\n",
       "55236                                       exactly wrong      1  train   \n",
       "100531                                          the brain      0  train   \n",
       "90848                  routine action and jokes like this      1  train   \n",
       "17512   If you like quirky , odd movies and\\/or the ir...      0  train   \n",
       "107105                 to accomplish what few sequels can      0  train   \n",
       "\n",
       "                                                    words  \\\n",
       "55236             [ \\n , xxbos, xxfld, 1, exactly, wrong]   \n",
       "100531                [ \\n , xxbos, xxfld, 1, the, brain]   \n",
       "90848   [ \\n , xxbos, xxfld, 1, routine, action, and, ...   \n",
       "17512   [ \\n , xxbos, xxfld, 1, xxmaj, if, you, like, ...   \n",
       "107105  [ \\n , xxbos, xxfld, 1, to, accomplish, what, ...   \n",
       "\n",
       "                                                   tokens  \n",
       "55236                              [3, 4, 5, 2, 628, 459]  \n",
       "100531                               [3, 4, 5, 2, 7, 808]  \n",
       "90848            [3, 4, 5, 2, 1036, 117, 10, 389, 37, 27]  \n",
       "17512   [3, 4, 5, 2, 6, 65, 29, 37, 382, 8, 1037, 111,...  \n",
       "107105          [3, 4, 5, 2, 13, 7436, 64, 186, 1456, 72]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([df_train,df_valid],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    117220\n",
       "test       1821\n",
       "valid       872\n",
       "Name: dstype, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['dstype'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df.loc[df['dstype']!='test']\n",
    "df_test=df.loc[df['dstype']=='test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((118092, 5), (1821, 5))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>dstype</th>\n",
       "      <th>words</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55236</th>\n",
       "      <td>exactly wrong</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>[ \\n , xxbos, xxfld, 1, exactly, wrong]</td>\n",
       "      <td>[3, 4, 5, 2, 628, 459]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100531</th>\n",
       "      <td>the brain</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>[ \\n , xxbos, xxfld, 1, the, brain]</td>\n",
       "      <td>[3, 4, 5, 2, 7, 808]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90848</th>\n",
       "      <td>routine action and jokes like this</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>[ \\n , xxbos, xxfld, 1, routine, action, and, ...</td>\n",
       "      <td>[3, 4, 5, 2, 1036, 117, 10, 389, 37, 27]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17512</th>\n",
       "      <td>If you like quirky , odd movies and\\/or the ir...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>[ \\n , xxbos, xxfld, 1, xxmaj, if, you, like, ...</td>\n",
       "      <td>[3, 4, 5, 2, 6, 65, 29, 37, 382, 8, 1037, 111,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107105</th>\n",
       "      <td>to accomplish what few sequels can</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>[ \\n , xxbos, xxfld, 1, to, accomplish, what, ...</td>\n",
       "      <td>[3, 4, 5, 2, 13, 7436, 64, 186, 1456, 72]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label dstype  \\\n",
       "55236                                       exactly wrong      1  train   \n",
       "100531                                          the brain      0  train   \n",
       "90848                  routine action and jokes like this      1  train   \n",
       "17512   If you like quirky , odd movies and\\/or the ir...      0  train   \n",
       "107105                 to accomplish what few sequels can      0  train   \n",
       "\n",
       "                                                    words  \\\n",
       "55236             [ \\n , xxbos, xxfld, 1, exactly, wrong]   \n",
       "100531                [ \\n , xxbos, xxfld, 1, the, brain]   \n",
       "90848   [ \\n , xxbos, xxfld, 1, routine, action, and, ...   \n",
       "17512   [ \\n , xxbos, xxfld, 1, xxmaj, if, you, like, ...   \n",
       "107105  [ \\n , xxbos, xxfld, 1, to, accomplish, what, ...   \n",
       "\n",
       "                                                   tokens  \n",
       "55236                              [3, 4, 5, 2, 628, 459]  \n",
       "100531                               [3, 4, 5, 2, 7, 808]  \n",
       "90848            [3, 4, 5, 2, 1036, 117, 10, 389, 37, 27]  \n",
       "17512   [3, 4, 5, 2, 6, 65, 29, 37, 382, 8, 1037, 111,...  \n",
       "107105          [3, 4, 5, 2, 13, 7436, 64, 186, 1456, 72]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_train['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=52 # 52 - Jeremey, 20 - default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt= 70 #70 - Jeremey, 35 - default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>dstype</th>\n",
       "      <th>words</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55236</th>\n",
       "      <td>exactly wrong</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>[ \\n , xxbos, xxfld, 1, exactly, wrong]</td>\n",
       "      <td>[3, 4, 5, 2, 628, 459]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100531</th>\n",
       "      <td>the brain</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>[ \\n , xxbos, xxfld, 1, the, brain]</td>\n",
       "      <td>[3, 4, 5, 2, 7, 808]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90848</th>\n",
       "      <td>routine action and jokes like this</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>[ \\n , xxbos, xxfld, 1, routine, action, and, ...</td>\n",
       "      <td>[3, 4, 5, 2, 1036, 117, 10, 389, 37, 27]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17512</th>\n",
       "      <td>If you like quirky , odd movies and\\/or the ir...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>[ \\n , xxbos, xxfld, 1, xxmaj, if, you, like, ...</td>\n",
       "      <td>[3, 4, 5, 2, 6, 65, 29, 37, 382, 8, 1037, 111,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107105</th>\n",
       "      <td>to accomplish what few sequels can</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>[ \\n , xxbos, xxfld, 1, to, accomplish, what, ...</td>\n",
       "      <td>[3, 4, 5, 2, 13, 7436, 64, 186, 1456, 72]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label dstype  \\\n",
       "55236                                       exactly wrong      1  train   \n",
       "100531                                          the brain      0  train   \n",
       "90848                  routine action and jokes like this      1  train   \n",
       "17512   If you like quirky , odd movies and\\/or the ir...      0  train   \n",
       "107105                 to accomplish what few sequels can      0  train   \n",
       "\n",
       "                                                    words  \\\n",
       "55236             [ \\n , xxbos, xxfld, 1, exactly, wrong]   \n",
       "100531                [ \\n , xxbos, xxfld, 1, the, brain]   \n",
       "90848   [ \\n , xxbos, xxfld, 1, routine, action, and, ...   \n",
       "17512   [ \\n , xxbos, xxfld, 1, xxmaj, if, you, like, ...   \n",
       "107105  [ \\n , xxbos, xxfld, 1, to, accomplish, what, ...   \n",
       "\n",
       "                                                   tokens  \n",
       "55236                              [3, 4, 5, 2, 628, 459]  \n",
       "100531                               [3, 4, 5, 2, 7, 808]  \n",
       "90848            [3, 4, 5, 2, 1036, 117, 10, 389, 37, 27]  \n",
       "17512   [3, 4, 5, 2, 6, 65, 29, 37, 382, 8, 1037, 111,...  \n",
       "107105          [3, 4, 5, 2, 13, 7436, 64, 186, 1456, 72]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    64559\n",
       "1    53533\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6614\n",
       "1    5378\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_unk_', '_pad_', '1', ' \\n ', 'xxbos']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "padlen=1400\n",
    "padding_idx=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kirana/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_train['n_tok']=df_train['tokens'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    118092.000000\n",
       "mean         14.795202\n",
       "std           9.114027\n",
       "min           5.000000\n",
       "25%           8.000000\n",
       "50%          12.000000\n",
       "75%          19.000000\n",
       "max          66.000000\n",
       "Name: n_tok, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['n_tok'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kirana/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_train.sort_values(by='n_tok', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid['n_tok']=df_valid['tokens'].apply(len)\n",
    "df_valid.sort_values(by='n_tok', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kirana/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/kirana/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df_test['n_tok']=df_test['tokens'].apply(len)\n",
    "df_test.sort_values(by='n_tok', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 4. 5. 2. ... 1. 1. 1. 1.]\n",
      "1\n",
      "1400\n",
      "[3. 4. 5. 2. ... 1. 1. 1. 1.]\n",
      "0\n",
      "1400\n",
      "[3. 4. 5. 2. ... 1. 1. 1. 1.]\n",
      "1\n",
      "1400\n",
      "[3. 4. 5. 2. ... 1. 1. 1. 1.]\n",
      "1\n",
      "1400\n",
      "[3. 4. 5. 2. ... 1. 1. 1. 1.]\n",
      "1\n",
      "1400\n",
      "[3. 4. 5. 2. ... 1. 1. 1. 1.]\n",
      "1\n",
      "1400\n",
      "[3. 4. 5. 2. ... 1. 1. 1. 1.]\n",
      "1\n",
      "1400\n",
      "[3. 4. 5. 2. ... 1. 1. 1. 1.]\n",
      "1\n",
      "1400\n",
      "[3. 4. 5. 2. ... 1. 1. 1. 1.]\n",
      "0\n",
      "1400\n",
      "[3. 4. 5. 2. ... 1. 1. 1. 1.]\n",
      "1\n",
      "1400\n",
      "[3. 4. 5. 2. ... 1. 1. 1. 1.]\n",
      "1\n",
      "1400\n",
      "[3. 4. 5. 2. ... 1. 1. 1. 1.]\n",
      "0\n",
      "1400\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,20):\n",
    "    x=df_train['tokens'].values[i]\n",
    "    y=df_train['label'].values[i]    \n",
    "    out=np.ones(padlen)\n",
    "    if len(x) < padlen:\n",
    "        out[:len(x)]=x\n",
    "    else:\n",
    "        out=x[:padlen]\n",
    "    print (out)\n",
    "    print (y)\n",
    "    print (len(out))\n",
    "    if i>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118092, 6)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    64559\n",
       " 1    53533\n",
       " Name: label, dtype: int64, 0    6614\n",
       " 1    5378\n",
       " Name: label, dtype: int64, 1    912\n",
       " 0    909\n",
       " Name: label, dtype: int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].value_counts(), df_valid['label'].value_counts(),df_test['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((118092, 6), (11992, 6))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1400"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ds_sentiment (Dataset):\n",
    "    def __init__ (self,df,bs,padlen=64,xvar='tokens',yvar='label',len_var='n_tok',padding_idx=1):\n",
    "        self.x,self.y,self.padlen,self.padding_idx,self.len_var,self.bs=\\\n",
    "            df[xvar],df[yvar],padlen,padding_idx,df[len_var],bs\n",
    "        self.len_var=self.len_var.clip(0,padlen)\n",
    "    \n",
    "    def pad (self,x):\n",
    "        out=np.ones(self.padlen)*self.padding_idx\n",
    "        out=out.astype(int)\n",
    "        if len(x)>=self.padlen:\n",
    "            out[:]=x[:self.padlen]\n",
    "        else:\n",
    "            out[:len(x)]=x\n",
    "        return out\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        return self.pad(self.x.iloc[idx]),self.y.iloc[idx],self.len_var.iloc[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#padlen=max(df_train['n_tok'])\n",
    "padlen=1400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1400"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstrain=ds_sentiment(df_train,bs,padlen)\n",
    "dsvalid=ds_sentiment(df_valid,bs,padlen)\n",
    "dstest=ds_sentiment(df_test,bs,padlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dltrain=DataLoader(dstrain,bs,True)\n",
    "dlvalid=DataLoader(dsvalid,bs,False)\n",
    "dltest=DataLoader(dstest,bs,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xb,yb,xlen in dltrain:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3, 4, 5,  ..., 1, 1, 1],\n",
       "         [3, 4, 5,  ..., 1, 1, 1],\n",
       "         [3, 4, 5,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [3, 4, 5,  ..., 1, 1, 1],\n",
       "         [3, 4, 5,  ..., 1, 1, 1],\n",
       "         [3, 4, 5,  ..., 1, 1, 1]]),\n",
       " tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,\n",
       "         1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "         0, 0, 0, 0]),\n",
       " tensor([12, 12, 15,  6,  5,  7, 12,  7, 19, 23, 51, 12,  9,  8, 40, 11, 12, 13,\n",
       "         18,  8,  6,  8, 11, 17, 29,  6,  6, 11, 32, 11,  7, 30,  6,  8,  8, 10,\n",
       "         17, 11, 22, 13,  6,  5, 10,  8, 31, 10, 13, 13,  8, 11, 34, 19]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb, xlen"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cyclical learning rates\n",
    "\tvary learning rate by epoch between two values\n",
    "\t\n",
    "\n",
    "bptt=70\n",
    "n_emb=400\n",
    "n_hid=1150\n",
    "n_layers=3\n",
    "bs=48 # whatever is the max that can fit in memory\n",
    "\n",
    "# shuffle dataset\n",
    "# sort the data by length\n",
    "\n",
    "\n",
    "class ds_sentiment(Dataset):\n",
    "\tdef __init__(self,x,y):\n",
    "\t\tself.x,self.y=x,y\n",
    "\t\n",
    "\tdef __getitem__(self,idx):\n",
    "\t\treturn self.x[idx],self.y[idx]\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 70)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs,bptt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inp=len(itos)\n",
    "n_emb=400 #650\n",
    "n_hidden=400#400\n",
    "n_layers= 2 # 2\n",
    "dropout=0.5 # 0.5\n",
    "wd=1e-7\n",
    "bidirectional=True\n",
    "dropout_e=0.2 # 0.5 - changing to 0.4, 0.3 or any dropout value did not make much difference\n",
    "dropout_o=0.5 #0.5\n",
    "n_out=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([52, 1400]), torch.Size([52]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape,xlen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    64559\n",
       "1    53533\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_unk_', '_pad_', '1', ' \\n ', 'xxbos']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 8])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.AdaptiveAvgPool1d(5)\n",
    "input = torch.randn(1, 64, 8)\n",
    "input.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 5])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(input).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentiment_classifier (nn.Module):\n",
    "    def __init__(self,n_inp,n_emb,n_hidden,n_layers,bidirectional,bs,device,dropout_e=0.05,dropout=0.5,\\\n",
    "                 dropout_o=0.5,pretrain_mtx=None,n_out=1,padding_idx=1,n_filters=100,filter_sizes=[3,4,5]):\n",
    "        super().__init__()\n",
    "        self.n_inp,self.n_emb,self.n_hidden,self.n_layers,self.bidirectional,self.bs,self.device,self.pretrain_mtx,self.padding_idx=\\\n",
    "                            n_inp,n_emb,n_hidden,n_layers,bidirectional,bs,device,pretrain_mtx,padding_idx\n",
    "        self.n_out,self.n_filters,self.filter_sizes=n_out,n_filters,filter_sizes\n",
    "        self.dropout_e,self.dropout,self.dropout_o=dropout_e,dropout,dropout_o\n",
    "        \n",
    "        self.create_architecture()\n",
    "        if pretrain_mtx is not None:\n",
    "            print (f'initializing glove with {pretrain_mtx.shape}')\n",
    "            self.initialize_glove()\n",
    "        self.init_hidden()\n",
    "        self.criterion=nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def set_dropouts(self, dropout, dropout_o, dropout_e):\n",
    "        self.dropout, self.dropout_o, self.dropout_e = dropout, dropout_o, dropout_e\n",
    "    \n",
    "    \n",
    "    def freeze_embedding(self):\n",
    "        self.encoder.weight.requires_grad=False\n",
    "         \n",
    "    def unfreeze_embedding(self):\n",
    "        self.encoder.weight.requires_grad=True\n",
    "\n",
    "    def initialize_glove(self):\n",
    "        self.encoder.weight.data.copy_(torch.Tensor(self.pretrain_mtx))\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # Initialize hidden\n",
    "        self.hidden=(Variable(torch.zeros(self.n_layers,self.bs,self.n_hidden,requires_grad=False).to(self.device)),\n",
    "                     Variable(torch.zeros(self.n_layers,self.bs,self.n_hidden,requires_grad=False).to(self.device)))\n",
    "    \n",
    "\n",
    "    def create_architecture(self):\n",
    "        ###################################\n",
    "        # Embedding layer - common to both\n",
    "        ###################################\n",
    "        self.dropout_enc=nn.Dropout(self.dropout_e)\n",
    "        self.encoder=nn.Embedding(self.n_inp,self.n_emb,padding_idx=self.padding_idx)\n",
    "        \n",
    "        #######################################\n",
    "        # For RNN #############################\n",
    "        #######################################\n",
    "        # Embedding Layer: Embedding layer just maps each word to an index. n_inp to n_emb mapping is all it does\n",
    "            # input to this is of shape n_batch * n_seq\n",
    "         # LSTM Layer\n",
    "        self.lstm=nn.LSTM(self.n_emb,self.n_hidden,self.n_layers,batch_first=True,dropout=self.dropout,\\\n",
    "                          bidirectional=self.bidirectional)\n",
    "          # embs are going to be of shape n_batch * n_seq * n_emb\n",
    "        self.dropout_op=nn.Dropout(self.dropout_o)\n",
    "        \n",
    "        self.avg_pool1d=torch.nn.AdaptiveAvgPool1d(1)\n",
    "        self.max_pool1d=torch.nn.AdaptiveMaxPool1d(1)\n",
    "        \n",
    "    \n",
    "        #######################################\n",
    "        # For CNN #############################\n",
    "        #######################################    \n",
    "        #embedding dimension is the \"depth\" of the filter and the number of tokens in the sentence is the width.\n",
    "        self.conv_0=torch.nn.Conv1d (self.n_emb,self.n_filters,kernel_size=self.filter_sizes[0])\n",
    "        self.conv_1=torch.nn.Conv1d (self.n_emb,self.n_filters,kernel_size=self.filter_sizes[1])\n",
    "        self.conv_2=torch.nn.Conv1d(self.n_emb,self.n_filters,kernel_size=self.filter_sizes[2])\n",
    "        \n",
    "        self.fc=nn.Linear(len(self.filter_sizes)*self.n_filters+self.n_hidden*4,self.n_out)\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward (self,Xb,Yb,Xb_lengths):\n",
    "        \n",
    "        ####RNN PORTION\n",
    "        embs=self.dropout_enc(self.encoder(Xb))\n",
    "        if Xb.size(0) < self.bs:\n",
    "            self.hidden=(self.hidden[0][:,:Xb.size(0),:].contiguous(),\n",
    "            self.hidden[1][:,:Xb.size(0),:].contiguous())\n",
    "        packed_embs = pack_padded_sequence(embs,Xb_lengths,batch_first=True, enforce_sorted=False)\n",
    "        lstm_out,(hidden,cell)=self.lstm(packed_embs)\n",
    "        lstm_out,lengths=pad_packed_sequence(lstm_out,batch_first=True)\n",
    "        hidden = self.dropout_op(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        avg_pool=self.avg_pool1d(lstm_out.permute(0,2,1)).view(Xb.size(0),-1)\n",
    "        max_pool=self.max_pool1d(lstm_out.permute(0,2,1)).view(Xb.size(0),-1)\n",
    "        \n",
    "        #CNN Portion\n",
    "        new_embs=embs.permute(0,2,1)        \n",
    "        conved_0=torch.relu(self.conv_0(new_embs))\n",
    "        conved_1=torch.relu(self.conv_1(new_embs))\n",
    "        conved_2=torch.relu(self.conv_2(new_embs)) \n",
    "        max_pool1d=torch.nn.MaxPool1d(conved_0.shape[2])\n",
    "        pooled_0=max_pool1d(conved_0).squeeze(2)\n",
    "        max_pool1d=torch.nn.MaxPool1d(conved_1.shape[2])\n",
    "        pooled_1=max_pool1d(conved_1).squeeze(2)\n",
    "        max_pool1d=torch.nn.MaxPool1d(conved_2.shape[2])\n",
    "        pooled_2=max_pool1d(conved_2).squeeze(2)\n",
    "        cat_cnn = self.dropout_op(torch.cat([pooled_0,pooled_1,pooled_2],dim=1))\n",
    "        \n",
    "        ## Concatenate\n",
    "        big_out=torch.cat([cat_cnn,hidden,max_pool],dim=1)\n",
    "        preds=self.fc(big_out)\n",
    "\n",
    "        loss=self.criterion(preds,Yb.contiguous().float().view(-1,1))\n",
    "\n",
    "        \n",
    "        return preds.view(-1),loss\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_binomial(preds,actual, device=\"cpu\", cutoff=0.5):\n",
    "    preds=torch.sigmoid(preds)\n",
    "    zeros=torch.zeros(len(preds)).to(device)\n",
    "    ones = torch.ones(len(preds)).to(device)\n",
    "\n",
    "    preds=torch.where(preds>cutoff,ones,zeros)\n",
    "    correct=torch.round(preds).long()==actual\n",
    "    return correct.float().sum()/len(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_binomial(preds, y, device=\"cpu\", cutoff=0.5):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    y=y.float()\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_lm_weights=pickle.load(open(f'{DATAPATH}/inter/varybpttpretrained_lm_weights','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if model forward works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16206, 400, 400, 2, True, 52, 'cpu', 0.2, 0.5, 0.5, 1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_inp,n_emb,n_hidden,n_layers,bidirectional,bs,device,dropout_e,dropout_o,dropout,n_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing glove with (16206, 400)\n"
     ]
    }
   ],
   "source": [
    "model_sentiment=sentiment_classifier (n_inp,n_emb,n_hidden,n_layers,bidirectional,bs,device,dropout_e=dropout_e,dropout=dropout,\\\n",
    "                 dropout_o=dropout_o,pretrain_mtx=pretrained_lm_weights,n_out=1,padding_idx=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 13,377,401 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {count_parameters(model_sentiment):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment_classifier(\n",
       "  (dropout_enc): Dropout(p=0.2)\n",
       "  (encoder): Embedding(16206, 400, padding_idx=1)\n",
       "  (lstm): LSTM(400, 400, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "  (dropout_op): Dropout(p=0.5)\n",
       "  (avg_pool1d): AdaptiveAvgPool1d(output_size=1)\n",
       "  (max_pool1d): AdaptiveMaxPool1d(output_size=1)\n",
       "  (conv_0): Conv1d(400, 100, kernel_size=(3,), stride=(1,))\n",
       "  (conv_1): Conv1d(400, 100, kernel_size=(4,), stride=(1,))\n",
       "  (conv_2): Conv1d(400, 100, kernel_size=(5,), stride=(1,))\n",
       "  (fc): Linear(in_features=1900, out_features=1, bias=True)\n",
       "  (criterion): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0913,  0.2028,  0.1955,  0.0666,  0.0973,  0.1095,  0.1677,  0.1011,\n",
       "          0.2282,  0.0662,  0.1162,  0.1889,  0.3111,  0.3090,  0.3414,  0.1239,\n",
       "          0.2064,  0.1587,  0.3076,  0.2383,  0.1050,  0.2128,  0.1956,  0.2649,\n",
       "          0.0613,  0.0504, -0.0365,  0.0047,  0.2606,  0.0542,  0.1052,  0.1514,\n",
       "          0.0614,  0.1064,  0.1071,  0.1445,  0.1342,  0.2717,  0.1774,  0.2164,\n",
       "          0.1520,  0.1030, -0.0641,  0.2737,  0.4493,  0.0790,  0.2542,  0.1322,\n",
       "          0.1005,  0.2995,  0.0244,  0.2588], grad_fn=<ViewBackward>),\n",
       " tensor(0.7113, grad_fn=<BinaryCrossEntropyWithLogitsBackward>))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sentiment.forward(xb,yb,xlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds,loss=model_sentiment.forward(xb,yb,xlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2081,  0.2063,  0.2703,  0.1055,  0.1278,  0.1643,  0.1241,  0.1315,\n",
       "         0.1483,  0.1771,  0.2232,  0.1870,  0.1492,  0.0223,  0.2462,  0.1787,\n",
       "         0.0300,  0.1415,  0.3247,  0.1125,  0.1255,  0.2631,  0.2229,  0.2371,\n",
       "         0.0840, -0.0616,  0.1579,  0.0863,  0.2747,  0.2379,  0.2112,  0.1756,\n",
       "         0.0942,  0.0804,  0.0555,  0.2014,  0.3159,  0.2950,  0.2507,  0.1369,\n",
       "         0.0615,  0.0718,  0.2729,  0.1516, -0.0664,  0.1887,  0.1910,  0.2037,\n",
       "         0.1685,  0.3118,  0.3335,  0.1391], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,\n",
       "        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "        0, 0, 0, 0])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([52])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([52])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4423)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_binomial(preds.to(device),yb.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4761904761904762"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(yb,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01301 , -0.105954, -0.285109, -0.044226, ..., -0.053858, -0.01929 ,  0.043896,  0.068098],\n",
       "       [ 0.00827 , -0.032289,  0.043977, -0.048435, ..., -0.179876,  0.008289, -0.005907, -0.028031],\n",
       "       [-0.23123 ,  0.223312, -0.131881,  0.020966, ..., -0.048621,  0.039325,  0.008791,  0.165966],\n",
       "       [ 0.094656,  0.297782,  0.098514,  0.054791, ..., -0.028909, -0.073314, -0.230205,  0.217315],\n",
       "       ...,\n",
       "       [ 0.300145, -0.141749,  0.076261,  0.071102, ..., -0.198638,  0.188576,  0.521759,  0.338996],\n",
       "       [-0.29253 , -0.366779, -0.062945, -0.49639 , ..., -0.151594, -0.152283,  0.324542, -0.003129],\n",
       "       [-0.12251 , -0.28435 , -0.002887,  0.009289, ...,  0.321684,  0.055438, -0.03748 , -0.035654],\n",
       "       [ 0.398688, -0.061674, -0.069627, -0.229945, ..., -0.649922, -0.712466, -0.19228 , -0.392854]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_lm_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1e-07\n",
      "1 6e-07\n",
      "2 3.6e-06\n",
      "3 2.16e-05\n",
      "4 0.0001296\n",
      "5 0.0007776\n"
     ]
    }
   ],
   "source": [
    "# Weight Decay Schedule\n",
    "tempstart=1e-7\n",
    "for i in range (6):\n",
    "    print (i, tempstart)\n",
    "    tempstart=tempstart*6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.005\n",
      "1 0.005\n",
      "2 0.0034999999999999996\n",
      "3 0.0017149999999999995\n",
      "4 0.0005882449999999997\n",
      "5 0.0001412376244999999\n"
     ]
    }
   ],
   "source": [
    "tempstart=5e-3\n",
    "for i in range (6):\n",
    "    print (i, tempstart)\n",
    "    tempstart=tempstart*(0.7**i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(1,5,(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs=[1e-2,5e-3,1e-4,5e-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(0,4,(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrs[torch.randint(0,4,(1,))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self,model,optimizer,metric_fn,device,bptt=12,print_every=5,clip_val=None,\\\n",
    "                 cycle_mult=0,lr_decay=0.7,wd_mult=6):\n",
    "        self.model,self.optimizer,self.metric_fn,self.device,self.print_every,self.bptt,self.losses,self.clip_val=\\\n",
    "            model,optimizer,metric_fn,device,print_every,bptt,[],clip_val\n",
    "        self.n_epochs=1\n",
    "        self.cycle_mult,self.lr_decay=cycle_mult,lr_decay\n",
    "        self.wd_mult=wd_mult\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            self.start_lr=param_group['lr']\n",
    "            self.start_wd=param_group['weight_decay']\n",
    "        self.wd=self.start_wd\n",
    "        self.lr=self.start_lr\n",
    "        self.n_epoch=0\n",
    "        self.lrs=[1e-2,5e-3,1e-4,5e-4]\n",
    "        self.preds,self.preds_valid,self.trainY,self.actual=[],[],[],[]\n",
    "        \n",
    "    def fit (self,Xb,Yb,Xlen,mode_train=True):\n",
    "        if mode_train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            \n",
    "        preds,loss=self.model(Xb,Yb,Xlen)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            acc=self.metric_fn(preds,Yb.view(-1),self.device)\n",
    "            acc=acc.item()\n",
    "            \n",
    "            \n",
    "            if mode_train:\n",
    "                self.trainY.append(Yb.view(-1))\n",
    "                self.preds.append(preds.data)\n",
    "            else:\n",
    "                self.actual.append(Yb.view(-1))\n",
    "                self.preds_valid.append(preds.data)\n",
    "\n",
    "            \n",
    "            del preds\n",
    "        \n",
    "        if mode_train:\n",
    "            if 1==0:\n",
    "                lr =self.lrs[torch.randint(0,4,(1,))]\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr']=lr\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        myloss=loss.item()\n",
    "        del loss\n",
    "        \n",
    "        if self.clip_val is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip_val)\n",
    "        \n",
    "        return myloss, acc\n",
    "    \n",
    "    def lr_find (self,start_lr,end_lr,iterator,n_batch):\n",
    "        losses,lrs=[],[]\n",
    "        ratio=end_lr/start_lr\n",
    "        num_steps=n_batch\n",
    "        lr=start_lr\n",
    "        for i in range(num_steps):            \n",
    "            lr=lr*(end_lr/start_lr)**(1/num_steps)\n",
    "            lrs.append(lr)\n",
    "        self.lrs=lrs\n",
    "        self.run_epoch(iterator,mode_train=True,lrs=lrs)\n",
    "    \n",
    "    def run_epoch(self,iterator,mode_train,lrs=None):\n",
    "        epoch_loss,epoch_acc,i,k=0,0,0,0\n",
    "        self.model.init_hidden()\n",
    "        for Xb,Yb,Xlen in iterator:\n",
    "            Xb=Xb.to(self.device)\n",
    "            Yb=Yb.to(self.device)\n",
    "            Xlen=Xlen.to(self.device)\n",
    "            \n",
    "            if lrs is not None:\n",
    "                lr=lrs[k]\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr']=lr \n",
    "            \n",
    "\n",
    "            loss,acc=self.fit(Xb,Yb,Xlen,mode_train)\n",
    "            \n",
    "            if lrs is not None:\n",
    "                self.losses.append(loss)\n",
    "            \n",
    "            \n",
    "            epoch_loss+=loss\n",
    "            epoch_acc+=acc\n",
    "            \n",
    "            k=k+1\n",
    "            if k%self.print_every == 0:\n",
    "                if k:\n",
    "                    print (f'Batch:{k} {epoch_loss/(k)}  {epoch_acc/(k)}')  \n",
    "                    torch.cuda.empty_cache()\n",
    "        epoch_loss=epoch_loss/len(iterator)\n",
    "        epoch_acc=epoch_acc/len(iterator)\n",
    "            \n",
    "        return epoch_loss,epoch_acc\n",
    "    \n",
    "    def plot_lrs(self, n_roll=1):\n",
    "        import seaborn as sns\n",
    "        ax=sns.lineplot(x=self.lrs,y=pd.Series(self.losses).rolling(n_roll).mean())\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.set_xlabel('Learning Rate')\n",
    "\n",
    "     \n",
    "    def run_epochs(self,dltrain,dlvalid,n_epochs=1):\n",
    "        \n",
    "        if self.cycle_mult > 0:\n",
    "            reset_cycle=self.cycle_mult\n",
    "        \n",
    "        for epoch in range(n_epochs):                \n",
    "\n",
    "            \n",
    "            loss,acc=self.run_epoch(dltrain,True)\n",
    "            lossv,accv=self.run_epoch(dlvalid,mode_train=False)\n",
    "            print (f'Epoch:{epoch} Learning rate {self.lr} Weight Decay {self.wd} Train Loss:{loss} Train Accuracy:{acc} Valid Loss:{lossv} Valid Accuracy:{accv}')\n",
    "        \n",
    "            if self.cycle_mult:\n",
    "                if self.n_epoch==reset_cycle:\n",
    "                    self.lr=self.start_lr\n",
    "                    #self.wd=self.start_wd\n",
    "                    reset_cycle=self.n_epoch+reset_cycle\n",
    "                else:\n",
    "                    self.lr*=(self.lr_decay**self.n_epoch)  \n",
    "                    if self.n_epoch>1:\n",
    "                        self.wd*=self.wd_mult\n",
    "            self.n_epoch+=1\n",
    "                \n",
    "                \n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr']=self.lr\n",
    "                #param_group['weight_decay']=self.wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing glove with (16206, 400)\n"
     ]
    }
   ],
   "source": [
    "model_sentiment=sentiment_classifier (n_inp,n_emb,n_hidden,n_layers,bidirectional,bs,device,dropout_e,dropout,\\\n",
    "                 dropout_o,pretrain_mtx=pretrained_lm_weights,n_out=1,padding_idx=1)\n",
    "model_sentiment=model_sentiment.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(model_sentiment.parameters(),lr=5e-3,betas=(0.9,0.999), weight_decay=wd)\n",
    "#optimizer=torch.optim.SGD(model_sentiment.parameters(),lr=1e-2,momentum=0.9, weight_decay=wd)\n",
    "metric_fn=accuracy_binomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2271, 231)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dltrain),len(dlvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6614\n",
       "1    5378\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner=Learner(model_sentiment,optimizer,accuracy_binomial,device,bptt,100,0.25,cycle_mult=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sentiment.freeze_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sentiment.encoder.weight.requires_grad, learner.model.encoder.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2, 0.5, 0.5, 0.2, 0.5, 0.5)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sentiment.dropout_e,model_sentiment.dropout,model_sentiment.dropout_o, learner.model.dropout_e,learner.model.dropout,learner.model.dropout_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:100 0.5909485232830047  0.6980769488215447\n",
      "Batch:200 0.5485953556001186  0.7303846408426762\n",
      "Batch:300 0.525516549795866  0.7436538725097974\n",
      "Batch:400 0.49789963245391844  0.7605769506841898\n",
      "Batch:500 0.4850561005473137  0.768423104584217\n",
      "Batch:600 0.4723142488549153  0.7769872075815996\n",
      "Batch:700 0.4634835795845304  0.7825275009444782\n",
      "Batch:800 0.45274866512045264  0.7884375285729766\n",
      "Batch:900 0.4439710036913554  0.79331199483739\n",
      "Batch:1000 0.4381228656321764  0.7971538756787777\n",
      "Batch:1100 0.4309715035557747  0.801206323531541\n",
      "Batch:1200 0.42554636576523386  0.804775671089689\n",
      "Batch:1300 0.42124695611687807  0.8076035802868696\n",
      "Batch:1400 0.4151538156185831  0.8103434365774904\n",
      "Batch:1500 0.41074812969565394  0.8128974660038948\n",
      "Batch:1600 0.408701460454613  0.8141707031987607\n",
      "Batch:1700 0.40425703863010687  0.8165271797425606\n",
      "Batch:1800 0.40019029789500765  0.8185470390154256\n",
      "Batch:1900 0.3971103906317761  0.8204453747366605\n",
      "Batch:2000 0.3940261724367738  0.822076953843236\n",
      "Batch:2100 0.3902474709706647  0.824395635340895\n",
      "Batch:2200 0.38689054751260715  0.8262587721916762\n",
      "Batch:100 0.2553937724977732  0.8892308074235916\n",
      "Batch:200 0.2544248292967677  0.8933654218912125\n",
      "Epoch:0 Learning rate 0.005 Weight Decay 1e-07 Train Loss:0.38470822475597966 Train Accuracy:0.8273295708056433 Valid Loss:0.25036002566546073 Valid Accuracy:0.8940955253390522\n",
      "Batch:100 0.28284230284392836  0.8794231122732162\n",
      "Batch:200 0.29016159262508157  0.8750000339746475\n",
      "Batch:300 0.29984277081986266  0.8717949068546296\n",
      "Batch:400 0.29684764610603454  0.8732211899757385\n",
      "Batch:500 0.2975061613321304  0.8730769591331482\n",
      "Batch:600 0.2982543294255932  0.8727884974082311\n",
      "Batch:700 0.29639994431819233  0.873214321732521\n",
      "Batch:800 0.296489964351058  0.8736058051884175\n",
      "Batch:900 0.2958096620523267  0.8739102926519182\n",
      "Batch:1000 0.29494277327507734  0.8746154207587242\n",
      "Batch:1100 0.296262419027361  0.8747203157164833\n",
      "Batch:1200 0.29551618434488774  0.8750801642239093\n",
      "Batch:1300 0.29525622360408305  0.8755917520247973\n",
      "Batch:1400 0.2948709858794297  0.8758379477688244\n",
      "Batch:1500 0.29459173360963664  0.8761538820664088\n",
      "Batch:1600 0.29411627997644246  0.8762740743160248\n",
      "Batch:1700 0.2934876419297036  0.876674244018162\n",
      "Batch:1800 0.2942674123206072  0.8763782409495777\n",
      "Batch:1900 0.2940182417042946  0.8764474042779521\n",
      "Batch:2000 0.2939473562166095  0.876567343622446\n",
      "Batch:2100 0.29312200655539833  0.8768590104012263\n",
      "Batch:2200 0.2931443282182921  0.8768182176893408\n",
      "Batch:100 0.21015928894281388  0.9150000423192978\n",
      "Batch:200 0.20515758957713842  0.9202885010838509\n",
      "Epoch:1 Learning rate 0.005 Weight Decay 1e-07 Train Loss:0.2930822762222292 Train Accuracy:0.8767825443279581 Valid Loss:0.2001748157753831 Valid Accuracy:0.9223901500433555\n",
      "Batch:100 0.23027719482779502  0.9009615784883499\n",
      "Batch:200 0.2411829739063978  0.8971154230833054\n",
      "Batch:300 0.24078487547735372  0.8969231144587199\n",
      "Batch:400 0.23852467987686396  0.8982692696154118\n",
      "Batch:500 0.23655294105410576  0.899269269824028\n",
      "Batch:600 0.23918389021108546  0.8992949107289314\n",
      "Batch:700 0.2403484038689307  0.8990385003600802\n",
      "Batch:800 0.24000937886536122  0.8995433080941438\n",
      "Batch:900 0.23963994171884326  0.9000000390079287\n",
      "Batch:1000 0.2405536453947425  0.8999423466920853\n",
      "Batch:1100 0.24165130627426235  0.8997028362209146\n",
      "Batch:1200 0.2410838484764099  0.8999519621332487\n",
      "Batch:1300 0.2395445684228952  0.9005917548216307\n",
      "Batch:1400 0.2389206034370831  0.9012225663661957\n",
      "Batch:1500 0.2388594433615605  0.901474397778511\n",
      "Batch:1600 0.23903747853823007  0.9015505195781589\n",
      "Batch:1700 0.23812496178728693  0.9021606723701253\n",
      "Batch:1800 0.2380552454251382  0.9022329448991352\n",
      "Batch:1900 0.23804382862229095  0.902287488360154\n",
      "Batch:2000 0.23805886355414987  0.9023269619941712\n",
      "Batch:2100 0.2380910364041726  0.90233520368735\n",
      "Batch:2200 0.2380464550920508  0.9023951436985623\n",
      "Batch:100 0.1710422272235155  0.9325000387430191\n",
      "Batch:200 0.16633219508454203  0.9364423453807831\n",
      "Epoch:2 Learning rate 0.0034999999999999996 Weight Decay 1e-07 Train Loss:0.2379388633815269 Train Accuracy:0.9024066370208585 Valid Loss:0.1669628590474397 Valid Accuracy:0.935637316404483\n",
      "Batch:100 0.20331147488206625  0.91980772793293\n",
      "Batch:200 0.19603828758001327  0.9213461896777153\n",
      "Batch:300 0.19406132387618225  0.9215384987990062\n",
      "Batch:400 0.1960297990683466  0.9203846529126167\n",
      "Batch:500 0.19592132487148045  0.9208077301979065\n",
      "Batch:600 0.1964779742496709  0.9208013199766477\n",
      "Batch:700 0.1948856200756771  0.9219780602625438\n",
      "Batch:800 0.19280064085032791  0.9230288846045732\n",
      "Batch:900 0.1924569834354851  0.923290636671914\n",
      "Batch:1000 0.19284391094744205  0.9231538843512536\n",
      "Batch:1100 0.19207586987452074  0.9235664719343185\n",
      "Batch:1200 0.1927496600865076  0.9232692693670591\n",
      "Batch:1300 0.1918873573352511  0.9235651270701335\n",
      "Batch:1400 0.19009188483336142  0.9240659723537309\n",
      "Batch:1500 0.18922555677096048  0.9245769612789154\n",
      "Batch:1600 0.1897500500176102  0.92427888430655\n",
      "Batch:1700 0.18978751016232898  0.9241629340718774\n",
      "Batch:1800 0.18933786464027233  0.9244231151209937\n",
      "Batch:1900 0.18879756645152443  0.9245850583754088\n",
      "Batch:2000 0.1884321577847004  0.924682730525732\n",
      "Batch:2100 0.18874551136756226  0.9246886829535167\n",
      "Batch:2200 0.1883527594940229  0.924860178232193\n",
      "Batch:100 0.1304183379560709  0.9517308038473129\n",
      "Batch:200 0.1199548623058945  0.9567308032512665\n",
      "Epoch:3 Learning rate 0.0017149999999999995 Weight Decay 6e-07 Train Loss:0.18749909163774608 Train Accuracy:0.9252447627914576 Valid Loss:0.11527738693569388 Valid Accuracy:0.958104429306922\n",
      "Batch:100 0.14708869844675065  0.9400000375509262\n",
      "Batch:200 0.15263252045959233  0.9400000369548798\n",
      "Batch:300 0.15268314825991788  0.9392949096361796\n",
      "Batch:400 0.1518371883314103  0.940817344635725\n",
      "Batch:500 0.1532821033746004  0.9401538838148117\n",
      "Batch:600 0.15289363692204158  0.9404167037208875\n",
      "Batch:700 0.15203697539599878  0.940714322584016\n",
      "Batch:800 0.15149004321778192  0.9408413828909397\n",
      "Batch:900 0.15197985842823983  0.9408547376924091\n",
      "Batch:1000 0.15232004127278925  0.9410192677974701\n",
      "Batch:1100 0.15122895783321424  0.941521016034213\n",
      "Batch:1200 0.15046608459514876  0.9418750365575155\n",
      "Batch:1300 0.15039143048226833  0.9416272553572288\n",
      "Batch:1400 0.15122182087174485  0.9413049816659519\n",
      "Batch:1500 0.15120228093117474  0.9411795239448547\n",
      "Batch:1600 0.15126580727752298  0.9410216714069247\n",
      "Batch:1700 0.15173556454917964  0.9408597654454849\n",
      "Batch:1800 0.1521681523302363  0.9408226865530014\n",
      "Batch:1900 0.15224643693746706  0.9408603609235663\n",
      "Batch:2000 0.15166862823907287  0.9411058060526848\n",
      "Batch:2100 0.15172175414061972  0.9409798904259999\n",
      "Batch:2200 0.15191017316400326  0.9407867502624339\n",
      "Batch:100 0.11344302931800485  0.9578846472501755\n",
      "Batch:200 0.10100736583583057  0.9641346460580826\n",
      "Epoch:4 Learning rate 0.0005882449999999997 Weight Decay 3.6e-06 Train Loss:0.15205114772555614 Train Accuracy:0.94069034635417 Valid Loss:0.09718764302405444 Valid Accuracy:0.9652327152041645\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(dltrain,dlvalid,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kirana/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type sentiment_classifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model_sentiment.state_dict(),f'{DATAPATH}/inter/combo_ytsm')\n",
    "torch.save(optimizer.state_dict(),f'{DATAPATH}/inter/combo9_0.ytsm')\n",
    "torch.save (model_sentiment,f'{DATAPATH}/inter/combo9_ytsm')\n",
    "torch.save (optimizer,f'{DATAPATH}/inter/combo9_ytsm')\n",
    "torch.save (learner,f'{DATAPATH}/inter/combo9_ytsm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:100 0.1423038242198527  0.9440384972095489\n",
      "Batch:200 0.14198769419454038  0.9450961890816688\n",
      "Batch:300 0.13988345813627046  0.9464743942022323\n",
      "Batch:400 0.14080730407964437  0.9462019582092762\n",
      "Batch:500 0.1387798697091639  0.946884650349617\n",
      "Batch:600 0.13860795518072944  0.9465705478191375\n",
      "Batch:700 0.1391347512842289  0.9460989363704408\n",
      "Batch:800 0.13816526560811326  0.9467548429965973\n",
      "Batch:900 0.13883872881738676  0.9464743942684597\n",
      "Batch:1000 0.13895554983057082  0.9462884968519211\n",
      "Batch:1100 0.13851418417963116  0.946293741681359\n",
      "Batch:1200 0.13755214336172988  0.946570548315843\n",
      "Batch:1300 0.13789078454701947  0.9463609821979816\n",
      "Batch:1400 0.137262621938384  0.9467720135194915\n",
      "Batch:1500 0.13671974578251442  0.9469615738391877\n",
      "Batch:1600 0.1361846348445397  0.9470072471350431\n",
      "Batch:1700 0.13698934907422347  0.9465498093647116\n",
      "Batch:1800 0.1366460019350052  0.9468162747886446\n",
      "Batch:1900 0.1369487076233092  0.9466599545980755\n",
      "Batch:2000 0.13681243092752993  0.946740420371294\n",
      "Batch:2100 0.13718079041334844  0.9466575449421293\n",
      "Batch:2200 0.13673536437775263  0.9467308050394059\n",
      "Batch:100 0.10832956230267882  0.9611538779735566\n",
      "Batch:200 0.09591758026741445  0.9667307996749878\n",
      "Epoch:0 Learning rate 0.0001412376244999999 Weight Decay 2.16e-05 Train Loss:0.13657679439832768 Train Accuracy:0.9468296262620567 Valid Loss:0.09273614863022453 Valid Accuracy:0.967646967022966\n",
      "Batch:100 0.13654862420633435  0.9448077321052551\n",
      "Batch:200 0.1298855850752443  0.948750037252903\n",
      "Batch:300 0.1269521251755456  0.9512820873657862\n",
      "Batch:400 0.12874581065960228  0.9500000365078449\n",
      "Batch:500 0.1298091477677226  0.9496154210567475\n",
      "Batch:600 0.13238382877781987  0.9487179852525394\n",
      "Batch:700 0.13291967188939452  0.9481044314588819\n",
      "Batch:800 0.13304572162451223  0.9477644589543343\n",
      "Batch:900 0.1323099651746452  0.9481410616636277\n",
      "Batch:1000 0.1329092039298266  0.9479038822054863\n",
      "Batch:1100 0.13305368416519328  0.9478671688925137\n",
      "Batch:1200 0.13236142345548918  0.9483173433939616\n",
      "Batch:1300 0.13245340490427154  0.9483432309902632\n",
      "Batch:1400 0.1330269648733416  0.9480632226381983\n",
      "Batch:1500 0.13277117243533332  0.9483205486138662\n",
      "Batch:1600 0.13243678883300164  0.9483774394914508\n",
      "Batch:1700 0.13205236027345937  0.9482918910068624\n",
      "Batch:1800 0.13186490233366688  0.9483867877390649\n",
      "Batch:1900 0.13206680428824927  0.9481579303741455\n",
      "Batch:2000 0.1315128903388977  0.9483269586265087\n",
      "Batch:2100 0.13172763920522162  0.9482051638194493\n",
      "Batch:2200 0.13166310990906574  0.9483479376814582\n",
      "Batch:100 0.10714427499100566  0.9607692635059357\n",
      "Batch:200 0.09441671180538833  0.9670192608237267\n",
      "Epoch:1 Learning rate 2.3737807549714975e-05 Weight Decay 0.0001296 Train Loss:0.13166581188012383 Train Accuracy:0.9483115215597568 Valid Loss:0.09114302655286861 Valid Accuracy:0.9678134667924988\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(dltrain,dlvalid,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_sentiment.state_dict(),f'{DATAPATH}/inter/model_sentiment_state_dict')\n",
    "torch.save(optimizer.state_dict(),f'{DATAPATH}/inter/optimizer_state_dict')\n",
    "torch.save (model_sentiment,f'{DATAPATH}/inter/model_sentiment')\n",
    "torch.save (optimizer,f'{DATAPATH}/inter/optimizer')\n",
    "torch.save (learner,f'{DATAPATH}/inter/learner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sentiment=torch.load (f'{DATAPATH}/inter/model_sentiment_state_dict')\n",
    "optimizer=torch.load (f'{DATAPATH}/inter/optimizer_state_dict')\n",
    "learner=torch.load (f'{DATAPATH}/inter/model_sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001296"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sentiment.wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "preds=list(chain.from_iterable(learner.preds))[-df_train.shape[0]:]\n",
    "preds_valid=list(chain.from_iterable(learner.preds_valid))[-df_valid.shape[0]:]\n",
    "trainY=list(chain.from_iterable(learner.trainY))[-df_train.shape[0]:]\n",
    "actual=list(chain.from_iterable(learner.actual))[-df_valid.shape[0]:]\n",
    "preds=[x.item() for x in preds]\n",
    "preds_valid=[x.item() for x in preds_valid]\n",
    "trainY=[x.item() for x in trainY]\n",
    "actual=[x.item() for x in actual]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score,precision_score, recall_score, f1_score\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9891491058761531, 0.9949269740432497)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(trainY,preds),roc_auc_score(actual,preds_valid),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9430490763202091, 0.9643845727994095)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(trainY,np.round(expit(preds))), f1_score(actual,np.round(expit(preds_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9420470855779447, 0.9571428571428572)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(trainY,np.round(expit(preds))), precision_score(actual,np.round(expit(preds_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9440532008293949, 0.9717367050948308)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(trainY,np.round(expit(preds))), recall_score(actual,np.round(expit(preds_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_wd=learner.wd*.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Learner' object has no attribute 'unfreeze_embedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-6da8a0a3ed89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_sentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munfreeze_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Learner' object has no attribute 'unfreeze_embedding'"
     ]
    }
   ],
   "source": [
    "model_sentiment.unfreeze_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sentiment.encoder.weight.requires_grad, learner.model.encoder.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:100 0.1789311107620597  0.9311538875102997\n",
      "Batch:200 0.2329713966138661  0.9111538860201835\n",
      "Batch:300 0.2509736227616668  0.9042949108282725\n",
      "Batch:400 0.26191826361231507  0.8983173455297947\n",
      "Batch:500 0.26753545679897067  0.8957692681550979\n",
      "Batch:600 0.26879613279675446  0.8953846523165703\n",
      "Batch:700 0.27038188688989195  0.8940934434107372\n",
      "Batch:800 0.27164643675554545  0.8934856137633324\n",
      "Batch:900 0.27330568608310485  0.8929060198863348\n",
      "Batch:1000 0.27197415528446434  0.8935769602656365\n",
      "Batch:1100 0.27094284764067694  0.8936364008621736\n",
      "Batch:1200 0.26861331348617873  0.8943429856995742\n",
      "Batch:1300 0.2671618191553996  0.8947337649877255\n",
      "Batch:1400 0.265266539560897  0.8957280593258994\n",
      "Batch:1500 0.26431826198597747  0.8959102937777838\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-b5341e41f170>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdltrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdlvalid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-71-9f574d708c77>\u001b[0m in \u001b[0;36mrun_epochs\u001b[0;34m(self, dltrain, dlvalid, n_epochs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdltrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m             \u001b[0mlossv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdlvalid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch:{epoch} Learning rate {self.lr} Weight Decay {self.wd} Train Loss:{loss} Train Accuracy:{acc} Valid Loss:{lossv} Valid Accuracy:{accv}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-9f574d708c77>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(self, iterator, mode_train, lrs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mYb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mXlen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlrs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-9f574d708c77>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, Xb, Yb, Xlen, mode_train)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mmyloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learner.run_epochs(dltrain,dlvalid,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_sentiment.state_dict(),f'{DATAPATH}/inter/combo_ytsm_nofreeze')\n",
    "torch.save(optimizer.state_dict(),f'{DATAPATH}/inter/combo9_0.ytsm_nofreeze')\n",
    "torch.save (model_sentiment,f'{DATAPATH}/inter/combo9_ytsm_nofreeze')\n",
    "torch.save (optimizer,f'{DATAPATH}/inter/combo9_ytsm_nofreeze')\n",
    "torch.save (learner,f'{DATAPATH}/inter/combo9_ytsm_nofreeze')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9867109634551495, 0.987012987012987)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
