{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import os \n",
    "import fastai\n",
    "import fastai\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=\"/home/kirana/Documents/phd\"\n",
    "DATAPATH=\"/home/kirana/Documents/phd/data/experiment/SST_2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "[df_train,df_valid,itos, train_tokens, valid_tokens, trn_lm, val_lm]=pickle.load(open(f'{DATAPATH}/inter/dfs_tokens_fastai.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=52 # 52 - Jeremey, 20 - default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt= 70 #70 - Jeremey, 35 - default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_lm(tokens,bs):\n",
    "    import itertools\n",
    "    # Collapse into a single large array\n",
    "    tokens=np.asarray(list (itertools.chain(*tokens)))\n",
    "    # How many batches\n",
    "    n_batch=len(tokens)//bs\n",
    "    # Truncate to exclude the ones at the end\n",
    "    tokens=tokens[:bs*n_batch]\n",
    "    # Reshape\n",
    "    tokens=tokens.reshape(bs,-1)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52, 30919), (52, 3417))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens=create_data_lm(df_train['tokens'],bs)\n",
    "valid_tokens=create_data_lm(df_valid['tokens'],bs)\n",
    "train_tokens.shape, valid_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i):\n",
    "    seq_len = min(args.bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52, 70) (52, 70)\n"
     ]
    }
   ],
   "source": [
    "n_batch=train_tokens.shape[1]\n",
    "for i in range(0,n_batch,bptt):\n",
    "    seq_len=min(bptt,n_batch-1-i)\n",
    "    x=train_tokens[:,i:i+seq_len]\n",
    "    y=train_tokens[:,i+1:i+1+seq_len]\n",
    "    print (x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52, 70), (52, 70))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   3,    4,    5,    2, ...,  285,   31,   99, 1242],\n",
       "        [   3,    4,    5,    2, ...,    6, 7542,   19,   23],\n",
       "        [   5,    2,    9,  261, ...,    5,    2,    8,   28],\n",
       "        [3427,    8, 1543,   10, ...,  692,   11, 2006,   14],\n",
       "        ...,\n",
       "        [  13, 2068,    7,  352, ...,   66,    8,    7,  588],\n",
       "        [   2,   16,  227,  166, ...,   16, 4688,  273, 7273],\n",
       "        [  52,   22,  428,   43, ..., 1152,   18,   79, 2276],\n",
       "        [1442,    6, 1150,   39, ...,   12,    3,    4,    5]]),\n",
       " array([[   4,    5,    2,  628, ...,   31,   99, 1242,   11],\n",
       "        [   4,    5,    2,    7, ..., 7542,   19,   23, 7543],\n",
       "        [   2,    9,  261,  172, ...,    2,    8,   28,   36],\n",
       "        [   8, 1543,   10, 4003, ...,   11, 2006,   14,   19],\n",
       "        ...,\n",
       "        [2068,    7,  352,   11, ...,    8,    7,  588,  105],\n",
       "        [  16,  227,  166, 1336, ..., 4688,  273, 7273,  293],\n",
       "        [  22,  428,   43,   10, ...,   18,   79, 2276,   21],\n",
       "        [   6, 1150,   39,    8, ...,    3,    4,    5,    2]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52, 70), (52, 70))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 30919)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-trained AWD-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_AWD_LSTM='/home/kirana/Documents/phd/data/pre-trained/awd_lstm/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bwd_wt103_enc.h5  fwd_wt103_enc.h5  itos_wt103.pkl  \u001b[0m\u001b[01;34mwt103_60002\u001b[0m/\r\n",
      "bwd_wt103.h5      fwd_wt103.h5      \u001b[01;34mwt103_238642\u001b[0m/   \u001b[01;31mwt103_tiny.tgz\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "ls {PATH_AWD_LSTM}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts = torch.load(f'{PATH_AWD_LSTM}/fwd_wt103.h5', map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.encoder.weight',\n",
       "              tensor([[-1.2274e-01,  2.7886e-01, -3.8850e-01,  ..., -1.0404e-01,\n",
       "                        1.9580e-02,  1.8548e-01],\n",
       "                      [ 1.4854e-05, -2.3424e-05,  1.9693e-05,  ...,  2.1349e-05,\n",
       "                        2.1776e-05, -1.2394e-05],\n",
       "                      [ 1.8070e-01,  1.5874e+00, -1.1738e-01,  ..., -4.5935e-02,\n",
       "                       -8.1352e-02,  1.8054e-01],\n",
       "                      ...,\n",
       "                      [-1.8595e-03, -6.8529e-03,  1.6999e-03,  ...,  1.7039e-03,\n",
       "                        4.1632e-03, -1.3171e-03],\n",
       "                      [-2.3120e-03, -6.9001e-03,  1.8772e-03,  ...,  5.0309e-04,\n",
       "                        4.6596e-03, -2.5850e-03],\n",
       "                      [-2.2463e-03, -9.1512e-03,  1.3927e-03,  ...,  1.2296e-03,\n",
       "                        5.8085e-03, -1.8940e-03]])),\n",
       "             ('0.encoder_with_dropout.embed.weight',\n",
       "              tensor([[-1.2274e-01,  2.7886e-01, -3.8850e-01,  ..., -1.0404e-01,\n",
       "                        1.9580e-02,  1.8548e-01],\n",
       "                      [ 1.4854e-05, -2.3424e-05,  1.9693e-05,  ...,  2.1349e-05,\n",
       "                        2.1776e-05, -1.2394e-05],\n",
       "                      [ 1.8070e-01,  1.5874e+00, -1.1738e-01,  ..., -4.5935e-02,\n",
       "                       -8.1352e-02,  1.8054e-01],\n",
       "                      ...,\n",
       "                      [-1.8595e-03, -6.8529e-03,  1.6999e-03,  ...,  1.7039e-03,\n",
       "                        4.1632e-03, -1.3171e-03],\n",
       "                      [-2.3120e-03, -6.9001e-03,  1.8772e-03,  ...,  5.0309e-04,\n",
       "                        4.6596e-03, -2.5850e-03],\n",
       "                      [-2.2463e-03, -9.1512e-03,  1.3927e-03,  ...,  1.2296e-03,\n",
       "                        5.8085e-03, -1.8940e-03]])),\n",
       "             ('0.rnns.0.module.weight_ih_l0',\n",
       "              tensor([[-0.0812, -0.0811, -0.0937,  ..., -0.0259, -0.1403, -0.3247],\n",
       "                      [ 0.1154,  0.1142,  0.0938,  ..., -0.0711,  0.1669, -0.0387],\n",
       "                      [-0.0051,  0.1007,  0.2071,  ..., -0.0860, -0.0288, -0.0894],\n",
       "                      ...,\n",
       "                      [ 0.0055,  0.0157,  0.2990,  ...,  0.0616,  0.1159, -0.4737],\n",
       "                      [ 0.0181,  0.0426,  0.1130,  ...,  0.3529, -0.0114, -0.0125],\n",
       "                      [-0.0167, -0.1328,  0.1741,  ...,  0.0548, -0.0045,  0.1688]])),\n",
       "             ('0.rnns.0.module.bias_ih_l0',\n",
       "              tensor([ 0.1503, -0.4701, -0.1885,  ..., -0.5919, -0.2172, -0.1207])),\n",
       "             ('0.rnns.0.module.bias_hh_l0',\n",
       "              tensor([ 0.1503, -0.4701, -0.1885,  ..., -0.5919, -0.2172, -0.1207])),\n",
       "             ('0.rnns.0.module.weight_hh_l0_raw',\n",
       "              tensor([[-0.1013,  0.1786, -0.0528,  ...,  0.0741,  0.0306,  0.2467],\n",
       "                      [ 0.1780, -0.0853, -0.0243,  ..., -0.1129, -0.1310, -0.1498],\n",
       "                      [ 0.0661, -0.0496,  0.0921,  ...,  0.1829,  0.0533, -0.1525],\n",
       "                      ...,\n",
       "                      [-0.0322, -0.0704,  0.1653,  ...,  0.2142, -0.0558,  0.0315],\n",
       "                      [-0.1651, -0.0290,  0.1748,  ..., -0.0446,  0.5444,  0.0616],\n",
       "                      [ 0.0905, -0.1704, -0.0053,  ..., -0.0057,  0.2269,  0.0328]])),\n",
       "             ('0.rnns.1.module.weight_ih_l0',\n",
       "              tensor([[ 0.3307,  0.0385,  0.0860,  ...,  0.0685, -0.0444,  0.0539],\n",
       "                      [ 0.0720,  0.1607,  0.0562,  ...,  0.0276,  0.0613,  0.1632],\n",
       "                      [-0.1565, -0.1168,  0.1897,  ..., -0.0357,  0.0296,  0.0961],\n",
       "                      ...,\n",
       "                      [-0.0897, -0.1464, -0.0760,  ...,  0.0536,  0.0422, -0.0580],\n",
       "                      [ 0.1166, -0.1534, -0.1784,  ..., -0.0689,  0.2170,  0.1461],\n",
       "                      [-0.0413,  0.0689,  0.0581,  ..., -0.0640, -0.1703, -0.0945]])),\n",
       "             ('0.rnns.1.module.bias_ih_l0',\n",
       "              tensor([-0.8577, -0.6784, -0.7249,  ..., -0.6782,  0.0567, -0.5026])),\n",
       "             ('0.rnns.1.module.bias_hh_l0',\n",
       "              tensor([-0.8577, -0.6784, -0.7249,  ..., -0.6782,  0.0567, -0.5026])),\n",
       "             ('0.rnns.1.module.weight_hh_l0_raw',\n",
       "              tensor([[-0.0273, -0.2277,  0.0782,  ...,  0.1355, -0.1282,  0.1669],\n",
       "                      [ 0.1218,  0.0017, -0.0998,  ..., -0.2085, -0.0686, -0.1389],\n",
       "                      [-0.3878, -0.0498, -0.1748,  ..., -0.4014,  0.1986, -0.4400],\n",
       "                      ...,\n",
       "                      [-0.2097, -0.4298,  0.3551,  ...,  0.0316, -0.1198,  0.1266],\n",
       "                      [ 0.0037, -0.0223,  0.0032,  ..., -0.2672, -0.3093, -0.0361],\n",
       "                      [-0.0464,  0.1664, -0.1348,  ...,  0.1600, -0.1138,  0.0845]])),\n",
       "             ('0.rnns.2.module.weight_ih_l0',\n",
       "              tensor([[-0.0741,  0.0447, -0.0744,  ..., -0.0419,  0.1600, -0.0553],\n",
       "                      [ 0.0270,  0.0118,  0.0449,  ...,  0.1165, -0.1080, -0.0681],\n",
       "                      [-0.1023, -0.1662, -0.0229,  ...,  0.1652, -0.1070,  0.0970],\n",
       "                      ...,\n",
       "                      [-0.0989, -0.4425, -0.0343,  ..., -0.1434,  0.5851, -0.0291],\n",
       "                      [ 0.0802, -0.1067,  0.2789,  ..., -0.0916, -0.2240,  0.1020],\n",
       "                      [-0.4078,  0.7220,  0.1142,  ...,  0.5287,  0.2035, -0.1811]])),\n",
       "             ('0.rnns.2.module.bias_ih_l0',\n",
       "              tensor([-0.3681, -0.9079, -0.1998,  ...,  0.8533,  0.3202,  1.2172])),\n",
       "             ('0.rnns.2.module.bias_hh_l0',\n",
       "              tensor([-0.3681, -0.9079, -0.1998,  ...,  0.8533,  0.3202,  1.2172])),\n",
       "             ('0.rnns.2.module.weight_hh_l0_raw',\n",
       "              tensor([[-0.0966,  0.0236, -0.0152,  ...,  0.0388, -0.0531, -0.0395],\n",
       "                      [-0.0328, -0.2217,  0.0028,  ...,  0.0143, -0.0368, -0.0085],\n",
       "                      [ 0.0167, -0.0081, -0.0561,  ...,  0.0125,  0.0442, -0.0139],\n",
       "                      ...,\n",
       "                      [-0.0212, -0.1034, -0.0106,  ..., -0.0561,  0.0200, -0.0157],\n",
       "                      [ 0.0183,  0.0364, -0.0251,  ..., -0.0240, -0.1150,  0.0046],\n",
       "                      [ 0.0100, -0.1824,  0.1076,  ..., -0.0269,  0.2733,  0.1846]])),\n",
       "             ('1.decoder.weight',\n",
       "              tensor([[-1.2274e-01,  2.7886e-01, -3.8850e-01,  ..., -1.0404e-01,\n",
       "                        1.9580e-02,  1.8548e-01],\n",
       "                      [ 1.4854e-05, -2.3424e-05,  1.9693e-05,  ...,  2.1349e-05,\n",
       "                        2.1776e-05, -1.2394e-05],\n",
       "                      [ 1.8070e-01,  1.5874e+00, -1.1738e-01,  ..., -4.5935e-02,\n",
       "                       -8.1352e-02,  1.8054e-01],\n",
       "                      ...,\n",
       "                      [-1.8595e-03, -6.8529e-03,  1.6999e-03,  ...,  1.7039e-03,\n",
       "                        4.1632e-03, -1.3171e-03],\n",
       "                      [-2.3120e-03, -6.9001e-03,  1.8772e-03,  ...,  5.0309e-04,\n",
       "                        4.6596e-03, -2.5850e-03],\n",
       "                      [-2.2463e-03, -9.1512e-03,  1.3927e-03,  ...,  1.2296e-03,\n",
       "                        5.8085e-03, -1.8940e-03]]))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts = torch.load(f'{PATH_AWD_LSTM}/fwd_wt103_enc.h5', map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('encoder.weight',\n",
       "              tensor([[-1.2274e-01,  2.7886e-01, -3.8850e-01,  ..., -1.0404e-01,\n",
       "                        1.9580e-02,  1.8548e-01],\n",
       "                      [ 1.4854e-05, -2.3424e-05,  1.9693e-05,  ...,  2.1349e-05,\n",
       "                        2.1776e-05, -1.2394e-05],\n",
       "                      [ 1.8070e-01,  1.5874e+00, -1.1738e-01,  ..., -4.5935e-02,\n",
       "                       -8.1352e-02,  1.8054e-01],\n",
       "                      ...,\n",
       "                      [-1.8595e-03, -6.8529e-03,  1.6999e-03,  ...,  1.7039e-03,\n",
       "                        4.1632e-03, -1.3171e-03],\n",
       "                      [-2.3120e-03, -6.9001e-03,  1.8772e-03,  ...,  5.0309e-04,\n",
       "                        4.6596e-03, -2.5850e-03],\n",
       "                      [-2.2463e-03, -9.1512e-03,  1.3927e-03,  ...,  1.2296e-03,\n",
       "                        5.8085e-03, -1.8940e-03]])),\n",
       "             ('encoder_with_dropout.embed.weight',\n",
       "              tensor([[-1.2274e-01,  2.7886e-01, -3.8850e-01,  ..., -1.0404e-01,\n",
       "                        1.9580e-02,  1.8548e-01],\n",
       "                      [ 1.4854e-05, -2.3424e-05,  1.9693e-05,  ...,  2.1349e-05,\n",
       "                        2.1776e-05, -1.2394e-05],\n",
       "                      [ 1.8070e-01,  1.5874e+00, -1.1738e-01,  ..., -4.5935e-02,\n",
       "                       -8.1352e-02,  1.8054e-01],\n",
       "                      ...,\n",
       "                      [-1.8595e-03, -6.8529e-03,  1.6999e-03,  ...,  1.7039e-03,\n",
       "                        4.1632e-03, -1.3171e-03],\n",
       "                      [-2.3120e-03, -6.9001e-03,  1.8772e-03,  ...,  5.0309e-04,\n",
       "                        4.6596e-03, -2.5850e-03],\n",
       "                      [-2.2463e-03, -9.1512e-03,  1.3927e-03,  ...,  1.2296e-03,\n",
       "                        5.8085e-03, -1.8940e-03]])),\n",
       "             ('rnns.0.module.weight_ih_l0',\n",
       "              tensor([[-0.0812, -0.0811, -0.0937,  ..., -0.0259, -0.1403, -0.3247],\n",
       "                      [ 0.1154,  0.1142,  0.0938,  ..., -0.0711,  0.1669, -0.0387],\n",
       "                      [-0.0051,  0.1007,  0.2071,  ..., -0.0860, -0.0288, -0.0894],\n",
       "                      ...,\n",
       "                      [ 0.0055,  0.0157,  0.2990,  ...,  0.0616,  0.1159, -0.4737],\n",
       "                      [ 0.0181,  0.0426,  0.1130,  ...,  0.3529, -0.0114, -0.0125],\n",
       "                      [-0.0167, -0.1328,  0.1741,  ...,  0.0548, -0.0045,  0.1688]])),\n",
       "             ('rnns.0.module.bias_ih_l0',\n",
       "              tensor([ 0.1503, -0.4701, -0.1885,  ..., -0.5919, -0.2172, -0.1207])),\n",
       "             ('rnns.0.module.bias_hh_l0',\n",
       "              tensor([ 0.1503, -0.4701, -0.1885,  ..., -0.5919, -0.2172, -0.1207])),\n",
       "             ('rnns.0.module.weight_hh_l0_raw',\n",
       "              tensor([[-0.1013,  0.1786, -0.0528,  ...,  0.0741,  0.0306,  0.2467],\n",
       "                      [ 0.1780, -0.0853, -0.0243,  ..., -0.1129, -0.1310, -0.1498],\n",
       "                      [ 0.0661, -0.0496,  0.0921,  ...,  0.1829,  0.0533, -0.1525],\n",
       "                      ...,\n",
       "                      [-0.0322, -0.0704,  0.1653,  ...,  0.2142, -0.0558,  0.0315],\n",
       "                      [-0.1651, -0.0290,  0.1748,  ..., -0.0446,  0.5444,  0.0616],\n",
       "                      [ 0.0905, -0.1704, -0.0053,  ..., -0.0057,  0.2269,  0.0328]])),\n",
       "             ('rnns.1.module.weight_ih_l0',\n",
       "              tensor([[ 0.3307,  0.0385,  0.0860,  ...,  0.0685, -0.0444,  0.0539],\n",
       "                      [ 0.0720,  0.1607,  0.0562,  ...,  0.0276,  0.0613,  0.1632],\n",
       "                      [-0.1565, -0.1168,  0.1897,  ..., -0.0357,  0.0296,  0.0961],\n",
       "                      ...,\n",
       "                      [-0.0897, -0.1464, -0.0760,  ...,  0.0536,  0.0422, -0.0580],\n",
       "                      [ 0.1166, -0.1534, -0.1784,  ..., -0.0689,  0.2170,  0.1461],\n",
       "                      [-0.0413,  0.0689,  0.0581,  ..., -0.0640, -0.1703, -0.0945]])),\n",
       "             ('rnns.1.module.bias_ih_l0',\n",
       "              tensor([-0.8577, -0.6784, -0.7249,  ..., -0.6782,  0.0567, -0.5026])),\n",
       "             ('rnns.1.module.bias_hh_l0',\n",
       "              tensor([-0.8577, -0.6784, -0.7249,  ..., -0.6782,  0.0567, -0.5026])),\n",
       "             ('rnns.1.module.weight_hh_l0_raw',\n",
       "              tensor([[-0.0273, -0.2277,  0.0782,  ...,  0.1355, -0.1282,  0.1669],\n",
       "                      [ 0.1218,  0.0017, -0.0998,  ..., -0.2085, -0.0686, -0.1389],\n",
       "                      [-0.3878, -0.0498, -0.1748,  ..., -0.4014,  0.1986, -0.4400],\n",
       "                      ...,\n",
       "                      [-0.2097, -0.4298,  0.3551,  ...,  0.0316, -0.1198,  0.1266],\n",
       "                      [ 0.0037, -0.0223,  0.0032,  ..., -0.2672, -0.3093, -0.0361],\n",
       "                      [-0.0464,  0.1664, -0.1348,  ...,  0.1600, -0.1138,  0.0845]])),\n",
       "             ('rnns.2.module.weight_ih_l0',\n",
       "              tensor([[-0.0741,  0.0447, -0.0744,  ..., -0.0419,  0.1600, -0.0553],\n",
       "                      [ 0.0270,  0.0118,  0.0449,  ...,  0.1165, -0.1080, -0.0681],\n",
       "                      [-0.1023, -0.1662, -0.0229,  ...,  0.1652, -0.1070,  0.0970],\n",
       "                      ...,\n",
       "                      [-0.0989, -0.4425, -0.0343,  ..., -0.1434,  0.5851, -0.0291],\n",
       "                      [ 0.0802, -0.1067,  0.2789,  ..., -0.0916, -0.2240,  0.1020],\n",
       "                      [-0.4078,  0.7220,  0.1142,  ...,  0.5287,  0.2035, -0.1811]])),\n",
       "             ('rnns.2.module.bias_ih_l0',\n",
       "              tensor([-0.3681, -0.9079, -0.1998,  ...,  0.8533,  0.3202,  1.2172])),\n",
       "             ('rnns.2.module.bias_hh_l0',\n",
       "              tensor([-0.3681, -0.9079, -0.1998,  ...,  0.8533,  0.3202,  1.2172])),\n",
       "             ('rnns.2.module.weight_hh_l0_raw',\n",
       "              tensor([[-0.0966,  0.0236, -0.0152,  ...,  0.0388, -0.0531, -0.0395],\n",
       "                      [-0.0328, -0.2217,  0.0028,  ...,  0.0143, -0.0368, -0.0085],\n",
       "                      [ 0.0167, -0.0081, -0.0561,  ...,  0.0125,  0.0442, -0.0139],\n",
       "                      ...,\n",
       "                      [-0.0212, -0.1034, -0.0106,  ..., -0.0561,  0.0200, -0.0157],\n",
       "                      [ 0.0183,  0.0364, -0.0251,  ..., -0.0240, -0.1150,  0.0046],\n",
       "                      [ 0.0100, -0.1824,  0.1076,  ..., -0.0269,  0.2733,  0.1846]]))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos2=pickle.load(open(f'{PATH_AWD_LSTM}/itos_wt103.pkl','rb'))\n",
    "stoi2 = collections.defaultdict(lambda: -1, { v: k for k, v in enumerate(itos2) })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([238462, 400])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgts['encoder.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_wgts = wgts['encoder.weight'].numpy() # converts np.ndarray from torch.FloatTensor.output shape: (238462, 400)\n",
    "row_m = enc_wgts.mean(0) # returns the average of the array elements along axis 0. output shape: (400,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((238462, 400), 16206)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_wgts.shape, len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_emb=enc_wgts.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_w = np.zeros((len(itos),n_emb), dtype=np.float32) # shape: (60002, 400)\n",
    "\n",
    "for i, w in enumerate(itos):\n",
    "    r = stoi2[w]\n",
    "    new_w[i] = enc_wgts[r] if r >= 0 else row_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(itos).difference(set(itos2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inp=len(itos)\n",
    "n_emb=400 #650\n",
    "n_hidden=400 #650\n",
    "n_layers=2\n",
    "dropout=0.5\n",
    "wd=1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class language_model (nn.Module):\n",
    "    def __init__(self,n_inp,n_emb,n_hidden,n_layers,bidirectional,bs,device,dropout_e=0.05,dropout=0.5,\\\n",
    "                 dropout_o=0.5,pretrain_mtx=None,adaptive_log_softmax=True,tie_weights=True):\n",
    "        super().__init__()\n",
    "        self.n_inp,self.n_emb,self.n_hidden,self.n_layers,self.bidirectional,self.bs,self.device,self.pretrain_mtx=\\\n",
    "                            n_inp,n_emb,n_hidden,n_layers,bidirectional,bs,device,pretrain_mtx\n",
    "        self.adaptive_log_softmax,self.tie_weights=adaptive_log_softmax,tie_weights\n",
    "        self.dropout_e,self.dropout,self.dropout_o=dropout_e,dropout,dropout_o\n",
    "        self.gen_hidden()\n",
    "        self.create_architecture()\n",
    "        if pretrain_mtx is not None:\n",
    "            print (\"initializing\")\n",
    "            self.initialize_glove()\n",
    "            \n",
    "        if self.adaptive_log_softmax is False:\n",
    "            self.criterion=nn.CrossEntropyLoss()\n",
    "        \n",
    "    def create_architecture(self):\n",
    "        # Dropout layer\n",
    "        self.dropout_enc=nn.Dropout(self.dropout_e)\n",
    "        # Embedding Layer\n",
    "        self.encoder=nn.Embedding(self.n_inp,self.n_emb)\n",
    "        # LSTM Layer\n",
    "        self.lstm=nn.LSTM(self.n_emb,self.n_hidden,self.n_layers,batch_first=True,dropout=self.dropout,\\\n",
    "                          bidirectional=False)\n",
    "        self.dropout_op=nn.Dropout(self.dropout_o)\n",
    "        \n",
    "        if self.adaptive_log_softmax:\n",
    "            # Adaptive Log Softmax Loss\n",
    "            self.adaptive_softmax=AdaptiveLogSoftmaxWithLoss(self.n_hidden,\n",
    "                                    self.n_inp,\n",
    "                                    cutoffs=[round(self.n_inp/15),3*round(self.n_inp/15)],\n",
    "                                    div_value=4,\n",
    "                                    get_full_prob=True)\n",
    "        else:\n",
    "            self.decoder=nn.Linear(self.n_hidden,self.n_inp)\n",
    "    \n",
    "    def freeze_embedding(self):\n",
    "        self.encoder.weight.requires_grad=False\n",
    "        if self.tie_weights:\n",
    "            self.decoder.weight.requires_grad=False\n",
    "        \n",
    "    \n",
    "    def unfreeze_embedding(self):\n",
    "        self.encoder.weight.requires_grad=True\n",
    "        if self.tie_weights:\n",
    "            self.decoder.weight.requires_grad=True\n",
    "        \n",
    "    def initialize_glove(self):\n",
    "        self.encoder.weight.data=torch.Tensor(self.pretrain_mtx)\n",
    "        if self.tie_weights:\n",
    "            self.decoder.weight=self.encoder.weight\n",
    "    \n",
    "    def gen_hidden(self):\n",
    "        # Initialize hidden\n",
    "        self.hidden=(Variable(torch.zeros(self.n_layers,self.bs,self.n_hidden,requires_grad=False).to(self.device)),\n",
    "                     Variable(torch.zeros(self.n_layers,self.bs,self.n_hidden,requires_grad=False).to(self.device)))\n",
    "    \n",
    "        \n",
    "    def forward(self,Xb,Yb):\n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        embs=self.dropout_enc(self.encoder(Xb))\n",
    "        if Xb.size(0) < self.bs:\n",
    "            self.hidden=(self.hidden[0][:,:Xb.size(0),:].contiguous(),\n",
    "            self.hidden[1][:,:Xb.size(0),:].contiguous())\n",
    "        out,new_hidden=self.lstm(embs,self.hidden)\n",
    "        out=self.dropout_op(out)\n",
    "         # Wrap the hidden state in a new tensor without the gradients\n",
    "        self.hidden=(Variable(new_hidden[0].data,requires_grad=False).to(self.device),\\\n",
    "                     Variable(new_hidden[1].data,requires_grad=False).to(self.device))\n",
    "        if self.adaptive_log_softmax:\n",
    "            out=out.reshape(out.size(0)*out.size(1),out.size(2))        # output is of shape n_batch * n_seq * n_hidden\n",
    "      \n",
    "            out=self.adaptive_softmax(out,Yb.view(-1))\n",
    "            loss=out.loss\n",
    "            preds=out.output_full\n",
    "        else:\n",
    "            #import pdb\n",
    "            #pdb.set_trace()\n",
    "            preds=self.decoder(out.contiguous().view(out.size(0)*out.size(1), out.size(2)))\n",
    "            loss=self.criterion(preds,Yb.contiguous().view(-1))\n",
    "\n",
    "        return preds, loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_multinomial(preds,actual):\n",
    "    preds=preds.max(1)[1]\n",
    "    correct=preds==actual\n",
    "    return correct.float().sum()/len(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing\n"
     ]
    }
   ],
   "source": [
    "device=\"cuda:0\"\n",
    "model=language_model(n_inp,n_emb,n_hidden,n_layers,False,bs,device,0.05,0.5,0.5,new_w,False,True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16206, 400)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16206, 400]), torch.Size([16206, 400]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.weight.data.shape,model.decoder.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16206, 400]), 400, 16206)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Linear(n_hidden,n_inp).weight.data.shape, n_hidden, n_inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test if model forward works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16206, 400]), torch.Size([16206, 400]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder.weight.shape,model.encoder.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 70)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1==0:\n",
    "    model.forward(torch.LongTensor(x),torch.LongTensor(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self,model,optimizer,metric_fn,device,bptt=12,print_every=5,clip_val=None):\n",
    "        self.model,self.optimizer,self.metric_fn,self.device,self.print_every,self.bptt,self.losses,self.clip_val=\\\n",
    "            model,optimizer,metric_fn,device,print_every,bptt,[],clip_val\n",
    "        self.n_epochs=1\n",
    "\n",
    "    \n",
    "    def fit (self,Xb,Yb,mode_train=True):\n",
    "        if mode_train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            \n",
    "        preds,loss=self.model(Xb,Yb)\n",
    "        \n",
    "       \n",
    "            \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            acc=self.metric_fn(preds,Yb.view(-1))\n",
    "            acc=acc.item()\n",
    "            del preds\n",
    "        \n",
    "        if mode_train:\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        myloss=loss.item()\n",
    "        del loss\n",
    "        \n",
    "        if self.clip_val is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.25)\n",
    "            if 1==0:\n",
    "                for p in self.model.parameters():\n",
    "                    p.data.add_(self.lr, p.grad.data)\n",
    "        \n",
    "        return myloss, acc\n",
    "    \n",
    "    def lr_find (self,start_lr,end_lr,iterator,n_batch):\n",
    "        losses,lrs=[],[]\n",
    "        ratio=end_lr/start_lr\n",
    "        num_steps=n_batch\n",
    "        lr=start_lr\n",
    "        for i in range(num_steps):            \n",
    "            lr=lr*(end_lr/start_lr)**(1/num_steps)\n",
    "            lrs.append(lr)\n",
    "        self.lrs=lrs\n",
    "        self.run_epoch(iterator,mode_train=True,lrs=lrs)\n",
    "    \n",
    "    def run_epoch(self,iterator,mode_train,lrs=None):\n",
    "        n_batch=iterator.shape[1]\n",
    "        epoch_loss,epoch_acc,i,k=0,0,0,0\n",
    "        self.model.gen_hidden()\n",
    "        #for k,i in enumerate(range(0,n_batch,self.bptt)):\n",
    "        n_batch=iterator.shape[1]\n",
    "        while i<n_batch-bptt:\n",
    "            if mode_train:\n",
    "                cust_bptt=self.bptt if np.random.random() < 0.95 else self.bptt//np.random.randint (2,4)\n",
    "            else:\n",
    "                cust_bptt=bptt\n",
    "            seq_len=min(cust_bptt,n_batch-1-i)\n",
    "            Xb=train_tokens[:,i:i+seq_len]\n",
    "            Yb=train_tokens[:,i+1:i+1+seq_len]\n",
    "            Xb=torch.LongTensor(Xb)\n",
    "            Yb=torch.LongTensor(Yb)\n",
    "            Xb=Xb.to(self.device)\n",
    "            Yb=Yb.to(self.device)\n",
    "            \n",
    "            if lrs is not None:\n",
    "                lr=lrs[k]\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr']=lr \n",
    "            \n",
    "\n",
    "            loss,acc=self.fit(Xb,Yb,mode_train)\n",
    "            \n",
    "            if lrs is not None:\n",
    "                self.losses.append(loss)\n",
    "            \n",
    "            \n",
    "            epoch_loss+=loss\n",
    "            epoch_acc+=acc\n",
    "            if k%self.print_every == 0:\n",
    "                if k:\n",
    "                    print (f'Batch:{k} {epoch_loss/(k)}  {epoch_acc/(k)}')  \n",
    "                    torch.cuda.empty_cache()\n",
    "            k=k+1\n",
    "            i=i+seq_len\n",
    "        epoch_loss=epoch_loss/k\n",
    "        epoch_acc=epoch_acc/k\n",
    "        \n",
    "        if 1==0:\n",
    "            lr /= 4.0\n",
    "            # Freeze all the layers initially\n",
    "            for param in resnet.parameters():\n",
    "                param.requires_grad=False\n",
    "            torch.save(resnet,'resnet')\n",
    "            torch.save(resnet.state_dict(),'resnet_state_dict')\n",
    "            resnet.load_state_dict(torch.load('resnet_state_dict'))\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr']=lr  \n",
    "            \n",
    "        return epoch_loss,epoch_acc\n",
    "    \n",
    "    def plot_lrs(self, n_roll=1):\n",
    "        import seaborn as sns\n",
    "        ax=sns.lineplot(x=self.lrs,y=pd.Series(self.losses).rolling(n_roll).mean())\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.set_xlabel('Learning Rate')\n",
    "\n",
    "    \n",
    "    def run_epochs(self,dltrain,dlvalid,n_epochs=1):\n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            loss,acc=self.run_epoch(dltrain,True)\n",
    "            print (f'Epoch:{epoch} Loss:{loss}')\n",
    "            lossv,accv=self.run_epoch(dlvalid,mode_train=False)\n",
    "            print (f'Epoch:{epoch} Loss:{loss} Accuracy:{acc} Loss:{lossv} Accuracy:{accv}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(model.parameters(),lr=1e-3,betas=(0.9,0.999), weight_decay=wd)\n",
    "metric_fn=accuracy_multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_batch=np.int(np.ceil(train_tokens.shape[1]/bptt))\n",
    "n_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.weight.requires_grad, model.decoder.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner=Learner(model,optimizer,accuracy_multinomial,device,bptt,500,0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16206"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Loss:6.919100963676369\n",
      "Epoch:0 Loss:6.919100963676369 Accuracy:0.13795176182443705 Loss:6.290080229441325 Accuracy:0.17168041101346412\n",
      "Epoch:1 Loss:6.339776808517989\n",
      "Epoch:1 Loss:6.339776808517989 Accuracy:0.16216519543307317 Loss:5.8777219752470655 Accuracy:0.1892227636029323\n",
      "Epoch:2 Loss:5.991374429488024\n",
      "Epoch:2 Loss:5.991374429488024 Accuracy:0.17336776694714628 Loss:5.520935734113057 Accuracy:0.2032566467920939\n",
      "Epoch:3 Loss:5.691079924591874\n",
      "Epoch:3 Loss:5.691079924591874 Accuracy:0.1832196998648939 Loss:5.2054474751154585 Accuracy:0.2169070631886522\n",
      "Epoch:4 Loss:5.419516736715376\n",
      "Epoch:4 Loss:5.419516736715376 Accuracy:0.1915554537187589 Loss:4.9079897701740265 Accuracy:0.2312729066858689\n",
      "Epoch:5 Loss:5.168744438079767\n",
      "Epoch:5 Loss:5.168744438079767 Accuracy:0.202082014955808 Loss:4.632564624150594 Accuracy:0.246348454306523\n",
      "Epoch:6 Loss:4.941565928480173\n",
      "Epoch:6 Loss:4.941565928480173 Accuracy:0.24297591590933862 Loss:4.386468718449275 Accuracy:0.3235748776545127\n",
      "Epoch:7 Loss:4.739097241733385\n",
      "Epoch:7 Loss:4.739097241733385 Accuracy:0.27617566928915355 Loss:4.183379024267197 Accuracy:0.3315190163751443\n",
      "Epoch:8 Loss:4.567179900997288\n",
      "Epoch:8 Loss:4.567179900997288 Accuracy:0.28310079941382776 Loss:4.001706590255101 Accuracy:0.341815489033858\n",
      "Epoch:9 Loss:4.429031028497688\n",
      "Epoch:9 Loss:4.429031028497688 Accuracy:0.2879693222488378 Loss:3.868275369207064 Accuracy:0.3492502439767122\n",
      "Epoch:10 Loss:4.318697900961566\n",
      "Epoch:10 Loss:4.318697900961566 Accuracy:0.2918654948012455 Loss:3.75944418211778 Accuracy:0.35542011943956214\n",
      "Epoch:11 Loss:4.237753816524615\n",
      "Epoch:11 Loss:4.237753816524615 Accuracy:0.29502935776647354 Loss:3.675913463036219 Accuracy:0.3606685164074103\n",
      "Epoch:12 Loss:4.180086332300435\n",
      "Epoch:12 Loss:4.180086332300435 Accuracy:0.2972150329662406 Loss:3.6253013561169305 Accuracy:0.3648924194276333\n",
      "Epoch:13 Loss:4.1296379906790595\n",
      "Epoch:13 Loss:4.1296379906790595 Accuracy:0.30005018167443326 Loss:3.578097606698672 Accuracy:0.36870995598534745\n",
      "Epoch:14 Loss:4.095323054801\n",
      "Epoch:14 Loss:4.095323054801 Accuracy:0.30167020292796753 Loss:3.5321967800458274 Accuracy:0.3730082617451747\n",
      "Epoch:15 Loss:4.065600973393948\n",
      "Epoch:15 Loss:4.065600973393948 Accuracy:0.30384561714884467 Loss:3.507961372534434 Accuracy:0.3766655419021845\n",
      "Epoch:16 Loss:4.042777150274644\n",
      "Epoch:16 Loss:4.042777150274644 Accuracy:0.30562318322928145 Loss:3.4758294820785522 Accuracy:0.38082648317019147\n",
      "Epoch:17 Loss:4.023797281983123\n",
      "Epoch:17 Loss:4.023797281983123 Accuracy:0.3076378790502736 Loss:3.456903874874115 Accuracy:0.3830700758844614\n",
      "Epoch:18 Loss:4.0039286376624705\n",
      "Epoch:18 Loss:4.0039286376624705 Accuracy:0.3093875565670973 Loss:3.435523122549057 Accuracy:0.38486723229289055\n",
      "Epoch:19 Loss:3.9852642288757796\n",
      "Epoch:19 Loss:3.9852642288757796 Accuracy:0.3111041016430654 Loss:3.418689027428627 Accuracy:0.3888164112965266\n",
      "Epoch:20 Loss:3.9726592952315642\n",
      "Epoch:20 Loss:3.9726592952315642 Accuracy:0.3077835544581971 Loss:3.387520452340444 Accuracy:0.3927770306666692\n",
      "Epoch:21 Loss:3.9600263380742335\n",
      "Epoch:21 Loss:3.9600263380742335 Accuracy:0.31134820239884514 Loss:3.370899667342504 Accuracy:0.39680633631845313\n",
      "Epoch:22 Loss:3.9485919407585213\n",
      "Epoch:22 Loss:3.9485919407585213 Accuracy:0.3148255922030984 Loss:3.36222697297732 Accuracy:0.39783655541638535\n",
      "Epoch:23 Loss:3.9349950633926563\n",
      "Epoch:23 Loss:3.9349950633926563 Accuracy:0.2983665996274504 Loss:3.336628705263138 Accuracy:0.37434182316064835\n",
      "Epoch:24 Loss:3.9256417840196374\n",
      "Epoch:24 Loss:3.9256417840196374 Accuracy:0.314960256676402 Loss:3.3394367694854736 Accuracy:0.40083564072847366\n",
      "Epoch:25 Loss:3.9168224819901214\n",
      "Epoch:25 Loss:3.9168224819901214 Accuracy:0.312027524349726 Loss:3.317060202360153 Accuracy:0.4038061052560806\n",
      "Epoch:26 Loss:3.903099846152667\n",
      "Epoch:26 Loss:3.903099846152667 Accuracy:0.308935352611436 Loss:3.296292687455813 Accuracy:0.38072346275051433\n",
      "Epoch:27 Loss:3.8965450029457567\n",
      "Epoch:27 Loss:3.8965450029457567 Accuracy:0.3181658136633645 Loss:3.2932192236185074 Accuracy:0.4080986895908912\n",
      "Epoch:28 Loss:3.888357322121507\n",
      "Epoch:28 Loss:3.888357322121507 Accuracy:0.3147740621398724 Loss:3.2830465833346048 Accuracy:0.4097871060172717\n",
      "Epoch:29 Loss:3.879669456418496\n",
      "Epoch:29 Loss:3.879669456418496 Accuracy:0.32112840842249124 Loss:3.2667068938414254 Accuracy:0.4126373815039794\n",
      "Epoch:30 Loss:3.8701577501675106\n",
      "Epoch:30 Loss:3.8701577501675106 Accuracy:0.30897428567976676 Loss:3.262821008761724 Accuracy:0.41301512842377025\n",
      "Epoch:31 Loss:3.8712590703797654\n",
      "Epoch:31 Loss:3.8712590703797654 Accuracy:0.3194816711167581 Loss:3.249760160843531 Accuracy:0.38869621853033703\n",
      "Epoch:32 Loss:3.8617747886732676\n",
      "Epoch:32 Loss:3.8617747886732676 Accuracy:0.31474898456225747 Loss:3.248285229007403 Accuracy:0.38953183963894844\n",
      "Epoch:33 Loss:3.8522923592700073\n",
      "Epoch:33 Loss:3.8522923592700073 Accuracy:0.3124518797097617 Loss:3.236420621474584 Accuracy:0.4179258414854606\n",
      "Epoch:34 Loss:3.846788732298128\n",
      "Epoch:34 Loss:3.846788732298128 Accuracy:0.31551431384715406 Loss:3.2281932632128396 Accuracy:0.41790867162247497\n",
      "Epoch:35 Loss:3.842363038788671\n",
      "Epoch:35 Loss:3.842363038788671 Accuracy:0.3162506634774415 Loss:3.2176066090663276 Accuracy:0.39369850791990757\n",
      "Epoch:36 Loss:3.8357379694384983\n",
      "Epoch:36 Loss:3.8357379694384983 Accuracy:0.3171249518199735 Loss:3.2136327624320984 Accuracy:0.420352582509319\n",
      "Epoch:37 Loss:3.8316703728267125\n",
      "Epoch:37 Loss:3.8316703728267125 Accuracy:0.31454950688959477 Loss:3.1995621770620346 Accuracy:0.4232600921144088\n",
      "Epoch:38 Loss:3.8246424478332113\n",
      "Epoch:38 Loss:3.8246424478332113 Accuracy:0.3226852825369909 Loss:3.199775512019793 Accuracy:0.42371224736173946\n",
      "Epoch:39 Loss:3.820338497077841\n",
      "Epoch:39 Loss:3.820338497077841 Accuracy:0.3232630773525406 Loss:3.1913091838359833 Accuracy:0.39860922036071617\n",
      "Epoch:40 Loss:3.8153587392887007\n",
      "Epoch:40 Loss:3.8153587392887007 Accuracy:0.3117874404189339 Loss:3.183692435423533 Accuracy:0.425543749704957\n",
      "Epoch:41 Loss:3.8118538919663587\n",
      "Epoch:41 Loss:3.8118538919663587 Accuracy:0.33031002456778724 Loss:3.1734525561332703 Accuracy:0.4010474079598983\n",
      "Epoch:42 Loss:3.8069563593183244\n",
      "Epoch:42 Loss:3.8069563593183244 Accuracy:0.3235045003366994 Loss:3.1829127073287964 Accuracy:0.42619049673279125\n",
      "Epoch:43 Loss:3.802717688296615\n",
      "Epoch:43 Loss:3.802717688296615 Accuracy:0.3193619390848156 Loss:3.1602044701576233 Accuracy:0.4015281784037749\n",
      "Epoch:44 Loss:3.7996720437961526\n",
      "Epoch:44 Loss:3.7996720437961526 Accuracy:0.3248903556410962 Loss:3.169452041387558 Accuracy:0.4281364691754182\n",
      "Epoch:45 Loss:3.795669155685525\n",
      "Epoch:45 Loss:3.795669155685525 Accuracy:0.32602976197213457 Loss:3.164525330066681 Accuracy:0.43000231372813386\n",
      "Epoch:46 Loss:3.7911405626096224\n",
      "Epoch:46 Loss:3.7911405626096224 Accuracy:0.3199041069468908 Loss:3.1507002115249634 Accuracy:0.40439562189082306\n",
      "Epoch:47 Loss:3.7917692764496906\n",
      "Epoch:47 Loss:3.7917692764496906 Accuracy:0.3240472651792295 Loss:3.1524871985117593 Accuracy:0.4304430155704419\n",
      "Epoch:48 Loss:3.7854505871060633\n",
      "Epoch:48 Loss:3.7854505871060633 Accuracy:0.32132191903986784 Loss:3.138089577356974 Accuracy:0.4058035897711913\n",
      "Epoch:49 Loss:3.7794198610924727\n",
      "Epoch:49 Loss:3.7794198610924727 Accuracy:0.32028704855352574 Loss:3.138760263721148 Accuracy:0.40600390794376534\n",
      "Epoch:50 Loss:3.77523591818399\n",
      "Epoch:50 Loss:3.77523591818399 Accuracy:0.31927196974259603 Loss:3.1435303737719855 Accuracy:0.43406595662236214\n",
      "Epoch:51 Loss:3.775524434290434\n",
      "Epoch:51 Loss:3.775524434290434 Accuracy:0.33062716651903956 Loss:3.1380778153737388 Accuracy:0.4064217197398345\n",
      "Epoch:52 Loss:3.7699204604531174\n",
      "Epoch:52 Loss:3.7699204604531174 Accuracy:0.32504731941853326 Loss:3.1259131332238517 Accuracy:0.4072745119531949\n",
      "Epoch:53 Loss:3.7690913629951983\n",
      "Epoch:53 Loss:3.7690913629951983 Accuracy:0.3265815129757978 Loss:3.124198332428932 Accuracy:0.43591462758680183\n",
      "Epoch:54 Loss:3.7631869542414518\n",
      "Epoch:54 Loss:3.7631869542414518 Accuracy:0.32774080674906175 Loss:3.1244990030924478 Accuracy:0.4353480090697606\n",
      "Epoch:55 Loss:3.7652291557063227\n",
      "Epoch:55 Loss:3.7652291557063227 Accuracy:0.3176663298321807 Loss:3.119340235988299 Accuracy:0.43606915945808095\n",
      "Epoch:56 Loss:3.757569725248549\n",
      "Epoch:56 Loss:3.757569725248549 Accuracy:0.33193463762601216 Loss:3.107152650753657 Accuracy:0.43841005116701126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:57 Loss:3.7589533546696536\n",
      "Epoch:57 Loss:3.7589533546696536 Accuracy:0.31931260403083717 Loss:3.1159826467434564 Accuracy:0.43697346436480683\n",
      "Epoch:58 Loss:3.755268177965231\n",
      "Epoch:58 Loss:3.755268177965231 Accuracy:0.3212039959535264 Loss:3.1095045854647956 Accuracy:0.41088028252124786\n",
      "Epoch:59 Loss:3.753373416892269\n",
      "Epoch:59 Loss:3.753373416892269 Accuracy:0.3182781245232674 Loss:3.095067540804545 Accuracy:0.4119620124499003\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(train_tokens,valid_tokens,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),f'{DATAPATH}/inter/varybptt_model_state_dict')\n",
    "torch.save(optimizer.state_dict(),f'{DATAPATH}/inter/varybptt_learner_state_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.unfreeze_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.weight.requires_grad, model.decoder.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Loss:3.4230909852792095\n",
      "Epoch:0 Loss:3.4230909852792095 Accuracy:0.4609213714173298 Loss:2.618056063850721 Accuracy:0.5857944451272488\n",
      "Epoch:1 Loss:3.2049191406794955\n",
      "Epoch:1 Loss:3.2049191406794955 Accuracy:0.4862217019547473 Loss:2.478322913249334 Accuracy:0.5971211368838946\n",
      "Epoch:2 Loss:3.074958617952135\n",
      "Epoch:2 Loss:3.074958617952135 Accuracy:0.49774672004911635 Loss:2.359963600834211 Accuracy:0.6113667823374271\n",
      "Epoch:3 Loss:2.970897956470867\n",
      "Epoch:3 Loss:2.970897956470867 Accuracy:0.5067141367183937 Loss:2.2538239657878876 Accuracy:0.622075350334247\n",
      "Epoch:4 Loss:2.875206515424267\n",
      "Epoch:4 Loss:2.875206515424267 Accuracy:0.515497596723806 Loss:2.157458650569121 Accuracy:0.6333104732135931\n",
      "Epoch:5 Loss:2.7953958213329315\n",
      "Epoch:5 Loss:2.7953958213329315 Accuracy:0.5232134187561378 Loss:2.0832270880540213 Accuracy:0.6418498506148657\n",
      "Epoch:6 Loss:2.7196535341507566\n",
      "Epoch:6 Loss:2.7196535341507566 Accuracy:0.5302085997124689 Loss:2.009914495050907 Accuracy:0.6528388584653536\n",
      "Epoch:7 Loss:2.653038995271178\n",
      "Epoch:7 Loss:2.653038995271178 Accuracy:0.5374186196786309 Loss:1.9501896724104881 Accuracy:0.6604911039272944\n",
      "Epoch:8 Loss:2.591734789328333\n",
      "Epoch:8 Loss:2.591734789328333 Accuracy:0.5441837529209802 Loss:1.885540875295798 Accuracy:0.6688702255487442\n",
      "Epoch:9 Loss:2.537374144884661\n",
      "Epoch:9 Loss:2.537374144884661 Accuracy:0.5505571481144718 Loss:1.8360366895794868 Accuracy:0.6759501211345196\n",
      "Epoch:10 Loss:2.4862470579671334\n",
      "Epoch:10 Loss:2.4862470579671334 Accuracy:0.5561515344368233 Loss:1.7889254142840703 Accuracy:0.6821600633362929\n",
      "Epoch:11 Loss:2.442196848605453\n",
      "Epoch:11 Loss:2.442196848605453 Accuracy:0.5617321756930133 Loss:1.7519084562857945 Accuracy:0.6873054330547651\n",
      "Epoch:12 Loss:2.394597424342569\n",
      "Epoch:12 Loss:2.394597424342569 Accuracy:0.5680933388988528 Loss:1.7133846854170163 Accuracy:0.6931147302190462\n",
      "Epoch:13 Loss:2.357503770726972\n",
      "Epoch:13 Loss:2.357503770726972 Accuracy:0.5727525185958474 Loss:1.6800565098722775 Accuracy:0.6986664695044359\n",
      "Epoch:14 Loss:2.3289495248461396\n",
      "Epoch:14 Loss:2.3289495248461396 Accuracy:0.5761978237129195 Loss:1.6459189107020695 Accuracy:0.7035943580170473\n",
      "Epoch:15 Loss:2.2915665280870994\n",
      "Epoch:15 Loss:2.2915665280870994 Accuracy:0.581170055153068 Loss:1.6194795568784077 Accuracy:0.7075721497337023\n",
      "Epoch:16 Loss:2.2612529747318804\n",
      "Epoch:16 Loss:2.2612529747318804 Accuracy:0.5853201678970403 Loss:1.5965695654352505 Accuracy:0.7107944426437219\n",
      "Epoch:17 Loss:2.2336199557281224\n",
      "Epoch:17 Loss:2.2336199557281224 Accuracy:0.588837491097566 Loss:1.5697035789489746 Accuracy:0.7150813080370426\n",
      "Epoch:18 Loss:2.2101417730576594\n",
      "Epoch:18 Loss:2.2101417730576594 Accuracy:0.5919026714264697 Loss:1.5458698247869809 Accuracy:0.7193109331031641\n",
      "Epoch:19 Loss:2.1822328156982826\n",
      "Epoch:19 Loss:2.1822328156982826 Accuracy:0.596379447983327 Loss:1.526125135521094 Accuracy:0.7222470616300901\n",
      "Epoch:20 Loss:2.158471847475233\n",
      "Epoch:20 Loss:2.158471847475233 Accuracy:0.5997816747913823 Loss:1.5107559487223625 Accuracy:0.7243475628395876\n",
      "Epoch:21 Loss:2.1389646952731343\n",
      "Epoch:21 Loss:2.1389646952731343 Accuracy:0.6023156006435336 Loss:1.4969261462489765 Accuracy:0.7266426632801691\n",
      "Epoch:22 Loss:2.1162923012687043\n",
      "Epoch:22 Loss:2.1162923012687043 Accuracy:0.605847161559932 Loss:1.479283479352792 Accuracy:0.7287145492931207\n",
      "Epoch:23 Loss:2.1011546443275604\n",
      "Epoch:23 Loss:2.1011546443275604 Accuracy:0.6077762596977618 Loss:1.4621052121122677 Accuracy:0.7313702329993248\n",
      "Epoch:24 Loss:2.0808109568707462\n",
      "Epoch:24 Loss:2.0808109568707462 Accuracy:0.6103963705877594 Loss:1.450894112388293 Accuracy:0.7330872565507889\n",
      "Epoch:25 Loss:2.0644743993746495\n",
      "Epoch:25 Loss:2.0644743993746495 Accuracy:0.6132679420762357 Loss:1.4346110771099727 Accuracy:0.7365499412020048\n",
      "Epoch:26 Loss:2.046726096231499\n",
      "Epoch:26 Loss:2.046726096231499 Accuracy:0.6155160315285236 Loss:1.4212418372432392 Accuracy:0.7382841495176157\n",
      "Epoch:27 Loss:2.0317903859274726\n",
      "Epoch:27 Loss:2.0317903859274726 Accuracy:0.6175030045456938 Loss:1.4093387325604756 Accuracy:0.7402072263260683\n",
      "Epoch:28 Loss:2.018946368526257\n",
      "Epoch:28 Loss:2.018946368526257 Accuracy:0.6199838836812763 Loss:1.3978314772248268 Accuracy:0.7419872097671032\n",
      "Epoch:29 Loss:2.005061021258064\n",
      "Epoch:29 Loss:2.005061021258064 Accuracy:0.621952826528111 Loss:1.3917354394992192 Accuracy:0.7433322245875994\n",
      "Epoch:30 Loss:1.9919591821957885\n",
      "Epoch:30 Loss:1.9919591821957885 Accuracy:0.6239346122637587 Loss:1.3804936210314434 Accuracy:0.7454270037511984\n",
      "Epoch:31 Loss:1.9778879823265494\n",
      "Epoch:31 Loss:1.9778879823265494 Accuracy:0.6257936455391265 Loss:1.3686354358990986 Accuracy:0.7476591467857361\n",
      "Epoch:32 Loss:1.9657224687782608\n",
      "Epoch:32 Loss:1.9657224687782608 Accuracy:0.6279051968330316 Loss:1.3623360792795818 Accuracy:0.7482830012838045\n",
      "Epoch:33 Loss:1.9557067990563892\n",
      "Epoch:33 Loss:1.9557067990563892 Accuracy:0.6292381304880722 Loss:1.3516790842016537 Accuracy:0.750257587681214\n",
      "Epoch:34 Loss:1.943100225008451\n",
      "Epoch:34 Loss:1.943100225008451 Accuracy:0.6315029708893745 Loss:1.3428240045905113 Accuracy:0.7515682602922121\n",
      "Epoch:35 Loss:1.931820146050222\n",
      "Epoch:35 Loss:1.931820146050222 Accuracy:0.6328468272864556 Loss:1.3365959872802098 Accuracy:0.7527072255810102\n",
      "Epoch:36 Loss:1.9202480365941812\n",
      "Epoch:36 Loss:1.9202480365941812 Accuracy:0.6347255391078991 Loss:1.3283342520395915 Accuracy:0.7542639970779419\n",
      "Epoch:37 Loss:1.9090614945156947\n",
      "Epoch:37 Loss:1.9090614945156947 Accuracy:0.6364907127605632 Loss:1.3201760798692703 Accuracy:0.7545902331670126\n",
      "Epoch:38 Loss:1.9014033617171566\n",
      "Epoch:38 Loss:1.9014033617171566 Accuracy:0.6375029900696425 Loss:1.3155031030376751 Accuracy:0.7560096507271131\n",
      "Epoch:39 Loss:1.8934740928098328\n",
      "Epoch:39 Loss:1.8934740928098328 Accuracy:0.6393878920451957 Loss:1.3078980892896652 Accuracy:0.7573889965812365\n",
      "Epoch:40 Loss:1.882709949613939\n",
      "Epoch:40 Loss:1.882709949613939 Accuracy:0.6407795301297816 Loss:1.297879082461198 Accuracy:0.7590888639291128\n",
      "Epoch:41 Loss:1.8754118307273646\n",
      "Epoch:41 Loss:1.8754118307273646 Accuracy:0.6420399570307195 Loss:1.296709934870402 Accuracy:0.7591060337920984\n",
      "Epoch:42 Loss:1.8681090752458782\n",
      "Epoch:42 Loss:1.8681090752458782 Accuracy:0.643035735852918 Loss:1.28842489918073 Accuracy:0.7608001704017321\n",
      "Epoch:43 Loss:1.8574051730958376\n",
      "Epoch:43 Loss:1.8574051730958376 Accuracy:0.6445562230332833 Loss:1.2842204396923382 Accuracy:0.7613496221601963\n",
      "Epoch:44 Loss:1.8474096472042774\n",
      "Epoch:44 Loss:1.8474096472042774 Accuracy:0.6463893473410921 Loss:1.2803690433502197 Accuracy:0.7620650505026182\n",
      "Epoch:45 Loss:1.8441759605033725\n",
      "Epoch:45 Loss:1.8441759605033725 Accuracy:0.6467487900345413 Loss:1.273545265197754 Accuracy:0.763038040449222\n",
      "Epoch:46 Loss:1.8359549348191184\n",
      "Epoch:46 Loss:1.8359549348191184 Accuracy:0.6483217508061306 Loss:1.2670870398481686 Accuracy:0.7644688983758291\n",
      "Epoch:47 Loss:1.829256128275759\n",
      "Epoch:47 Loss:1.829256128275759 Accuracy:0.649027929295619 Loss:1.2630286638935406 Accuracy:0.7647665192683538\n",
      "Epoch:48 Loss:1.8217067250557113\n",
      "Epoch:48 Loss:1.8217067250557113 Accuracy:0.6506569826027804 Loss:1.2602779244383175 Accuracy:0.7654247197012106\n",
      "Epoch:49 Loss:1.8147638809364455\n",
      "Epoch:49 Loss:1.8147638809364455 Accuracy:0.6517364596371102 Loss:1.2535487065712612 Accuracy:0.7668784720202287\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(train_tokens,valid_tokens,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),f'{DATAPATH}/inter/varybptt_model_state_dict_unfreeze')\n",
    "torch.save(optimizer.state_dict(),f'{DATAPATH}/inter/varybptt_learner_state_dict_unfreeze')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kirana/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type language_model. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save (model,f'{DATAPATH}/inter/varybptt_model_awd_lstm')\n",
    "torch.save (optimizer,f'{DATAPATH}/inter/varybptt_optimizer_awd_lstm')\n",
    "torch.save (learner,f'{DATAPATH}/inter/varybptt_learner_awd_lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Loss:1.8093397842658745\n",
      "Epoch:0 Loss:1.8093397842658745 Accuracy:0.6525833670909588 Loss:1.2510008563597996 Accuracy:0.7667811674376329\n",
      "Epoch:1 Loss:1.80106171261659\n",
      "Epoch:1 Loss:1.80106171261659 Accuracy:0.6536564950648259 Loss:1.2444622615973155 Accuracy:0.7679430283606052\n",
      "Epoch:2 Loss:1.795681797116008\n",
      "Epoch:2 Loss:1.795681797116008 Accuracy:0.654837611626842 Loss:1.241359956562519 Accuracy:0.7683780118823051\n",
      "Epoch:3 Loss:1.7904357753986875\n",
      "Epoch:3 Loss:1.7904357753986875 Accuracy:0.6554506166012527 Loss:1.2385850995779037 Accuracy:0.7693624372283617\n",
      "Epoch:4 Loss:1.7820135477398122\n",
      "Epoch:4 Loss:1.7820135477398122 Accuracy:0.6568631501868367 Loss:1.234282764295737 Accuracy:0.7701808934410413\n",
      "Epoch:5 Loss:1.77428889936871\n",
      "Epoch:5 Loss:1.77428889936871 Accuracy:0.658147214518653 Loss:1.2304666936397552 Accuracy:0.7709707332154115\n",
      "Epoch:6 Loss:1.7712087833855064\n",
      "Epoch:6 Loss:1.7712087833855064 Accuracy:0.6585732303707805 Loss:1.225143591562907 Accuracy:0.7718063505987326\n",
      "Epoch:7 Loss:1.7642945496241251\n",
      "Epoch:7 Loss:1.7642945496241251 Accuracy:0.6602273227108849 Loss:1.221646214524905 Accuracy:0.7723100185394287\n",
      "Epoch:8 Loss:1.7595111653868076\n",
      "Epoch:8 Loss:1.7595111653868076 Accuracy:0.6610168676197001 Loss:1.220006749033928 Accuracy:0.772653424491485\n",
      "Epoch:9 Loss:1.757579055699435\n",
      "Epoch:9 Loss:1.757579055699435 Accuracy:0.660718762513363 Loss:1.2187880650162697 Accuracy:0.7728880854944388\n",
      "Epoch:10 Loss:1.7520398959714412\n",
      "Epoch:10 Loss:1.7520398959714412 Accuracy:0.6618591536509308 Loss:1.2124050532778103 Accuracy:0.7737752174337705\n",
      "Epoch:11 Loss:1.7483243151025458\n",
      "Epoch:11 Loss:1.7483243151025458 Accuracy:0.6628059017789233 Loss:1.2112967347105343 Accuracy:0.7735863526662191\n",
      "Epoch:12 Loss:1.7394663977622986\n",
      "Epoch:12 Loss:1.7394663977622986 Accuracy:0.6643118622567918 Loss:1.2052948648730915 Accuracy:0.7749199084937572\n",
      "Epoch:13 Loss:1.7355742546430244\n",
      "Epoch:13 Loss:1.7355742546430244 Accuracy:0.664638029715038 Loss:1.201723908384641 Accuracy:0.77530337870121\n",
      "Epoch:14 Loss:1.7335296851018325\n",
      "Epoch:14 Loss:1.7335296851018325 Accuracy:0.6649403449482156 Loss:1.2009188358982403 Accuracy:0.7759157841404279\n",
      "Epoch:15 Loss:1.725355129062602\n",
      "Epoch:15 Loss:1.725355129062602 Accuracy:0.6663251231729457 Loss:1.1964848736921947 Accuracy:0.7764480660359064\n",
      "Epoch:16 Loss:1.7210847684089712\n",
      "Epoch:16 Loss:1.7210847684089712 Accuracy:0.667122516016297 Loss:1.1945991317431133 Accuracy:0.7769231125712395\n",
      "Epoch:17 Loss:1.7176147094099201\n",
      "Epoch:17 Loss:1.7176147094099201 Accuracy:0.6677624870083453 Loss:1.1908418958385785 Accuracy:0.7777873538434505\n",
      "Epoch:18 Loss:1.7153733302952174\n",
      "Epoch:18 Loss:1.7153733302952174 Accuracy:0.6682013244292068 Loss:1.1870859911044438 Accuracy:0.7788805303474268\n",
      "Epoch:19 Loss:1.7108476096886214\n",
      "Epoch:19 Loss:1.7108476096886214 Accuracy:0.6688385314295906 Loss:1.1874094232916832 Accuracy:0.7780105670293173\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(train_tokens,valid_tokens,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Loss:1.7090238940533193\n",
      "Epoch:0 Loss:1.7090238940533193 Accuracy:0.6688894502965314 Loss:1.1833711539705594 Accuracy:0.7791037497421106\n",
      "Epoch:1 Loss:1.704445949826625\n",
      "Epoch:1 Loss:1.704445949826625 Accuracy:0.6700475326290837 Loss:1.181585930287838 Accuracy:0.7795272792379061\n",
      "Epoch:2 Loss:1.6992818966376042\n",
      "Epoch:2 Loss:1.6992818966376042 Accuracy:0.6708904078840154 Loss:1.1794891407092412 Accuracy:0.779527281721433\n",
      "Epoch:3 Loss:1.6943875393678438\n",
      "Epoch:3 Loss:1.6943875393678438 Accuracy:0.6719206421910928 Loss:1.176107866068681 Accuracy:0.780551773806413\n",
      "Epoch:4 Loss:1.6918254281980876\n",
      "Epoch:4 Loss:1.6918254281980876 Accuracy:0.6722753766087184 Loss:1.1728282868862152 Accuracy:0.780671967814366\n",
      "Epoch:5 Loss:1.6892850482856834\n",
      "Epoch:5 Loss:1.6892850482856834 Accuracy:0.6726599445709815 Loss:1.1702159841855366 Accuracy:0.7813187179466089\n",
      "Epoch:6 Loss:1.684846241564021\n",
      "Epoch:6 Loss:1.684846241564021 Accuracy:0.6731853541936684 Loss:1.1684076587359111 Accuracy:0.7818738954762617\n",
      "Epoch:7 Loss:1.678341119358122\n",
      "Epoch:7 Loss:1.678341119358122 Accuracy:0.6745062245762269 Loss:1.1664663727084796 Accuracy:0.7817823191483816\n",
      "Epoch:8 Loss:1.6758164922659498\n",
      "Epoch:8 Loss:1.6758164922659498 Accuracy:0.6748385278068104 Loss:1.1640828773379326 Accuracy:0.782182956735293\n",
      "Epoch:9 Loss:1.6774177499551834\n",
      "Epoch:9 Loss:1.6774177499551834 Accuracy:0.6743740396747879 Loss:1.1630565226078033 Accuracy:0.782514917353789\n",
      "Epoch:10 Loss:1.6711861509583594\n",
      "Epoch:10 Loss:1.6711861509583594 Accuracy:0.6753731517014524 Loss:1.1606935933232307 Accuracy:0.7832189040879408\n",
      "Epoch:11 Loss:1.6660469014916504\n",
      "Epoch:11 Loss:1.6660469014916504 Accuracy:0.6764960611859957 Loss:1.1601649026076 Accuracy:0.7827037970225016\n",
      "Epoch:12 Loss:1.6662430690206114\n",
      "Epoch:12 Loss:1.6662430690206114 Accuracy:0.6765653174085221 Loss:1.1579991281032562 Accuracy:0.7836538826425871\n",
      "Epoch:13 Loss:1.661011389698751\n",
      "Epoch:13 Loss:1.661011389698751 Accuracy:0.6774063095909908 Loss:1.1554511189460754 Accuracy:0.7836767782767614\n",
      "Epoch:14 Loss:1.65630128378408\n",
      "Epoch:14 Loss:1.65630128378408 Accuracy:0.6781291425750967 Loss:1.1531964937845867 Accuracy:0.7840602484842142\n",
      "Epoch:15 Loss:1.6539861453904046\n",
      "Epoch:15 Loss:1.6539861453904046 Accuracy:0.6787793480025397 Loss:1.1504516899585724 Accuracy:0.7844551652669907\n",
      "Epoch:16 Loss:1.6527513597618069\n",
      "Epoch:16 Loss:1.6527513597618069 Accuracy:0.6786031256380834 Loss:1.1491743351022403 Accuracy:0.7850332371890545\n",
      "Epoch:17 Loss:1.6484774241562223\n",
      "Epoch:17 Loss:1.6484774241562223 Accuracy:0.6790144263524195 Loss:1.148260338852803 Accuracy:0.7848443624873956\n",
      "Epoch:18 Loss:1.6472076164554454\n",
      "Epoch:18 Loss:1.6472076164554454 Accuracy:0.6797068193391845 Loss:1.1474248878657818 Accuracy:0.7853823651870092\n",
      "Epoch:19 Loss:1.643701009435968\n",
      "Epoch:19 Loss:1.643701009435968 Accuracy:0.6803678428733742 Loss:1.1470903816322486 Accuracy:0.78518204515179\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(train_tokens,valid_tokens,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Loss:1.6414882575764376\n",
      "Epoch:0 Loss:1.6414882575764376 Accuracy:0.6812373098464832 Loss:1.1441115277508895 Accuracy:0.7863324582576752\n",
      "Epoch:1 Loss:1.6387577438563632\n",
      "Epoch:1 Loss:1.6387577438563632 Accuracy:0.6811340147988838 Loss:1.143005261818568 Accuracy:0.7856914301713308\n",
      "Epoch:2 Loss:1.6349285091672625\n",
      "Epoch:2 Loss:1.6349285091672625 Accuracy:0.6819944330624171 Loss:1.1388371425370376 Accuracy:0.7866815862556299\n",
      "Epoch:3 Loss:1.631730903326544\n",
      "Epoch:3 Loss:1.631730903326544 Accuracy:0.6825169131455832 Loss:1.1391915467878182 Accuracy:0.7871222893397013\n",
      "Epoch:4 Loss:1.6304739414996312\n",
      "Epoch:4 Loss:1.6304739414996312 Accuracy:0.6827009507254058 Loss:1.1370935700833797 Accuracy:0.787282545119524\n",
      "Epoch:5 Loss:1.6270468792735606\n",
      "Epoch:5 Loss:1.6270468792735606 Accuracy:0.6833858114652782 Loss:1.1348146225015323 Accuracy:0.7875973408420881\n",
      "Epoch:6 Loss:1.6226344021161396\n",
      "Epoch:6 Loss:1.6226344021161396 Accuracy:0.6841492301887936 Loss:1.1341620857516925 Accuracy:0.7877575928966204\n",
      "Epoch:7 Loss:1.6223197676536796\n",
      "Epoch:7 Loss:1.6223197676536796 Accuracy:0.683852961278697 Loss:1.133168353388707 Accuracy:0.787671742339929\n",
      "Epoch:8 Loss:1.6239822619880726\n",
      "Epoch:8 Loss:1.6239822619880726 Accuracy:0.6841049720259273 Loss:1.131383125980695 Accuracy:0.7880952768027782\n",
      "Epoch:9 Loss:1.6182325688037245\n",
      "Epoch:9 Loss:1.6182325688037245 Accuracy:0.6844710557015388 Loss:1.131083406507969 Accuracy:0.7881582342088223\n",
      "Epoch:10 Loss:1.6183613359147284\n",
      "Epoch:10 Loss:1.6183613359147284 Accuracy:0.6846369036420464 Loss:1.1277027403314908 Accuracy:0.788867944230636\n",
      "Epoch:11 Loss:1.6136138213226219\n",
      "Epoch:11 Loss:1.6136138213226219 Accuracy:0.685651938120524 Loss:1.1266901356478531 Accuracy:0.7889022814730803\n",
      "Epoch:12 Loss:1.610324583583408\n",
      "Epoch:12 Loss:1.610324583583408 Accuracy:0.6861804729037815 Loss:1.1252439183493455 Accuracy:0.7895604769388834\n",
      "Epoch:13 Loss:1.6065768602147568\n",
      "Epoch:13 Loss:1.6065768602147568 Accuracy:0.6866404689782488 Loss:1.1240470930933952 Accuracy:0.7895204164087772\n",
      "Epoch:14 Loss:1.6067849462872261\n",
      "Epoch:14 Loss:1.6067849462872261 Accuracy:0.6870805446376425 Loss:1.1223251310487588 Accuracy:0.7896177160243193\n",
      "Epoch:15 Loss:1.6039218357407552\n",
      "Epoch:15 Loss:1.6039218357407552 Accuracy:0.6873499912819142 Loss:1.12153984233737 Accuracy:0.7897722435494264\n",
      "Epoch:16 Loss:1.60343560785578\n",
      "Epoch:16 Loss:1.60343560785578 Accuracy:0.6875433015980219 Loss:1.117400115976731 Accuracy:0.7913118551174799\n",
      "Epoch:17 Loss:1.5989675637895027\n",
      "Epoch:17 Loss:1.5989675637895027 Accuracy:0.6880926876205259 Loss:1.1172213765482109 Accuracy:0.7908482501904169\n",
      "Epoch:18 Loss:1.595484876367781\n",
      "Epoch:18 Loss:1.595484876367781 Accuracy:0.6888657457298702 Loss:1.11575881143411 Accuracy:0.7911458698411783\n",
      "Epoch:19 Loss:1.594804678531672\n",
      "Epoch:19 Loss:1.594804678531672 Accuracy:0.6889372942989762 Loss:1.1147610309223335 Accuracy:0.7914091510077318\n",
      "Epoch:20 Loss:1.5938348371049633\n",
      "Epoch:20 Loss:1.5938348371049633 Accuracy:0.689082710120989 Loss:1.1142892092466354 Accuracy:0.7912488927443823\n",
      "Epoch:21 Loss:1.591620925246485\n",
      "Epoch:21 Loss:1.591620925246485 Accuracy:0.6900556807223271 Loss:1.1124608293175697 Accuracy:0.7911229816575845\n",
      "Epoch:22 Loss:1.5876955269709923\n",
      "Epoch:22 Loss:1.5876955269709923 Accuracy:0.6903215848950219 Loss:1.111416647831599 Accuracy:0.7918498578170935\n",
      "Epoch:23 Loss:1.587011927313972\n",
      "Epoch:23 Loss:1.587011927313972 Accuracy:0.6903086811826941 Loss:1.110717050731182 Accuracy:0.791964323570331\n",
      "Epoch:24 Loss:1.585432055220823\n",
      "Epoch:24 Loss:1.585432055220823 Accuracy:0.690551085206038 Loss:1.1099461155633132 Accuracy:0.7923191798230013\n",
      "Epoch:25 Loss:1.58577699395186\n",
      "Epoch:25 Loss:1.58577699395186 Accuracy:0.6907656175824172 Loss:1.1098405582209427 Accuracy:0.7922848376135031\n",
      "Epoch:26 Loss:1.583384716117775\n",
      "Epoch:26 Loss:1.583384716117775 Accuracy:0.690931258882795 Loss:1.107304831345876 Accuracy:0.7927999496459961\n",
      "Epoch:27 Loss:1.5777962552527456\n",
      "Epoch:27 Loss:1.5777962552527456 Accuracy:0.6921253093820534 Loss:1.1057298742234707 Accuracy:0.7925595603883266\n",
      "Epoch:28 Loss:1.5809558444208913\n",
      "Epoch:28 Loss:1.5809558444208913 Accuracy:0.6915649087914141 Loss:1.1061674344042938 Accuracy:0.7930002709229788\n",
      "Epoch:29 Loss:1.5778165821935617\n",
      "Epoch:29 Loss:1.5778165821935617 Accuracy:0.6919442325635673 Loss:1.1038812100887299 Accuracy:0.7929430318375429\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(train_tokens,valid_tokens,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save (model,f'{DATAPATH}/inter/varybptt_model_awd_lstm_final')\n",
    "torch.save (optimizer,f'{DATAPATH}/inter/varybptt_optimizer_awd_lstm_final')\n",
    "torch.save (learner,f'{DATAPATH}/inter/varybptt_learner_awd_lstm_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_lm_weights=model.encoder.weight.data.cpu().numpy()\n",
    "import pickle\n",
    "pickle.dump(pretrained_lm_weights,open(f'{DATAPATH}/inter/varybpttpretrained_lm_weights','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0041660239464334"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(1.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
