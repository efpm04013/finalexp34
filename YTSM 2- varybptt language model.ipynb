{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import os \n",
    "import fastai\n",
    "import fastai\n",
    "from fastai.text import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=\"/home/kirana/Documents/phd\"\n",
    "DATAPATH=\"/home/kirana/Documents/phd/data/experiment/YTS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "[df_train,df_valid,itos, train_tokens, valid_tokens, trn_lm, val_lm]=pickle.load(open(f'{DATAPATH}/inter/dfs_tokens_fastai.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=52 # 52 - Jeremey, 20 - default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt= 70 #70 - Jeremey, 35 - default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_lm(tokens,bs):\n",
    "    import itertools\n",
    "    # Collapse into a single large array\n",
    "    tokens=np.asarray(list (itertools.chain(*tokens)))\n",
    "    # How many batches\n",
    "    n_batch=len(tokens)//bs\n",
    "    # Truncate to exclude the ones at the end\n",
    "    tokens=tokens[:bs*n_batch]\n",
    "    # Reshape\n",
    "    tokens=tokens.reshape(bs,-1)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52, 927), (52, 101))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens=create_data_lm(df_train['tokens'],bs)\n",
    "valid_tokens=create_data_lm(df_valid['tokens'],bs)\n",
    "train_tokens.shape, valid_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i):\n",
    "    seq_len = min(args.bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52, 70) (52, 70)\n"
     ]
    }
   ],
   "source": [
    "n_batch=train_tokens.shape[1]\n",
    "for i in range(0,n_batch,bptt):\n",
    "    seq_len=min(bptt,n_batch-1-i)\n",
    "    x=train_tokens[:,i:i+seq_len]\n",
    "    y=train_tokens[:,i+1:i+1+seq_len]\n",
    "    print (x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52, 70), (52, 70))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  5,   6,   7,   4, ...,  21,  66,  86, 750],\n",
       "        [186,   3, 272,  22, ...,  26,   0,   5,   6],\n",
       "        [  0,  36,   2,   0, ...,  25,  55, 923,   0],\n",
       "        [  0,   0,   0,   0, ...,   0,   5,   6,   7],\n",
       "        ...,\n",
       "        [ 59,   8,  10,  79, ..., 162,  20,  19,  12],\n",
       "        [ 11,  15, 153,  33, ...,   5,   6,   7,   4],\n",
       "        [ 11,  22,  29, 155, ..., 568,  27, 283,  18],\n",
       "        [ 27, 100, 515,  46, ..., 130,  94,   2,  40]]),\n",
       " array([[  6,   7,   4,   2, ...,  66,  86, 750,   9],\n",
       "        [  3, 272,  22,   3, ...,   0,   5,   6,   7],\n",
       "        [ 36,   2,   0,   0, ...,  55, 923,   0,   5],\n",
       "        [  0,   0,   0,   0, ...,   5,   6,   7,   4],\n",
       "        ...,\n",
       "        [  8,  10,  79,  52, ...,  20,  19,  12,   3],\n",
       "        [ 15, 153,  33, 145, ...,   6,   7,   4,   2],\n",
       "        [ 22,  29, 155,  65, ...,  27, 283,  18, 250],\n",
       "        [100, 515,  46,  27, ...,  94,   2,  40, 263]]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52, 70), (52, 70))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 927)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-trained AWD-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_AWD_LSTM='/home/kirana/Documents/phd/data/pre-trained/awd_lstm/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bwd_wt103_enc.h5  fwd_wt103_enc.h5  itos_wt103.pkl  \u001b[0m\u001b[01;34mwt103_60002\u001b[0m/\r\n",
      "bwd_wt103.h5      fwd_wt103.h5      \u001b[01;34mwt103_238642\u001b[0m/   \u001b[01;31mwt103_tiny.tgz\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "ls {PATH_AWD_LSTM}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts = torch.load(f'{PATH_AWD_LSTM}/fwd_wt103.h5', map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.encoder.weight',\n",
       "              tensor([[-1.2274e-01,  2.7886e-01, -3.8850e-01,  ..., -1.0404e-01,\n",
       "                        1.9580e-02,  1.8548e-01],\n",
       "                      [ 1.4854e-05, -2.3424e-05,  1.9693e-05,  ...,  2.1349e-05,\n",
       "                        2.1776e-05, -1.2394e-05],\n",
       "                      [ 1.8070e-01,  1.5874e+00, -1.1738e-01,  ..., -4.5935e-02,\n",
       "                       -8.1352e-02,  1.8054e-01],\n",
       "                      ...,\n",
       "                      [-1.8595e-03, -6.8529e-03,  1.6999e-03,  ...,  1.7039e-03,\n",
       "                        4.1632e-03, -1.3171e-03],\n",
       "                      [-2.3120e-03, -6.9001e-03,  1.8772e-03,  ...,  5.0309e-04,\n",
       "                        4.6596e-03, -2.5850e-03],\n",
       "                      [-2.2463e-03, -9.1512e-03,  1.3927e-03,  ...,  1.2296e-03,\n",
       "                        5.8085e-03, -1.8940e-03]])),\n",
       "             ('0.encoder_with_dropout.embed.weight',\n",
       "              tensor([[-1.2274e-01,  2.7886e-01, -3.8850e-01,  ..., -1.0404e-01,\n",
       "                        1.9580e-02,  1.8548e-01],\n",
       "                      [ 1.4854e-05, -2.3424e-05,  1.9693e-05,  ...,  2.1349e-05,\n",
       "                        2.1776e-05, -1.2394e-05],\n",
       "                      [ 1.8070e-01,  1.5874e+00, -1.1738e-01,  ..., -4.5935e-02,\n",
       "                       -8.1352e-02,  1.8054e-01],\n",
       "                      ...,\n",
       "                      [-1.8595e-03, -6.8529e-03,  1.6999e-03,  ...,  1.7039e-03,\n",
       "                        4.1632e-03, -1.3171e-03],\n",
       "                      [-2.3120e-03, -6.9001e-03,  1.8772e-03,  ...,  5.0309e-04,\n",
       "                        4.6596e-03, -2.5850e-03],\n",
       "                      [-2.2463e-03, -9.1512e-03,  1.3927e-03,  ...,  1.2296e-03,\n",
       "                        5.8085e-03, -1.8940e-03]])),\n",
       "             ('0.rnns.0.module.weight_ih_l0',\n",
       "              tensor([[-0.0812, -0.0811, -0.0937,  ..., -0.0259, -0.1403, -0.3247],\n",
       "                      [ 0.1154,  0.1142,  0.0938,  ..., -0.0711,  0.1669, -0.0387],\n",
       "                      [-0.0051,  0.1007,  0.2071,  ..., -0.0860, -0.0288, -0.0894],\n",
       "                      ...,\n",
       "                      [ 0.0055,  0.0157,  0.2990,  ...,  0.0616,  0.1159, -0.4737],\n",
       "                      [ 0.0181,  0.0426,  0.1130,  ...,  0.3529, -0.0114, -0.0125],\n",
       "                      [-0.0167, -0.1328,  0.1741,  ...,  0.0548, -0.0045,  0.1688]])),\n",
       "             ('0.rnns.0.module.bias_ih_l0',\n",
       "              tensor([ 0.1503, -0.4701, -0.1885,  ..., -0.5919, -0.2172, -0.1207])),\n",
       "             ('0.rnns.0.module.bias_hh_l0',\n",
       "              tensor([ 0.1503, -0.4701, -0.1885,  ..., -0.5919, -0.2172, -0.1207])),\n",
       "             ('0.rnns.0.module.weight_hh_l0_raw',\n",
       "              tensor([[-0.1013,  0.1786, -0.0528,  ...,  0.0741,  0.0306,  0.2467],\n",
       "                      [ 0.1780, -0.0853, -0.0243,  ..., -0.1129, -0.1310, -0.1498],\n",
       "                      [ 0.0661, -0.0496,  0.0921,  ...,  0.1829,  0.0533, -0.1525],\n",
       "                      ...,\n",
       "                      [-0.0322, -0.0704,  0.1653,  ...,  0.2142, -0.0558,  0.0315],\n",
       "                      [-0.1651, -0.0290,  0.1748,  ..., -0.0446,  0.5444,  0.0616],\n",
       "                      [ 0.0905, -0.1704, -0.0053,  ..., -0.0057,  0.2269,  0.0328]])),\n",
       "             ('0.rnns.1.module.weight_ih_l0',\n",
       "              tensor([[ 0.3307,  0.0385,  0.0860,  ...,  0.0685, -0.0444,  0.0539],\n",
       "                      [ 0.0720,  0.1607,  0.0562,  ...,  0.0276,  0.0613,  0.1632],\n",
       "                      [-0.1565, -0.1168,  0.1897,  ..., -0.0357,  0.0296,  0.0961],\n",
       "                      ...,\n",
       "                      [-0.0897, -0.1464, -0.0760,  ...,  0.0536,  0.0422, -0.0580],\n",
       "                      [ 0.1166, -0.1534, -0.1784,  ..., -0.0689,  0.2170,  0.1461],\n",
       "                      [-0.0413,  0.0689,  0.0581,  ..., -0.0640, -0.1703, -0.0945]])),\n",
       "             ('0.rnns.1.module.bias_ih_l0',\n",
       "              tensor([-0.8577, -0.6784, -0.7249,  ..., -0.6782,  0.0567, -0.5026])),\n",
       "             ('0.rnns.1.module.bias_hh_l0',\n",
       "              tensor([-0.8577, -0.6784, -0.7249,  ..., -0.6782,  0.0567, -0.5026])),\n",
       "             ('0.rnns.1.module.weight_hh_l0_raw',\n",
       "              tensor([[-0.0273, -0.2277,  0.0782,  ...,  0.1355, -0.1282,  0.1669],\n",
       "                      [ 0.1218,  0.0017, -0.0998,  ..., -0.2085, -0.0686, -0.1389],\n",
       "                      [-0.3878, -0.0498, -0.1748,  ..., -0.4014,  0.1986, -0.4400],\n",
       "                      ...,\n",
       "                      [-0.2097, -0.4298,  0.3551,  ...,  0.0316, -0.1198,  0.1266],\n",
       "                      [ 0.0037, -0.0223,  0.0032,  ..., -0.2672, -0.3093, -0.0361],\n",
       "                      [-0.0464,  0.1664, -0.1348,  ...,  0.1600, -0.1138,  0.0845]])),\n",
       "             ('0.rnns.2.module.weight_ih_l0',\n",
       "              tensor([[-0.0741,  0.0447, -0.0744,  ..., -0.0419,  0.1600, -0.0553],\n",
       "                      [ 0.0270,  0.0118,  0.0449,  ...,  0.1165, -0.1080, -0.0681],\n",
       "                      [-0.1023, -0.1662, -0.0229,  ...,  0.1652, -0.1070,  0.0970],\n",
       "                      ...,\n",
       "                      [-0.0989, -0.4425, -0.0343,  ..., -0.1434,  0.5851, -0.0291],\n",
       "                      [ 0.0802, -0.1067,  0.2789,  ..., -0.0916, -0.2240,  0.1020],\n",
       "                      [-0.4078,  0.7220,  0.1142,  ...,  0.5287,  0.2035, -0.1811]])),\n",
       "             ('0.rnns.2.module.bias_ih_l0',\n",
       "              tensor([-0.3681, -0.9079, -0.1998,  ...,  0.8533,  0.3202,  1.2172])),\n",
       "             ('0.rnns.2.module.bias_hh_l0',\n",
       "              tensor([-0.3681, -0.9079, -0.1998,  ...,  0.8533,  0.3202,  1.2172])),\n",
       "             ('0.rnns.2.module.weight_hh_l0_raw',\n",
       "              tensor([[-0.0966,  0.0236, -0.0152,  ...,  0.0388, -0.0531, -0.0395],\n",
       "                      [-0.0328, -0.2217,  0.0028,  ...,  0.0143, -0.0368, -0.0085],\n",
       "                      [ 0.0167, -0.0081, -0.0561,  ...,  0.0125,  0.0442, -0.0139],\n",
       "                      ...,\n",
       "                      [-0.0212, -0.1034, -0.0106,  ..., -0.0561,  0.0200, -0.0157],\n",
       "                      [ 0.0183,  0.0364, -0.0251,  ..., -0.0240, -0.1150,  0.0046],\n",
       "                      [ 0.0100, -0.1824,  0.1076,  ..., -0.0269,  0.2733,  0.1846]])),\n",
       "             ('1.decoder.weight',\n",
       "              tensor([[-1.2274e-01,  2.7886e-01, -3.8850e-01,  ..., -1.0404e-01,\n",
       "                        1.9580e-02,  1.8548e-01],\n",
       "                      [ 1.4854e-05, -2.3424e-05,  1.9693e-05,  ...,  2.1349e-05,\n",
       "                        2.1776e-05, -1.2394e-05],\n",
       "                      [ 1.8070e-01,  1.5874e+00, -1.1738e-01,  ..., -4.5935e-02,\n",
       "                       -8.1352e-02,  1.8054e-01],\n",
       "                      ...,\n",
       "                      [-1.8595e-03, -6.8529e-03,  1.6999e-03,  ...,  1.7039e-03,\n",
       "                        4.1632e-03, -1.3171e-03],\n",
       "                      [-2.3120e-03, -6.9001e-03,  1.8772e-03,  ...,  5.0309e-04,\n",
       "                        4.6596e-03, -2.5850e-03],\n",
       "                      [-2.2463e-03, -9.1512e-03,  1.3927e-03,  ...,  1.2296e-03,\n",
       "                        5.8085e-03, -1.8940e-03]]))])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts = torch.load(f'{PATH_AWD_LSTM}/fwd_wt103_enc.h5', map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('encoder.weight',\n",
       "              tensor([[-1.2274e-01,  2.7886e-01, -3.8850e-01,  ..., -1.0404e-01,\n",
       "                        1.9580e-02,  1.8548e-01],\n",
       "                      [ 1.4854e-05, -2.3424e-05,  1.9693e-05,  ...,  2.1349e-05,\n",
       "                        2.1776e-05, -1.2394e-05],\n",
       "                      [ 1.8070e-01,  1.5874e+00, -1.1738e-01,  ..., -4.5935e-02,\n",
       "                       -8.1352e-02,  1.8054e-01],\n",
       "                      ...,\n",
       "                      [-1.8595e-03, -6.8529e-03,  1.6999e-03,  ...,  1.7039e-03,\n",
       "                        4.1632e-03, -1.3171e-03],\n",
       "                      [-2.3120e-03, -6.9001e-03,  1.8772e-03,  ...,  5.0309e-04,\n",
       "                        4.6596e-03, -2.5850e-03],\n",
       "                      [-2.2463e-03, -9.1512e-03,  1.3927e-03,  ...,  1.2296e-03,\n",
       "                        5.8085e-03, -1.8940e-03]])),\n",
       "             ('encoder_with_dropout.embed.weight',\n",
       "              tensor([[-1.2274e-01,  2.7886e-01, -3.8850e-01,  ..., -1.0404e-01,\n",
       "                        1.9580e-02,  1.8548e-01],\n",
       "                      [ 1.4854e-05, -2.3424e-05,  1.9693e-05,  ...,  2.1349e-05,\n",
       "                        2.1776e-05, -1.2394e-05],\n",
       "                      [ 1.8070e-01,  1.5874e+00, -1.1738e-01,  ..., -4.5935e-02,\n",
       "                       -8.1352e-02,  1.8054e-01],\n",
       "                      ...,\n",
       "                      [-1.8595e-03, -6.8529e-03,  1.6999e-03,  ...,  1.7039e-03,\n",
       "                        4.1632e-03, -1.3171e-03],\n",
       "                      [-2.3120e-03, -6.9001e-03,  1.8772e-03,  ...,  5.0309e-04,\n",
       "                        4.6596e-03, -2.5850e-03],\n",
       "                      [-2.2463e-03, -9.1512e-03,  1.3927e-03,  ...,  1.2296e-03,\n",
       "                        5.8085e-03, -1.8940e-03]])),\n",
       "             ('rnns.0.module.weight_ih_l0',\n",
       "              tensor([[-0.0812, -0.0811, -0.0937,  ..., -0.0259, -0.1403, -0.3247],\n",
       "                      [ 0.1154,  0.1142,  0.0938,  ..., -0.0711,  0.1669, -0.0387],\n",
       "                      [-0.0051,  0.1007,  0.2071,  ..., -0.0860, -0.0288, -0.0894],\n",
       "                      ...,\n",
       "                      [ 0.0055,  0.0157,  0.2990,  ...,  0.0616,  0.1159, -0.4737],\n",
       "                      [ 0.0181,  0.0426,  0.1130,  ...,  0.3529, -0.0114, -0.0125],\n",
       "                      [-0.0167, -0.1328,  0.1741,  ...,  0.0548, -0.0045,  0.1688]])),\n",
       "             ('rnns.0.module.bias_ih_l0',\n",
       "              tensor([ 0.1503, -0.4701, -0.1885,  ..., -0.5919, -0.2172, -0.1207])),\n",
       "             ('rnns.0.module.bias_hh_l0',\n",
       "              tensor([ 0.1503, -0.4701, -0.1885,  ..., -0.5919, -0.2172, -0.1207])),\n",
       "             ('rnns.0.module.weight_hh_l0_raw',\n",
       "              tensor([[-0.1013,  0.1786, -0.0528,  ...,  0.0741,  0.0306,  0.2467],\n",
       "                      [ 0.1780, -0.0853, -0.0243,  ..., -0.1129, -0.1310, -0.1498],\n",
       "                      [ 0.0661, -0.0496,  0.0921,  ...,  0.1829,  0.0533, -0.1525],\n",
       "                      ...,\n",
       "                      [-0.0322, -0.0704,  0.1653,  ...,  0.2142, -0.0558,  0.0315],\n",
       "                      [-0.1651, -0.0290,  0.1748,  ..., -0.0446,  0.5444,  0.0616],\n",
       "                      [ 0.0905, -0.1704, -0.0053,  ..., -0.0057,  0.2269,  0.0328]])),\n",
       "             ('rnns.1.module.weight_ih_l0',\n",
       "              tensor([[ 0.3307,  0.0385,  0.0860,  ...,  0.0685, -0.0444,  0.0539],\n",
       "                      [ 0.0720,  0.1607,  0.0562,  ...,  0.0276,  0.0613,  0.1632],\n",
       "                      [-0.1565, -0.1168,  0.1897,  ..., -0.0357,  0.0296,  0.0961],\n",
       "                      ...,\n",
       "                      [-0.0897, -0.1464, -0.0760,  ...,  0.0536,  0.0422, -0.0580],\n",
       "                      [ 0.1166, -0.1534, -0.1784,  ..., -0.0689,  0.2170,  0.1461],\n",
       "                      [-0.0413,  0.0689,  0.0581,  ..., -0.0640, -0.1703, -0.0945]])),\n",
       "             ('rnns.1.module.bias_ih_l0',\n",
       "              tensor([-0.8577, -0.6784, -0.7249,  ..., -0.6782,  0.0567, -0.5026])),\n",
       "             ('rnns.1.module.bias_hh_l0',\n",
       "              tensor([-0.8577, -0.6784, -0.7249,  ..., -0.6782,  0.0567, -0.5026])),\n",
       "             ('rnns.1.module.weight_hh_l0_raw',\n",
       "              tensor([[-0.0273, -0.2277,  0.0782,  ...,  0.1355, -0.1282,  0.1669],\n",
       "                      [ 0.1218,  0.0017, -0.0998,  ..., -0.2085, -0.0686, -0.1389],\n",
       "                      [-0.3878, -0.0498, -0.1748,  ..., -0.4014,  0.1986, -0.4400],\n",
       "                      ...,\n",
       "                      [-0.2097, -0.4298,  0.3551,  ...,  0.0316, -0.1198,  0.1266],\n",
       "                      [ 0.0037, -0.0223,  0.0032,  ..., -0.2672, -0.3093, -0.0361],\n",
       "                      [-0.0464,  0.1664, -0.1348,  ...,  0.1600, -0.1138,  0.0845]])),\n",
       "             ('rnns.2.module.weight_ih_l0',\n",
       "              tensor([[-0.0741,  0.0447, -0.0744,  ..., -0.0419,  0.1600, -0.0553],\n",
       "                      [ 0.0270,  0.0118,  0.0449,  ...,  0.1165, -0.1080, -0.0681],\n",
       "                      [-0.1023, -0.1662, -0.0229,  ...,  0.1652, -0.1070,  0.0970],\n",
       "                      ...,\n",
       "                      [-0.0989, -0.4425, -0.0343,  ..., -0.1434,  0.5851, -0.0291],\n",
       "                      [ 0.0802, -0.1067,  0.2789,  ..., -0.0916, -0.2240,  0.1020],\n",
       "                      [-0.4078,  0.7220,  0.1142,  ...,  0.5287,  0.2035, -0.1811]])),\n",
       "             ('rnns.2.module.bias_ih_l0',\n",
       "              tensor([-0.3681, -0.9079, -0.1998,  ...,  0.8533,  0.3202,  1.2172])),\n",
       "             ('rnns.2.module.bias_hh_l0',\n",
       "              tensor([-0.3681, -0.9079, -0.1998,  ...,  0.8533,  0.3202,  1.2172])),\n",
       "             ('rnns.2.module.weight_hh_l0_raw',\n",
       "              tensor([[-0.0966,  0.0236, -0.0152,  ...,  0.0388, -0.0531, -0.0395],\n",
       "                      [-0.0328, -0.2217,  0.0028,  ...,  0.0143, -0.0368, -0.0085],\n",
       "                      [ 0.0167, -0.0081, -0.0561,  ...,  0.0125,  0.0442, -0.0139],\n",
       "                      ...,\n",
       "                      [-0.0212, -0.1034, -0.0106,  ..., -0.0561,  0.0200, -0.0157],\n",
       "                      [ 0.0183,  0.0364, -0.0251,  ..., -0.0240, -0.1150,  0.0046],\n",
       "                      [ 0.0100, -0.1824,  0.1076,  ..., -0.0269,  0.2733,  0.1846]]))])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos2=pickle.load(open(f'{PATH_AWD_LSTM}/itos_wt103.pkl','rb'))\n",
    "stoi2 = collections.defaultdict(lambda: -1, { v: k for k, v in enumerate(itos2) })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([238462, 400])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgts['encoder.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_wgts = wgts['encoder.weight'].numpy() # converts np.ndarray from torch.FloatTensor.output shape: (238462, 400)\n",
    "row_m = enc_wgts.mean(0) # returns the average of the array elements along axis 0. output shape: (400,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((238462, 400), 1157)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_wgts.shape, len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_emb=enc_wgts.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_w = np.zeros((len(itos),n_emb), dtype=np.float32) # shape: (60002, 400)\n",
    "\n",
    "for i, w in enumerate(itos):\n",
    "    r = stoi2[w]\n",
    "    new_w[i] = enc_wgts[r] if r >= 0 else row_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(itos).difference(set(itos2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from adaptive import *\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inp=len(itos)\n",
    "n_emb=400 #650\n",
    "n_hidden=400 #650\n",
    "n_layers=2\n",
    "dropout=0.5\n",
    "wd=1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class language_model (nn.Module):\n",
    "    def __init__(self,n_inp,n_emb,n_hidden,n_layers,bidirectional,bs,device,dropout_e=0.05,dropout=0.5,\\\n",
    "                 dropout_o=0.5,pretrain_mtx=None,adaptive_log_softmax=True,tie_weights=True):\n",
    "        super().__init__()\n",
    "        self.n_inp,self.n_emb,self.n_hidden,self.n_layers,self.bidirectional,self.bs,self.device,self.pretrain_mtx=\\\n",
    "                            n_inp,n_emb,n_hidden,n_layers,bidirectional,bs,device,pretrain_mtx\n",
    "        self.adaptive_log_softmax,self.tie_weights=adaptive_log_softmax,tie_weights\n",
    "        self.dropout_e,self.dropout,self.dropout_o=dropout_e,dropout,dropout_o\n",
    "        self.gen_hidden()\n",
    "        self.create_architecture()\n",
    "        if pretrain_mtx is not None:\n",
    "            print (\"initializing\")\n",
    "            self.initialize_glove()\n",
    "            \n",
    "        if self.adaptive_log_softmax is False:\n",
    "            self.criterion=nn.CrossEntropyLoss()\n",
    "        \n",
    "    def create_architecture(self):\n",
    "        # Dropout layer\n",
    "        self.dropout_enc=nn.Dropout(self.dropout_e)\n",
    "        # Embedding Layer\n",
    "        self.encoder=nn.Embedding(self.n_inp,self.n_emb)\n",
    "        # LSTM Layer\n",
    "        self.lstm=nn.LSTM(self.n_emb,self.n_hidden,self.n_layers,batch_first=True,dropout=self.dropout,\\\n",
    "                          bidirectional=False)\n",
    "        self.dropout_op=nn.Dropout(self.dropout_o)\n",
    "        \n",
    "        if self.adaptive_log_softmax:\n",
    "            # Adaptive Log Softmax Loss\n",
    "            self.adaptive_softmax=AdaptiveLogSoftmaxWithLoss(self.n_hidden,\n",
    "                                    self.n_inp,\n",
    "                                    cutoffs=[round(self.n_inp/15),3*round(self.n_inp/15)],\n",
    "                                    div_value=4,\n",
    "                                    get_full_prob=True)\n",
    "        else:\n",
    "            self.decoder=nn.Linear(self.n_hidden,self.n_inp)\n",
    "    \n",
    "    def freeze_embedding(self):\n",
    "        self.encoder.weight.requires_grad=False\n",
    "        if self.tie_weights:\n",
    "            self.decoder.weight.requires_grad=False\n",
    "        \n",
    "    \n",
    "    def unfreeze_embedding(self):\n",
    "        self.encoder.weight.requires_grad=True\n",
    "        if self.tie_weights:\n",
    "            self.decoder.weight.requires_grad=True\n",
    "        \n",
    "    def initialize_glove(self):\n",
    "        self.encoder.weight.data=torch.Tensor(self.pretrain_mtx)\n",
    "        if self.tie_weights:\n",
    "            self.decoder.weight=self.encoder.weight\n",
    "    \n",
    "    def gen_hidden(self):\n",
    "        # Initialize hidden\n",
    "        self.hidden=(Variable(torch.zeros(self.n_layers,self.bs,self.n_hidden,requires_grad=False).to(self.device)),\n",
    "                     Variable(torch.zeros(self.n_layers,self.bs,self.n_hidden,requires_grad=False).to(self.device)))\n",
    "    \n",
    "        \n",
    "    def forward(self,Xb,Yb):\n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        embs=self.dropout_enc(self.encoder(Xb))\n",
    "        if Xb.size(0) < self.bs:\n",
    "            self.hidden=(self.hidden[0][:,:Xb.size(0),:].contiguous(),\n",
    "            self.hidden[1][:,:Xb.size(0),:].contiguous())\n",
    "        out,new_hidden=self.lstm(embs,self.hidden)\n",
    "        out=self.dropout_op(out)\n",
    "         # Wrap the hidden state in a new tensor without the gradients\n",
    "        self.hidden=(Variable(new_hidden[0].data,requires_grad=False).to(self.device),\\\n",
    "                     Variable(new_hidden[1].data,requires_grad=False).to(self.device))\n",
    "        if self.adaptive_log_softmax:\n",
    "            out=out.reshape(out.size(0)*out.size(1),out.size(2))        # output is of shape n_batch * n_seq * n_hidden\n",
    "      \n",
    "            out=self.adaptive_softmax(out,Yb.view(-1))\n",
    "            loss=out.loss\n",
    "            preds=out.output_full\n",
    "        else:\n",
    "            #import pdb\n",
    "            #pdb.set_trace()\n",
    "            preds=self.decoder(out.contiguous().view(out.size(0)*out.size(1), out.size(2)))\n",
    "            loss=self.criterion(preds,Yb.contiguous().view(-1))\n",
    "\n",
    "        return preds, loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_multinomial(preds,actual):\n",
    "    preds=preds.max(1)[1]\n",
    "    correct=preds==actual\n",
    "    return correct.float().sum()/len(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing\n"
     ]
    }
   ],
   "source": [
    "device=\"cuda:1\"\n",
    "model=language_model(n_inp,n_emb,n_hidden,n_layers,False,bs,device,0.05,0.5,0.5,new_w,False,True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1157, 400)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1157, 400]), torch.Size([1157, 400]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.weight.data.shape,model.decoder.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1157, 400]), 400, 1157)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Linear(n_hidden,n_inp).weight.data.shape, n_hidden, n_inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test if model forward works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1157, 400]), torch.Size([1157, 400]))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder.weight.shape,model.encoder.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 70)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1==0:\n",
    "    model.forward(torch.LongTensor(x),torch.LongTensor(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self,model,optimizer,metric_fn,device,bptt=12,print_every=5,clip_val=None):\n",
    "        self.model,self.optimizer,self.metric_fn,self.device,self.print_every,self.bptt,self.losses,self.clip_val=\\\n",
    "            model,optimizer,metric_fn,device,print_every,bptt,[],clip_val\n",
    "        self.n_epochs=1\n",
    "\n",
    "    \n",
    "    def fit (self,Xb,Yb,mode_train=True):\n",
    "        if mode_train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            \n",
    "        preds,loss=self.model(Xb,Yb)\n",
    "        \n",
    "       \n",
    "            \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            acc=self.metric_fn(preds,Yb.view(-1))\n",
    "            acc=acc.item()\n",
    "            del preds\n",
    "        \n",
    "        if mode_train:\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        myloss=loss.item()\n",
    "        del loss\n",
    "        \n",
    "        if self.clip_val is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.25)\n",
    "            if 1==0:\n",
    "                for p in self.model.parameters():\n",
    "                    p.data.add_(self.lr, p.grad.data)\n",
    "        \n",
    "        return myloss, acc\n",
    "    \n",
    "    def lr_find (self,start_lr,end_lr,iterator,n_batch):\n",
    "        losses,lrs=[],[]\n",
    "        ratio=end_lr/start_lr\n",
    "        num_steps=n_batch\n",
    "        lr=start_lr\n",
    "        for i in range(num_steps):            \n",
    "            lr=lr*(end_lr/start_lr)**(1/num_steps)\n",
    "            lrs.append(lr)\n",
    "        self.lrs=lrs\n",
    "        self.run_epoch(iterator,mode_train=True,lrs=lrs)\n",
    "    \n",
    "    def run_epoch(self,iterator,mode_train,lrs=None):\n",
    "        n_batch=iterator.shape[1]\n",
    "        epoch_loss,epoch_acc,i,k=0,0,0,0\n",
    "        self.model.gen_hidden()\n",
    "        #for k,i in enumerate(range(0,n_batch,self.bptt)):\n",
    "        n_batch=iterator.shape[1]\n",
    "        while i<n_batch-bptt:\n",
    "            if mode_train:\n",
    "                cust_bptt=self.bptt if np.random.random() < 0.95 else self.bptt//np.random.randint (2,4)\n",
    "            else:\n",
    "                cust_bptt=bptt\n",
    "            seq_len=min(cust_bptt,n_batch-1-i)\n",
    "            Xb=train_tokens[:,i:i+seq_len]\n",
    "            Yb=train_tokens[:,i+1:i+1+seq_len]\n",
    "            Xb=torch.LongTensor(Xb)\n",
    "            Yb=torch.LongTensor(Yb)\n",
    "            Xb=Xb.to(self.device)\n",
    "            Yb=Yb.to(self.device)\n",
    "            \n",
    "            if lrs is not None:\n",
    "                lr=lrs[k]\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr']=lr \n",
    "            \n",
    "\n",
    "            loss,acc=self.fit(Xb,Yb,mode_train)\n",
    "            \n",
    "            if lrs is not None:\n",
    "                self.losses.append(loss)\n",
    "            \n",
    "            \n",
    "            epoch_loss+=loss\n",
    "            epoch_acc+=acc\n",
    "            if k%self.print_every == 0:\n",
    "                if k:\n",
    "                    print (f'Batch:{k} {epoch_loss/(k)}  {epoch_acc/(k)}')  \n",
    "                    torch.cuda.empty_cache()\n",
    "            k=k+1\n",
    "            i=i+seq_len\n",
    "        epoch_loss=epoch_loss/k\n",
    "        epoch_acc=epoch_acc/k\n",
    "        \n",
    "        if 1==0:\n",
    "            lr /= 4.0\n",
    "            # Freeze all the layers initially\n",
    "            for param in resnet.parameters():\n",
    "                param.requires_grad=False\n",
    "            torch.save(resnet,'resnet')\n",
    "            torch.save(resnet.state_dict(),'resnet_state_dict')\n",
    "            resnet.load_state_dict(torch.load('resnet_state_dict'))\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr']=lr  \n",
    "            \n",
    "        return epoch_loss,epoch_acc\n",
    "    \n",
    "    def plot_lrs(self, n_roll=1):\n",
    "        import seaborn as sns\n",
    "        ax=sns.lineplot(x=self.lrs,y=pd.Series(self.losses).rolling(n_roll).mean())\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.set_xlabel('Learning Rate')\n",
    "\n",
    "    \n",
    "    def run_epochs(self,dltrain,dlvalid,n_epochs=1):\n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            loss,acc=self.run_epoch(dltrain,True)\n",
    "            print (f'Epoch:{epoch} Loss:{loss}')\n",
    "            lossv,accv=self.run_epoch(dlvalid,mode_train=False)\n",
    "            print (f'Epoch:{epoch} Loss:{loss} Accuracy:{acc} Loss:{lossv} Accuracy:{accv}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(model.parameters(),lr=1e-3,betas=(0.9,0.999), weight_decay=wd)\n",
    "metric_fn=accuracy_multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_batch=np.int(np.ceil(train_tokens.shape[1]/bptt))\n",
    "n_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.weight.requires_grad, model.decoder.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner=Learner(model,optimizer,accuracy_multinomial,device,bptt,500,0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1157"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Loss:6.263374475332407\n",
      "Epoch:0 Loss:6.263374475332407 Accuracy:0.06565934262023522 Loss:5.697565078735352 Accuracy:0.1329670399427414\n",
      "Epoch:1 Loss:5.719012223757231\n",
      "Epoch:1 Loss:5.719012223757231 Accuracy:0.12466188233632308 Loss:5.336146354675293 Accuracy:0.17142857611179352\n",
      "Epoch:2 Loss:5.43531689277062\n",
      "Epoch:2 Loss:5.43531689277062 Accuracy:0.1581572385934683 Loss:5.096234321594238 Accuracy:0.19203297793865204\n",
      "Epoch:3 Loss:5.242917390970083\n",
      "Epoch:3 Loss:5.242917390970083 Accuracy:0.17709490198355454 Loss:4.90220832824707 Accuracy:0.21291209757328033\n",
      "Epoch:4 Loss:5.111061793107253\n",
      "Epoch:4 Loss:5.111061793107253 Accuracy:0.19025782438424918 Loss:4.76827335357666 Accuracy:0.2260989099740982\n",
      "Epoch:5 Loss:4.993988110468938\n",
      "Epoch:5 Loss:4.993988110468938 Accuracy:0.20200761694174546 Loss:4.640859603881836 Accuracy:0.23571430146694183\n",
      "Epoch:6 Loss:4.89334381543673\n",
      "Epoch:6 Loss:4.89334381543673 Accuracy:0.21107355447915885 Loss:4.542698383331299 Accuracy:0.24725276231765747\n",
      "Epoch:7 Loss:4.804420691270095\n",
      "Epoch:7 Loss:4.804420691270095 Accuracy:0.21965343447831961 Loss:4.440776348114014 Accuracy:0.25824177265167236\n",
      "Epoch:8 Loss:4.722163383777325\n",
      "Epoch:8 Loss:4.722163383777325 Accuracy:0.2288250338572722 Loss:4.3544769287109375 Accuracy:0.27005496621131897\n",
      "Epoch:9 Loss:4.649045210618239\n",
      "Epoch:9 Loss:4.649045210618239 Accuracy:0.235502968613918 Loss:4.262136459350586 Accuracy:0.27335166931152344\n",
      "Epoch:10 Loss:4.586905036653791\n",
      "Epoch:10 Loss:4.586905036653791 Accuracy:0.24209184838192804 Loss:4.191817760467529 Accuracy:0.28626376390457153\n",
      "Epoch:11 Loss:4.516580691704383\n",
      "Epoch:11 Loss:4.516580691704383 Accuracy:0.2488910028567681 Loss:4.088625907897949 Accuracy:0.29615387320518494\n",
      "Epoch:12 Loss:4.439605602851281\n",
      "Epoch:12 Loss:4.439605602851281 Accuracy:0.2562764286994934 Loss:4.0315446853637695 Accuracy:0.3054945170879364\n",
      "Epoch:13 Loss:4.375287789564866\n",
      "Epoch:13 Loss:4.375287789564866 Accuracy:0.2656170886296492 Loss:3.971099376678467 Accuracy:0.3093406856060028\n",
      "Epoch:14 Loss:4.312357095571665\n",
      "Epoch:14 Loss:4.312357095571665 Accuracy:0.27009721902700573 Loss:3.9031057357788086 Accuracy:0.31401100754737854\n",
      "Epoch:15 Loss:4.264231425065261\n",
      "Epoch:15 Loss:4.264231425065261 Accuracy:0.2742392351994148 Loss:3.852952003479004 Accuracy:0.3206044137477875\n",
      "Epoch:16 Loss:4.224158096313476\n",
      "Epoch:16 Loss:4.224158096313476 Accuracy:0.27712215979894 Loss:3.7977590560913086 Accuracy:0.3263736367225647\n",
      "Epoch:17 Loss:4.175368455740122\n",
      "Epoch:17 Loss:4.175368455740122 Accuracy:0.28300288090339076 Loss:3.7439966201782227 Accuracy:0.3351648449897766\n",
      "Epoch:18 Loss:4.1223198633927565\n",
      "Epoch:18 Loss:4.1223198633927565 Accuracy:0.2891166645746965 Loss:3.7030632495880127 Accuracy:0.3431318700313568\n",
      "Epoch:19 Loss:4.067421528009268\n",
      "Epoch:19 Loss:4.067421528009268 Accuracy:0.29245563653799206 Loss:3.6307458877563477 Accuracy:0.343406617641449\n",
      "Epoch:20 Loss:4.021999010672936\n",
      "Epoch:20 Loss:4.021999010672936 Accuracy:0.2987531813291403 Loss:3.5665218830108643 Accuracy:0.36153846979141235\n",
      "Epoch:21 Loss:3.986130109200111\n",
      "Epoch:21 Loss:3.986130109200111 Accuracy:0.3024514065339015 Loss:3.5146484375 Accuracy:0.36236265301704407\n",
      "Epoch:22 Loss:3.9414440301748424\n",
      "Epoch:22 Loss:3.9414440301748424 Accuracy:0.305748109634106 Loss:3.4534292221069336 Accuracy:0.36840662360191345\n",
      "Epoch:23 Loss:3.8832773061899037\n",
      "Epoch:23 Loss:3.8832773061899037 Accuracy:0.31128488595669085 Loss:3.4037373065948486 Accuracy:0.37362638115882874\n",
      "Epoch:24 Loss:3.827796532557561\n",
      "Epoch:24 Loss:3.827796532557561 Accuracy:0.31760632074796236 Loss:3.352898359298706 Accuracy:0.38241758942604065\n",
      "Epoch:25 Loss:3.7960054140824537\n",
      "Epoch:25 Loss:3.7960054140824537 Accuracy:0.32085799941649806 Loss:3.2943999767303467 Accuracy:0.39313188195228577\n",
      "Epoch:26 Loss:3.7503927670992336\n",
      "Epoch:26 Loss:3.7503927670992336 Accuracy:0.32622570945666385 Loss:3.254986524581909 Accuracy:0.3978022038936615\n",
      "Epoch:27 Loss:3.7157977727743297\n",
      "Epoch:27 Loss:3.7157977727743297 Accuracy:0.33051565518746007 Loss:3.1895885467529297 Accuracy:0.40631869435310364\n",
      "Epoch:28 Loss:3.653497475844163\n",
      "Epoch:28 Loss:3.653497475844163 Accuracy:0.33614154962392956 Loss:3.127126455307007 Accuracy:0.41208791732788086\n",
      "Epoch:29 Loss:3.637979635825524\n",
      "Epoch:29 Loss:3.637979635825524 Accuracy:0.33744718249027544 Loss:3.118211269378662 Accuracy:0.4156593680381775\n",
      "Epoch:30 Loss:3.587671536665696\n",
      "Epoch:30 Loss:3.587671536665696 Accuracy:0.3419061853335454 Loss:3.0419046878814697 Accuracy:0.42582419514656067\n",
      "Epoch:31 Loss:3.536629401720487\n",
      "Epoch:31 Loss:3.536629401720487 Accuracy:0.3493449023136726 Loss:2.9798667430877686 Accuracy:0.4343406856060028\n",
      "Epoch:32 Loss:3.4916490958287167\n",
      "Epoch:32 Loss:3.4916490958287167 Accuracy:0.35473374449289763 Loss:2.95109486579895 Accuracy:0.44285717606544495\n",
      "Epoch:33 Loss:3.466027993422288\n",
      "Epoch:33 Loss:3.466027993422288 Accuracy:0.35641517547460705 Loss:2.911118268966675 Accuracy:0.44065937399864197\n",
      "Epoch:34 Loss:3.440968916966365\n",
      "Epoch:34 Loss:3.440968916966365 Accuracy:0.3596787979969612 Loss:2.8519480228424072 Accuracy:0.44862639904022217\n",
      "Epoch:35 Loss:3.3931039480062632\n",
      "Epoch:35 Loss:3.3931039480062632 Accuracy:0.36794169591023373 Loss:2.8146109580993652 Accuracy:0.45164838433265686\n",
      "Epoch:36 Loss:3.35651342685406\n",
      "Epoch:36 Loss:3.35651342685406 Accuracy:0.3697802401505984 Loss:2.7747912406921387 Accuracy:0.4593406915664673\n",
      "Epoch:37 Loss:3.308422198662391\n",
      "Epoch:37 Loss:3.308422198662391 Accuracy:0.3757607936859131 Loss:2.701862096786499 Accuracy:0.47280222177505493\n",
      "Epoch:38 Loss:3.268612659894503\n",
      "Epoch:38 Loss:3.268612659894503 Accuracy:0.3797870324208186 Loss:2.683516025543213 Accuracy:0.4818681478500366\n",
      "Epoch:39 Loss:3.27406644821167\n",
      "Epoch:39 Loss:3.27406644821167 Accuracy:0.3821906487147013 Loss:2.5859389305114746 Accuracy:0.5008242130279541\n",
      "Epoch:40 Loss:3.2300027779170444\n",
      "Epoch:40 Loss:3.2300027779170444 Accuracy:0.38567335052149637 Loss:2.5456948280334473 Accuracy:0.5049450993537903\n",
      "Epoch:41 Loss:3.1600611760066104\n",
      "Epoch:41 Loss:3.1600611760066104 Accuracy:0.39429877354548526 Loss:2.5005080699920654 Accuracy:0.519505500793457\n",
      "Epoch:42 Loss:3.1351038676041822\n",
      "Epoch:42 Loss:3.1351038676041822 Accuracy:0.39685124158859253 Loss:2.4588749408721924 Accuracy:0.52170330286026\n",
      "Epoch:43 Loss:3.104336518507737\n",
      "Epoch:43 Loss:3.104336518507737 Accuracy:0.40214269436322725 Loss:2.386859893798828 Accuracy:0.5348901152610779\n",
      "Epoch:44 Loss:3.048912085019625\n",
      "Epoch:44 Loss:3.048912085019625 Accuracy:0.4097743538709787 Loss:2.354912281036377 Accuracy:0.5420330166816711\n",
      "Epoch:45 Loss:3.036995245860173\n",
      "Epoch:45 Loss:3.036995245860173 Accuracy:0.4105810775206639 Loss:2.354609489440918 Accuracy:0.5315934419631958\n",
      "Epoch:46 Loss:2.9867449356959415\n",
      "Epoch:46 Loss:2.9867449356959415 Accuracy:0.41924640994805557 Loss:2.2841548919677734 Accuracy:0.5532967448234558\n",
      "Epoch:47 Loss:2.983651179533738\n",
      "Epoch:47 Loss:2.983651179533738 Accuracy:0.42037195655015797 Loss:2.269822597503662 Accuracy:0.5579670667648315\n",
      "Epoch:48 Loss:2.929006044681256\n",
      "Epoch:48 Loss:2.929006044681256 Accuracy:0.42751481441351086 Loss:2.224360704421997 Accuracy:0.5620879530906677\n",
      "Epoch:49 Loss:2.8883391527029185\n",
      "Epoch:49 Loss:2.8883391527029185 Accuracy:0.4306001892456642 Loss:2.1967787742614746 Accuracy:0.5626373887062073\n",
      "Epoch:50 Loss:2.84366827744704\n",
      "Epoch:50 Loss:2.84366827744704 Accuracy:0.4407227589533879 Loss:2.1500048637390137 Accuracy:0.5774725675582886\n",
      "Epoch:51 Loss:2.85579644716703\n",
      "Epoch:51 Loss:2.85579644716703 Accuracy:0.43816570135263294 Loss:2.135768175125122 Accuracy:0.5714285969734192\n",
      "Epoch:52 Loss:2.814512619605431\n",
      "Epoch:52 Loss:2.814512619605431 Accuracy:0.44494929909706116 Loss:2.1171324253082275 Accuracy:0.5840659737586975\n",
      "Epoch:53 Loss:2.7880833882551928\n",
      "Epoch:53 Loss:2.7880833882551928 Accuracy:0.44769655282680804 Loss:2.0539162158966064 Accuracy:0.6000000238418579\n",
      "Epoch:54 Loss:2.736711575434758\n",
      "Epoch:54 Loss:2.736711575434758 Accuracy:0.45557905848209673 Loss:2.0432939529418945 Accuracy:0.5961538553237915\n",
      "Epoch:55 Loss:2.7241925459641676\n",
      "Epoch:55 Loss:2.7241925459641676 Accuracy:0.45762893099051255 Loss:2.020112991333008 Accuracy:0.5978022217750549\n",
      "Epoch:56 Loss:2.710467049053737\n",
      "Epoch:56 Loss:2.710467049053737 Accuracy:0.4589840535606657 Loss:2.0142812728881836 Accuracy:0.5969780683517456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:57 Loss:2.6595315749828634\n",
      "Epoch:57 Loss:2.6595315749828634 Accuracy:0.46570897560853225 Loss:1.9695342779159546 Accuracy:0.6140109896659851\n",
      "Epoch:58 Loss:2.6587691123668966\n",
      "Epoch:58 Loss:2.6587691123668966 Accuracy:0.46804735981501067 Loss:1.9412063360214233 Accuracy:0.612912118434906\n",
      "Epoch:59 Loss:2.616838812828064\n",
      "Epoch:59 Loss:2.616838812828064 Accuracy:0.4736519881657192 Loss:1.9311718940734863 Accuracy:0.6167582869529724\n",
      "Epoch:60 Loss:2.5964413422804613\n",
      "Epoch:60 Loss:2.5964413422804613 Accuracy:0.4762257177096147 Loss:1.8960481882095337 Accuracy:0.6263736486434937\n",
      "Epoch:61 Loss:2.5996432644980296\n",
      "Epoch:61 Loss:2.5996432644980296 Accuracy:0.4791840272290366 Loss:1.8853416442871094 Accuracy:0.6258242130279541\n",
      "Epoch:62 Loss:2.568535327911377\n",
      "Epoch:62 Loss:2.568535327911377 Accuracy:0.4809805773771726 Loss:1.8517916202545166 Accuracy:0.634615421295166\n",
      "Epoch:63 Loss:2.534862756729126\n",
      "Epoch:63 Loss:2.534862756729126 Accuracy:0.4863482851248521 Loss:1.8238099813461304 Accuracy:0.6376373767852783\n",
      "Epoch:64 Loss:2.508223686899458\n",
      "Epoch:64 Loss:2.508223686899458 Accuracy:0.49086412361689974 Loss:1.8111495971679688 Accuracy:0.6483516693115234\n",
      "Epoch:65 Loss:2.4764685997596154\n",
      "Epoch:65 Loss:2.4764685997596154 Accuracy:0.4967878552583548 Loss:1.77360200881958 Accuracy:0.6480769515037537\n",
      "Epoch:66 Loss:2.4659883242387037\n",
      "Epoch:66 Loss:2.4659883242387037 Accuracy:0.4976120270215548 Loss:1.7495474815368652 Accuracy:0.6560440063476562\n",
      "Epoch:67 Loss:2.4120714481060324\n",
      "Epoch:67 Loss:2.4120714481060324 Accuracy:0.5072485506534576 Loss:1.7296068668365479 Accuracy:0.662912130355835\n",
      "Epoch:68 Loss:2.4094411043020396\n",
      "Epoch:68 Loss:2.4094411043020396 Accuracy:0.5063821008572211 Loss:1.7009460926055908 Accuracy:0.670604407787323\n",
      "Epoch:69 Loss:2.3781308210813084\n",
      "Epoch:69 Loss:2.3781308210813084 Accuracy:0.5125951216771052 Loss:1.6800904273986816 Accuracy:0.6741758584976196\n",
      "Epoch:70 Loss:2.3371314452244687\n",
      "Epoch:70 Loss:2.3371314452244687 Accuracy:0.5171965337716616 Loss:1.657112956047058 Accuracy:0.6813187003135681\n",
      "Epoch:71 Loss:2.336932218991793\n",
      "Epoch:71 Loss:2.336932218991793 Accuracy:0.5207220315933228 Loss:1.6462706327438354 Accuracy:0.6826923489570618\n",
      "Epoch:72 Loss:2.336288488828219\n",
      "Epoch:72 Loss:2.336288488828219 Accuracy:0.5196534509842212 Loss:1.6261866092681885 Accuracy:0.6815934181213379\n",
      "Epoch:73 Loss:2.324712844995352\n",
      "Epoch:73 Loss:2.324712844995352 Accuracy:0.5175190407496232 Loss:1.6320421695709229 Accuracy:0.686813235282898\n",
      "Epoch:74 Loss:2.298728117576012\n",
      "Epoch:74 Loss:2.298728117576012 Accuracy:0.5248098235863906 Loss:1.588140845298767 Accuracy:0.69010990858078\n",
      "Epoch:75 Loss:2.2762093727405253\n",
      "Epoch:75 Loss:2.2762093727405253 Accuracy:0.5286771196585435 Loss:1.5685817003250122 Accuracy:0.694505512714386\n",
      "Epoch:76 Loss:2.224622359642616\n",
      "Epoch:76 Loss:2.224622359642616 Accuracy:0.5340448251137366 Loss:1.5589747428894043 Accuracy:0.6969780325889587\n",
      "Epoch:77 Loss:2.233052235383254\n",
      "Epoch:77 Loss:2.233052235383254 Accuracy:0.5346576800713172 Loss:1.5446425676345825 Accuracy:0.7013736367225647\n",
      "Epoch:78 Loss:2.216950765022865\n",
      "Epoch:78 Loss:2.216950765022865 Accuracy:0.5366652837166419 Loss:1.536874771118164 Accuracy:0.7057692408561707\n",
      "Epoch:79 Loss:2.211955107175387\n",
      "Epoch:79 Loss:2.211955107175387 Accuracy:0.5364116865854996 Loss:1.5134304761886597 Accuracy:0.7112637758255005\n",
      "Epoch:80 Loss:2.190054706164769\n",
      "Epoch:80 Loss:2.190054706164769 Accuracy:0.538820743560791 Loss:1.492809772491455 Accuracy:0.71181321144104\n",
      "Epoch:81 Loss:2.160849956365732\n",
      "Epoch:81 Loss:2.160849956365732 Accuracy:0.5471728260700519 Loss:1.4733575582504272 Accuracy:0.7167582511901855\n",
      "Epoch:82 Loss:2.1675938826340895\n",
      "Epoch:82 Loss:2.1675938826340895 Accuracy:0.5448647645803598 Loss:1.4750711917877197 Accuracy:0.7101648449897766\n",
      "Epoch:83 Loss:2.140296294138982\n",
      "Epoch:83 Loss:2.140296294138982 Accuracy:0.5455410205400907 Loss:1.4658459424972534 Accuracy:0.718406617641449\n",
      "Epoch:84 Loss:2.1350643084599423\n",
      "Epoch:84 Loss:2.1350643084599423 Accuracy:0.5489856554911687 Loss:1.4578964710235596 Accuracy:0.7173077464103699\n",
      "Epoch:85 Loss:2.11743217248183\n",
      "Epoch:85 Loss:2.11743217248183 Accuracy:0.5504438051810632 Loss:1.438118577003479 Accuracy:0.720604419708252\n",
      "Epoch:86 Loss:2.0815902856680064\n",
      "Epoch:86 Loss:2.0815902856680064 Accuracy:0.5550149128987238 Loss:1.428552269935608 Accuracy:0.7269231081008911\n",
      "Epoch:87 Loss:2.0719846670444193\n",
      "Epoch:87 Loss:2.0719846670444193 Accuracy:0.5593406924834619 Loss:1.4154506921768188 Accuracy:0.7247253060340881\n",
      "Epoch:88 Loss:2.0934076492603007\n",
      "Epoch:88 Loss:2.0934076492603007 Accuracy:0.5562975819294269 Loss:1.3967491388320923 Accuracy:0.7266483902931213\n",
      "Epoch:89 Loss:2.047618499168983\n",
      "Epoch:89 Loss:2.047618499168983 Accuracy:0.5613060272656955 Loss:1.3841406106948853 Accuracy:0.7288461923599243\n",
      "Epoch:90 Loss:2.037563892511221\n",
      "Epoch:90 Loss:2.037563892511221 Accuracy:0.5618766133601849 Loss:1.3619688749313354 Accuracy:0.7285714745521545\n",
      "Epoch:91 Loss:1.9863371482262244\n",
      "Epoch:91 Loss:1.9863371482262244 Accuracy:0.5724696104343121 Loss:1.3574968576431274 Accuracy:0.7321428656578064\n",
      "Epoch:92 Loss:2.0064947513433604\n",
      "Epoch:92 Loss:2.0064947513433604 Accuracy:0.5694210024980398 Loss:1.3470641374588013 Accuracy:0.735714316368103\n",
      "Epoch:93 Loss:1.973967011158283\n",
      "Epoch:93 Loss:1.973967011158283 Accuracy:0.5743871835561899 Loss:1.3242435455322266 Accuracy:0.7376374006271362\n",
      "Epoch:94 Loss:1.9295809085552509\n",
      "Epoch:94 Loss:1.9295809085552509 Accuracy:0.5820923447608948 Loss:1.3084479570388794 Accuracy:0.7381868362426758\n",
      "Epoch:95 Loss:1.937738060951233\n",
      "Epoch:95 Loss:1.937738060951233 Accuracy:0.5795647089297955 Loss:1.3079123497009277 Accuracy:0.7442308068275452\n",
      "Epoch:96 Loss:1.9598565285022442\n",
      "Epoch:96 Loss:1.9598565285022442 Accuracy:0.5743449192780715 Loss:1.3067766427993774 Accuracy:0.7384615540504456\n",
      "Epoch:97 Loss:1.9159978798457555\n",
      "Epoch:97 Loss:1.9159978798457555 Accuracy:0.5807880333491734 Loss:1.2803138494491577 Accuracy:0.7398352026939392\n",
      "Epoch:98 Loss:1.9253236513871412\n",
      "Epoch:98 Loss:1.9253236513871412 Accuracy:0.5804522679402278 Loss:1.278071641921997 Accuracy:0.7447802424430847\n",
      "Epoch:99 Loss:1.8889506321686964\n",
      "Epoch:99 Loss:1.8889506321686964 Accuracy:0.5847844710716834 Loss:1.2680084705352783 Accuracy:0.7434066534042358\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(train_tokens,valid_tokens,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Loss:1.890881428351769\n",
      "Epoch:0 Loss:1.890881428351769 Accuracy:0.5864328237680289 Loss:1.2536641359329224 Accuracy:0.7467033267021179\n",
      "Epoch:1 Loss:1.8643977091862605\n",
      "Epoch:1 Loss:1.8643977091862605 Accuracy:0.588669226719783 Loss:1.2419167757034302 Accuracy:0.7513736486434937\n",
      "Epoch:2 Loss:1.862069927729093\n",
      "Epoch:2 Loss:1.862069927729093 Accuracy:0.5904057805354779 Loss:1.2317347526550293 Accuracy:0.7524725794792175\n",
      "Epoch:3 Loss:1.8186275042020357\n",
      "Epoch:3 Loss:1.8186275042020357 Accuracy:0.6005283502432016 Loss:1.2225782871246338 Accuracy:0.7543956637382507\n",
      "Epoch:4 Loss:1.8147938251495361\n",
      "Epoch:4 Loss:1.8147938251495361 Accuracy:0.5990279179352981 Loss:1.2105119228363037 Accuracy:0.7560439705848694\n",
      "Epoch:5 Loss:1.8257078102656774\n",
      "Epoch:5 Loss:1.8257078102656774 Accuracy:0.5959201029368809 Loss:1.1980080604553223 Accuracy:0.7568681836128235\n",
      "Epoch:6 Loss:1.8033296878521259\n",
      "Epoch:6 Loss:1.8033296878521259 Accuracy:0.6006762752166162 Loss:1.192304253578186 Accuracy:0.7587912678718567\n",
      "Epoch:7 Loss:1.7828995906389677\n",
      "Epoch:7 Loss:1.7828995906389677 Accuracy:0.6036624220701364 Loss:1.1815783977508545 Accuracy:0.7620879411697388\n",
      "Epoch:8 Loss:1.8013255871259248\n",
      "Epoch:8 Loss:1.8013255871259248 Accuracy:0.5999577641487122 Loss:1.1704717874526978 Accuracy:0.7645604610443115\n",
      "Epoch:9 Loss:1.7606208324432373\n",
      "Epoch:9 Loss:1.7606208324432373 Accuracy:0.6094491023283738 Loss:1.17837393283844 Accuracy:0.7593407034873962\n",
      "Epoch:10 Loss:1.77186080125662\n",
      "Epoch:10 Loss:1.77186080125662 Accuracy:0.6045012932557327 Loss:1.1613295078277588 Accuracy:0.7629120945930481\n",
      "Epoch:11 Loss:1.7611509561538696\n",
      "Epoch:11 Loss:1.7611509561538696 Accuracy:0.6065934300422668 Loss:1.1521435976028442 Accuracy:0.7634615898132324\n",
      "Epoch:12 Loss:1.739816739008977\n",
      "Epoch:12 Loss:1.739816739008977 Accuracy:0.6100803384414086 Loss:1.1499024629592896 Accuracy:0.7631868720054626\n",
      "Epoch:13 Loss:1.7356089078463042\n",
      "Epoch:13 Loss:1.7356089078463042 Accuracy:0.6092350161992587 Loss:1.148255705833435 Accuracy:0.7629120945930481\n",
      "Epoch:14 Loss:1.7070978421431322\n",
      "Epoch:14 Loss:1.7070978421431322 Accuracy:0.6171781649956336 Loss:1.1429004669189453 Accuracy:0.7629120945930481\n",
      "Epoch:15 Loss:1.7161335945129395\n",
      "Epoch:15 Loss:1.7161335945129395 Accuracy:0.6128487174327557 Loss:1.1438226699829102 Accuracy:0.764011025428772\n",
      "Epoch:16 Loss:1.720374189890348\n",
      "Epoch:16 Loss:1.720374189890348 Accuracy:0.6123838012035077 Loss:1.1271966695785522 Accuracy:0.7664835453033447\n",
      "Epoch:17 Loss:1.7061774219785417\n",
      "Epoch:17 Loss:1.7061774219785417 Accuracy:0.6160714626312256 Loss:1.1225578784942627 Accuracy:0.7645604610443115\n",
      "Epoch:18 Loss:1.691293111214271\n",
      "Epoch:18 Loss:1.691293111214271 Accuracy:0.6157650305674627 Loss:1.1197293996810913 Accuracy:0.7651098966598511\n",
      "Epoch:19 Loss:1.6874644848016591\n",
      "Epoch:19 Loss:1.6874644848016591 Accuracy:0.6146872639656067 Loss:1.1106632947921753 Accuracy:0.7722527980804443\n",
      "Epoch:20 Loss:1.6595987906822791\n",
      "Epoch:20 Loss:1.6595987906822791 Accuracy:0.6207523483496445 Loss:1.1122905015945435 Accuracy:0.7670329809188843\n",
      "Epoch:21 Loss:1.65790698161492\n",
      "Epoch:21 Loss:1.65790698161492 Accuracy:0.6239939286158636 Loss:1.1023850440979004 Accuracy:0.7678571939468384\n",
      "Epoch:22 Loss:1.6640317354883467\n",
      "Epoch:22 Loss:1.6640317354883467 Accuracy:0.6206436710698264 Loss:1.0957449674606323 Accuracy:0.7706044316291809\n",
      "Epoch:23 Loss:1.6358000681950495\n",
      "Epoch:23 Loss:1.6358000681950495 Accuracy:0.627092168881343 Loss:1.093847393989563 Accuracy:0.7719780802726746\n",
      "Epoch:24 Loss:1.614286835377033\n",
      "Epoch:24 Loss:1.614286835377033 Accuracy:0.6285291910171509 Loss:1.0909446477890015 Accuracy:0.7681319117546082\n",
      "Epoch:25 Loss:1.6409801611533532\n",
      "Epoch:25 Loss:1.6409801611533532 Accuracy:0.62358414209806 Loss:1.0803139209747314 Accuracy:0.7700549960136414\n",
      "Epoch:26 Loss:1.6254646869806142\n",
      "Epoch:26 Loss:1.6254646869806142 Accuracy:0.6259298737232502 Loss:1.0747381448745728 Accuracy:0.7736263871192932\n",
      "Epoch:27 Loss:1.6152829390305738\n",
      "Epoch:27 Loss:1.6152829390305738 Accuracy:0.6284023935978229 Loss:1.0724934339523315 Accuracy:0.7760989665985107\n",
      "Epoch:28 Loss:1.5866348926837628\n",
      "Epoch:28 Loss:1.5866348926837628 Accuracy:0.633201356117542 Loss:1.0756925344467163 Accuracy:0.7750000357627869\n",
      "Epoch:29 Loss:1.5867416491875281\n",
      "Epoch:29 Loss:1.5867416491875281 Accuracy:0.6340439090361962 Loss:1.067740559577942 Accuracy:0.7755494713783264\n",
      "Epoch:30 Loss:1.6006078294345312\n",
      "Epoch:30 Loss:1.6006078294345312 Accuracy:0.629355537039893 Loss:1.064456582069397 Accuracy:0.7758241891860962\n",
      "Epoch:31 Loss:1.5888409339464629\n",
      "Epoch:31 Loss:1.5888409339464629 Accuracy:0.630480747956496 Loss:1.057073712348938 Accuracy:0.7769231200218201\n",
      "Epoch:32 Loss:1.588803346340473\n",
      "Epoch:32 Loss:1.588803346340473 Accuracy:0.6322908080541171 Loss:1.0510847568511963 Accuracy:0.7782967686653137\n",
      "Epoch:33 Loss:1.559703515126155\n",
      "Epoch:33 Loss:1.559703515126155 Accuracy:0.6383347740540137 Loss:1.038403034210205 Accuracy:0.7793956398963928\n",
      "Epoch:34 Loss:1.578335063798087\n",
      "Epoch:34 Loss:1.578335063798087 Accuracy:0.6313903714929309 Loss:1.031365156173706 Accuracy:0.7788462042808533\n",
      "Epoch:35 Loss:1.5309640994438758\n",
      "Epoch:35 Loss:1.5309640994438758 Accuracy:0.6409993263391348 Loss:1.0288941860198975 Accuracy:0.7802197933197021\n",
      "Epoch:36 Loss:1.5350060921448927\n",
      "Epoch:36 Loss:1.5350060921448927 Accuracy:0.6414945767476008 Loss:1.0257999897003174 Accuracy:0.7785714864730835\n",
      "Epoch:37 Loss:1.5494873982209425\n",
      "Epoch:37 Loss:1.5494873982209425 Accuracy:0.6369188886422378 Loss:1.0176444053649902 Accuracy:0.7788462042808533\n",
      "Epoch:38 Loss:1.53077783034398\n",
      "Epoch:38 Loss:1.53077783034398 Accuracy:0.640955232656919 Loss:1.0091019868850708 Accuracy:0.7810440063476562\n",
      "Epoch:39 Loss:1.5099803667802076\n",
      "Epoch:39 Loss:1.5099803667802076 Accuracy:0.6434066249774053 Loss:1.0053260326385498 Accuracy:0.7807692885398865\n",
      "Epoch:40 Loss:1.4807017766512358\n",
      "Epoch:40 Loss:1.4807017766512358 Accuracy:0.6501902204293472 Loss:0.9998266100883484 Accuracy:0.781318724155426\n",
      "Epoch:41 Loss:1.4813872025563166\n",
      "Epoch:41 Loss:1.4813872025563166 Accuracy:0.6475394505720872 Loss:1.0002700090408325 Accuracy:0.7826923727989197\n",
      "Epoch:42 Loss:1.4930631014016957\n",
      "Epoch:42 Loss:1.4930631014016957 Accuracy:0.6449915858415457 Loss:0.9980341792106628 Accuracy:0.7802197933197021\n",
      "Epoch:43 Loss:1.4625788835378795\n",
      "Epoch:43 Loss:1.4625788835378795 Accuracy:0.6540152430534363 Loss:0.9867332577705383 Accuracy:0.7859890460968018\n",
      "Epoch:44 Loss:1.4544230424440825\n",
      "Epoch:44 Loss:1.4544230424440825 Accuracy:0.6528952075884893 Loss:0.9834861755371094 Accuracy:0.7837912440299988\n",
      "Epoch:45 Loss:1.4666426548591027\n",
      "Epoch:45 Loss:1.4666426548591027 Accuracy:0.651267987031203 Loss:0.9760592579841614 Accuracy:0.7821428775787354\n",
      "Epoch:46 Loss:1.4411129355430603\n",
      "Epoch:46 Loss:1.4411129355430603 Accuracy:0.6570720757756915 Loss:0.9735180139541626 Accuracy:0.7832418084144592\n",
      "Epoch:47 Loss:1.4491809331453764\n",
      "Epoch:47 Loss:1.4491809331453764 Accuracy:0.6548899503854605 Loss:0.9704657793045044 Accuracy:0.7840659618377686\n",
      "Epoch:48 Loss:1.4471300755228316\n",
      "Epoch:48 Loss:1.4471300755228316 Accuracy:0.6559620797634125 Loss:0.96815425157547 Accuracy:0.7851648926734924\n",
      "Epoch:49 Loss:1.4473899877988374\n",
      "Epoch:49 Loss:1.4473899877988374 Accuracy:0.6540997807796185 Loss:0.9656301736831665 Accuracy:0.7832418084144592\n",
      "Epoch:50 Loss:1.4327470797758837\n",
      "Epoch:50 Loss:1.4327470797758837 Accuracy:0.655494538637308 Loss:0.9604010581970215 Accuracy:0.7846153974533081\n",
      "Epoch:51 Loss:1.4274047704843373\n",
      "Epoch:51 Loss:1.4274047704843373 Accuracy:0.6578191427084116 Loss:0.9568505883216858 Accuracy:0.7846153974533081\n",
      "Epoch:52 Loss:1.4270109580113337\n",
      "Epoch:52 Loss:1.4270109580113337 Accuracy:0.6563609700936538 Loss:0.9542955756187439 Accuracy:0.7848901748657227\n",
      "Epoch:53 Loss:1.40724450808305\n",
      "Epoch:53 Loss:1.40724450808305 Accuracy:0.6617286893037649 Loss:0.9511129260063171 Accuracy:0.7854396104812622\n",
      "Epoch:54 Loss:1.4062300553688636\n",
      "Epoch:54 Loss:1.4062300553688636 Accuracy:0.6611580940393301 Loss:0.9435179829597473 Accuracy:0.785714328289032\n",
      "Epoch:55 Loss:1.385283818611732\n",
      "Epoch:55 Loss:1.385283818611732 Accuracy:0.6662722229957581 Loss:0.9446175694465637 Accuracy:0.7859890460968018\n",
      "Epoch:56 Loss:1.3902251805577959\n",
      "Epoch:56 Loss:1.3902251805577959 Accuracy:0.6639453172683716 Loss:0.9429150223731995 Accuracy:0.7862637639045715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:57 Loss:1.3989505859521718\n",
      "Epoch:57 Loss:1.3989505859521718 Accuracy:0.6621302045308627 Loss:0.939484715461731 Accuracy:0.7865384817123413\n",
      "Epoch:58 Loss:1.39860449387477\n",
      "Epoch:58 Loss:1.39860449387477 Accuracy:0.6615384862973139 Loss:0.9373356699943542 Accuracy:0.7868131995201111\n",
      "Epoch:59 Loss:1.3841655804560735\n",
      "Epoch:59 Loss:1.3841655804560735 Accuracy:0.6636517689778254 Loss:0.9309297800064087 Accuracy:0.7881868481636047\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(train_tokens,valid_tokens,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),f'{DATAPATH}/inter/varybptt_model_state_dict')\n",
    "torch.save(optimizer.state_dict(),f'{DATAPATH}/inter/varybptt_learner_state_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.unfreeze_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.weight.requires_grad, model.decoder.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Loss:1.3092224322832549\n",
      "Epoch:0 Loss:1.3092224322832549 Accuracy:0.6738377305177542 Loss:0.8163505792617798 Accuracy:0.8186813592910767\n",
      "Epoch:1 Loss:1.1967030671926646\n",
      "Epoch:1 Loss:1.1967030671926646 Accuracy:0.7120456878955548 Loss:0.7562476992607117 Accuracy:0.8439561128616333\n",
      "Epoch:2 Loss:1.1329299440750709\n",
      "Epoch:2 Loss:1.1329299440750709 Accuracy:0.7325857510933509 Loss:0.7096996903419495 Accuracy:0.8565934300422668\n",
      "Epoch:3 Loss:1.1043552527060876\n",
      "Epoch:3 Loss:1.1043552527060876 Accuracy:0.7453931019856379 Loss:0.6627073287963867 Accuracy:0.8700549602508545\n",
      "Epoch:4 Loss:1.0509100877321684\n",
      "Epoch:4 Loss:1.0509100877321684 Accuracy:0.758495385830219 Loss:0.6220210790634155 Accuracy:0.8750000596046448\n",
      "Epoch:5 Loss:1.0063160061836243\n",
      "Epoch:5 Loss:1.0063160061836243 Accuracy:0.7673288583755493 Loss:0.5852211713790894 Accuracy:0.8840659856796265\n",
      "Epoch:6 Loss:0.9578913817038903\n",
      "Epoch:6 Loss:0.9578913817038903 Accuracy:0.7757819478328412 Loss:0.5505117177963257 Accuracy:0.8917582631111145\n",
      "Epoch:7 Loss:0.9435579547515283\n",
      "Epoch:7 Loss:0.9435579547515283 Accuracy:0.7795647015938392 Loss:0.524219810962677 Accuracy:0.8920329809188843\n",
      "Epoch:8 Loss:0.9118417684848492\n",
      "Epoch:8 Loss:0.9118417684848492 Accuracy:0.7835799272243793 Loss:0.502581000328064 Accuracy:0.8986263871192932\n",
      "Epoch:9 Loss:0.881942515189831\n",
      "Epoch:9 Loss:0.881942515189831 Accuracy:0.7880389186052176 Loss:0.48287248611450195 Accuracy:0.9010989665985107\n",
      "Epoch:10 Loss:0.8560144488628094\n",
      "Epoch:10 Loss:0.8560144488628094 Accuracy:0.794082875435169 Loss:0.4630480408668518 Accuracy:0.9030219912528992\n",
      "Epoch:11 Loss:0.846968914781298\n",
      "Epoch:11 Loss:0.846968914781298 Accuracy:0.7934433094092778 Loss:0.4480663537979126 Accuracy:0.9043956398963928\n",
      "Epoch:12 Loss:0.8143800038557786\n",
      "Epoch:12 Loss:0.8143800038557786 Accuracy:0.8029374801195585 Loss:0.4339568614959717 Accuracy:0.9032967686653137\n",
      "Epoch:13 Loss:0.7850088018637437\n",
      "Epoch:13 Loss:0.7850088018637437 Accuracy:0.8055156744443454 Loss:0.41498804092407227 Accuracy:0.9082418084144592\n",
      "Epoch:14 Loss:0.7738815958683307\n",
      "Epoch:14 Loss:0.7738815958683307 Accuracy:0.8108833798995385 Loss:0.4048011004924774 Accuracy:0.9068681597709656\n",
      "Epoch:15 Loss:0.7380934770290668\n",
      "Epoch:15 Loss:0.7380934770290668 Accuracy:0.8146027372433589 Loss:0.3873308598995209 Accuracy:0.9126374125480652\n",
      "Epoch:16 Loss:0.7457531002851633\n",
      "Epoch:16 Loss:0.7457531002851633 Accuracy:0.8132079793856695 Loss:0.37547945976257324 Accuracy:0.9162088632583618\n",
      "Epoch:17 Loss:0.7109345335226792\n",
      "Epoch:17 Loss:0.7109345335226792 Accuracy:0.8195055310542767 Loss:0.36328884959220886 Accuracy:0.9175824522972107\n",
      "Epoch:18 Loss:0.7173396944999695\n",
      "Epoch:18 Loss:0.7173396944999695 Accuracy:0.8192942050787119 Loss:0.35317060351371765 Accuracy:0.9189561009407043\n",
      "Epoch:19 Loss:0.7054345011711121\n",
      "Epoch:19 Loss:0.7054345011711121 Accuracy:0.8243871927261353 Loss:0.3426123261451721 Accuracy:0.9214286208152771\n",
      "Epoch:20 Loss:0.6900381766832792\n",
      "Epoch:20 Loss:0.6900381766832792 Accuracy:0.8253592940477225 Loss:0.331356942653656 Accuracy:0.9241758584976196\n",
      "Epoch:21 Loss:0.6608664118326627\n",
      "Epoch:21 Loss:0.6608664118326627 Accuracy:0.8313609820145828 Loss:0.3243885040283203 Accuracy:0.9236264228820801\n",
      "Epoch:22 Loss:0.6702982324820298\n",
      "Epoch:22 Loss:0.6702982324820298 Accuracy:0.829289972782135 Loss:0.314353883266449 Accuracy:0.9258242249488831\n",
      "Epoch:23 Loss:0.6677058041095734\n",
      "Epoch:23 Loss:0.6677058041095734 Accuracy:0.8309782998902457 Loss:0.3001702129840851 Accuracy:0.9310439825057983\n",
      "Epoch:24 Loss:0.6508234785153315\n",
      "Epoch:24 Loss:0.6508234785153315 Accuracy:0.8338969166462238 Loss:0.29342007637023926 Accuracy:0.932417631149292\n",
      "Epoch:25 Loss:0.6367455079005315\n",
      "Epoch:25 Loss:0.6367455079005315 Accuracy:0.8355663877267104 Loss:0.2865299880504608 Accuracy:0.9332417845726013\n",
      "Epoch:26 Loss:0.6147753688005301\n",
      "Epoch:26 Loss:0.6147753688005301 Accuracy:0.8403423795333276 Loss:0.2825002670288086 Accuracy:0.9335165023803711\n",
      "Epoch:27 Loss:0.6070286402335534\n",
      "Epoch:27 Loss:0.6070286402335534 Accuracy:0.8438779849272507 Loss:0.27395179867744446 Accuracy:0.9348901510238647\n",
      "Epoch:28 Loss:0.612971122775759\n",
      "Epoch:28 Loss:0.612971122775759 Accuracy:0.8419835141726902 Loss:0.26962342858314514 Accuracy:0.9359890818595886\n",
      "Epoch:29 Loss:0.6101917670323298\n",
      "Epoch:29 Loss:0.6101917670323298 Accuracy:0.8416526088347802 Loss:0.26578664779663086 Accuracy:0.9362637996673584\n",
      "Epoch:30 Loss:0.5913201845609225\n",
      "Epoch:30 Loss:0.5913201845609225 Accuracy:0.8459848257211539 Loss:0.2553248405456543 Accuracy:0.9406594038009644\n",
      "Epoch:31 Loss:0.573995801118704\n",
      "Epoch:31 Loss:0.573995801118704 Accuracy:0.8505917512453519 Loss:0.24818269908428192 Accuracy:0.9412088394165039\n",
      "Epoch:32 Loss:0.5626158347496619\n",
      "Epoch:32 Loss:0.5626158347496619 Accuracy:0.8518597162686862 Loss:0.23827126622200012 Accuracy:0.944505512714386\n",
      "Epoch:33 Loss:0.5628214891140277\n",
      "Epoch:33 Loss:0.5628214891140277 Accuracy:0.8531276858769931 Loss:0.22989395260810852 Accuracy:0.946703314781189\n",
      "Epoch:34 Loss:0.5541980220721319\n",
      "Epoch:34 Loss:0.5541980220721319 Accuracy:0.8536771352474506 Loss:0.22715045511722565 Accuracy:0.9494506120681763\n",
      "Epoch:35 Loss:0.5489514515950129\n",
      "Epoch:35 Loss:0.5489514515950129 Accuracy:0.8555368093343881 Loss:0.22138772904872894 Accuracy:0.9480769634246826\n",
      "Epoch:36 Loss:0.5229790417047647\n",
      "Epoch:36 Loss:0.5229790417047647 Accuracy:0.8616230350274307 Loss:0.2175910770893097 Accuracy:0.949725329875946\n",
      "Epoch:37 Loss:0.5256781325890467\n",
      "Epoch:37 Loss:0.5256781325890467 Accuracy:0.8611369866591233 Loss:0.21631017327308655 Accuracy:0.9508242011070251\n",
      "Epoch:38 Loss:0.5307059952845941\n",
      "Epoch:38 Loss:0.5307059952845941 Accuracy:0.8607777311251714 Loss:0.20719914138317108 Accuracy:0.9516484141349792\n",
      "Epoch:39 Loss:0.503478297820458\n",
      "Epoch:39 Loss:0.503478297820458 Accuracy:0.8640891176003677 Loss:0.2083335518836975 Accuracy:0.9530220031738281\n",
      "Epoch:40 Loss:0.5099782989575312\n",
      "Epoch:40 Loss:0.5099782989575312 Accuracy:0.8633237572816702 Loss:0.20220161974430084 Accuracy:0.9516484141349792\n",
      "Epoch:41 Loss:0.5186170156185443\n",
      "Epoch:41 Loss:0.5186170156185443 Accuracy:0.8628275990486145 Loss:0.1986374706029892 Accuracy:0.9516484141349792\n",
      "Epoch:42 Loss:0.4937283213321979\n",
      "Epoch:42 Loss:0.4937283213321979 Accuracy:0.868822849713839 Loss:0.19742527604103088 Accuracy:0.9524725675582886\n",
      "Epoch:43 Loss:0.5034177142840165\n",
      "Epoch:43 Loss:0.5034177142840165 Accuracy:0.8652578546450689 Loss:0.18716661632061005 Accuracy:0.9568681716918945\n",
      "Epoch:44 Loss:0.4972083191076914\n",
      "Epoch:44 Loss:0.4972083191076914 Accuracy:0.8667662302652995 Loss:0.1860499382019043 Accuracy:0.9576923251152039\n",
      "Epoch:45 Loss:0.48841404914855957\n",
      "Epoch:45 Loss:0.48841404914855957 Accuracy:0.8687447630442106 Loss:0.18205584585666656 Accuracy:0.9582418203353882\n",
      "Epoch:46 Loss:0.4847984703687521\n",
      "Epoch:46 Loss:0.4847984703687521 Accuracy:0.870498776435852 Loss:0.17678602039813995 Accuracy:0.9598901271820068\n",
      "Epoch:47 Loss:0.47407384642532896\n",
      "Epoch:47 Loss:0.47407384642532896 Accuracy:0.872465227331434 Loss:0.17503897845745087 Accuracy:0.9598901271820068\n",
      "Epoch:48 Loss:0.4666350300495441\n",
      "Epoch:48 Loss:0.4666350300495441 Accuracy:0.8736052742371192 Loss:0.16994667053222656 Accuracy:0.9604396224021912\n",
      "Epoch:49 Loss:0.4502634841662187\n",
      "Epoch:49 Loss:0.4502634841662187 Accuracy:0.8775570896955637 Loss:0.17093372344970703 Accuracy:0.9596154093742371\n",
      "Epoch:50 Loss:0.4519909748009273\n",
      "Epoch:50 Loss:0.4519909748009273 Accuracy:0.8770681577069419 Loss:0.17276504635810852 Accuracy:0.9571428894996643\n",
      "Epoch:51 Loss:0.462504315834779\n",
      "Epoch:51 Loss:0.462504315834779 Accuracy:0.8762891430121201 Loss:0.165928915143013 Accuracy:0.9601649045944214\n",
      "Epoch:52 Loss:0.43877911338439357\n",
      "Epoch:52 Loss:0.43877911338439357 Accuracy:0.8808115353951087 Loss:0.15554383397102356 Accuracy:0.96181321144104\n",
      "Epoch:53 Loss:0.4503854948740739\n",
      "Epoch:53 Loss:0.4503854948740739 Accuracy:0.8774936978633587 Loss:0.154291033744812 Accuracy:0.9634615778923035\n",
      "Epoch:54 Loss:0.4304315172708951\n",
      "Epoch:54 Loss:0.4304315172708951 Accuracy:0.8836157551178565 Loss:0.14998416602611542 Accuracy:0.9653846621513367\n",
      "Epoch:55 Loss:0.4358143233335935\n",
      "Epoch:55 Loss:0.4358143233335935 Accuracy:0.8811073990968558 Loss:0.147298663854599 Accuracy:0.966208815574646\n",
      "Epoch:56 Loss:0.4238640688932859\n",
      "Epoch:56 Loss:0.4238640688932859 Accuracy:0.8845308881539565 Loss:0.1425466388463974 Accuracy:0.9656593799591064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:57 Loss:0.4029394250649672\n",
      "Epoch:57 Loss:0.4029394250649672 Accuracy:0.8898554077515235 Loss:0.14322823286056519 Accuracy:0.966208815574646\n",
      "Epoch:58 Loss:0.4101918133405539\n",
      "Epoch:58 Loss:0.4101918133405539 Accuracy:0.8878064568226154 Loss:0.13585084676742554 Accuracy:0.9686813354492188\n",
      "Epoch:59 Loss:0.4111857781043419\n",
      "Epoch:59 Loss:0.4111857781043419 Accuracy:0.8881023251093351 Loss:0.13609245419502258 Accuracy:0.9678571820259094\n",
      "Epoch:60 Loss:0.3981272807488075\n",
      "Epoch:60 Loss:0.3981272807488075 Accuracy:0.8920688399901757 Loss:0.13106219470500946 Accuracy:0.9708791375160217\n",
      "Epoch:61 Loss:0.39515947149350095\n",
      "Epoch:61 Loss:0.39515947149350095 Accuracy:0.8924979200729957 Loss:0.13129663467407227 Accuracy:0.9703297019004822\n",
      "Epoch:62 Loss:0.39794933337431687\n",
      "Epoch:62 Loss:0.39794933337431687 Accuracy:0.8911242989393381 Loss:0.12703180313110352 Accuracy:0.971428632736206\n",
      "Epoch:63 Loss:0.3880743602911631\n",
      "Epoch:63 Loss:0.3880743602911631 Accuracy:0.8934201598167419 Loss:0.12728388607501984 Accuracy:0.9711539149284363\n",
      "Epoch:64 Loss:0.38889397795383746\n",
      "Epoch:64 Loss:0.38889397795383746 Accuracy:0.892730387357565 Loss:0.1263018250465393 Accuracy:0.9728022217750549\n",
      "Epoch:65 Loss:0.3848722737569075\n",
      "Epoch:65 Loss:0.3848722737569075 Accuracy:0.8923500042695266 Loss:0.12329743802547455 Accuracy:0.9725275039672852\n",
      "Epoch:66 Loss:0.39464465471414417\n",
      "Epoch:66 Loss:0.39464465471414417 Accuracy:0.8908284398225638 Loss:0.12238894402980804 Accuracy:0.971428632736206\n",
      "Epoch:67 Loss:0.37002767278597903\n",
      "Epoch:67 Loss:0.37002767278597903 Accuracy:0.8967915360744183 Loss:0.12028749287128448 Accuracy:0.9717033505439758\n",
      "Epoch:68 Loss:0.37804786746318525\n",
      "Epoch:68 Loss:0.37804786746318525 Accuracy:0.8947591277269217 Loss:0.11837194859981537 Accuracy:0.971428632736206\n",
      "Epoch:69 Loss:0.35765257707008946\n",
      "Epoch:69 Loss:0.35765257707008946 Accuracy:0.9000643537594721 Loss:0.1187594085931778 Accuracy:0.9717033505439758\n",
      "Epoch:70 Loss:0.3684435440943791\n",
      "Epoch:70 Loss:0.3684435440943791 Accuracy:0.8982046659176166 Loss:0.11786513775587082 Accuracy:0.973626434803009\n",
      "Epoch:71 Loss:0.3759499925833482\n",
      "Epoch:71 Loss:0.3759499925833482 Accuracy:0.8960270927502558 Loss:0.11154498159885406 Accuracy:0.9752747416496277\n",
      "Epoch:72 Loss:0.37397685434137073\n",
      "Epoch:72 Loss:0.37397685434137073 Accuracy:0.8971947729587555 Loss:0.11139003932476044 Accuracy:0.9730769395828247\n",
      "Epoch:73 Loss:0.3530143522299253\n",
      "Epoch:73 Loss:0.3530143522299253 Accuracy:0.9008876176980826 Loss:0.10782712697982788 Accuracy:0.9747253060340881\n",
      "Epoch:74 Loss:0.3450557520756355\n",
      "Epoch:74 Loss:0.3450557520756355 Accuracy:0.903423533989833 Loss:0.10254455357789993 Accuracy:0.9782967567443848\n",
      "Epoch:75 Loss:0.35192635655403137\n",
      "Epoch:75 Loss:0.35192635655403137 Accuracy:0.9018385914655832 Loss:0.09965897351503372 Accuracy:0.9777473211288452\n",
      "Epoch:76 Loss:0.35046208134064305\n",
      "Epoch:76 Loss:0.35046208134064305 Accuracy:0.9032967411554776 Loss:0.09574717283248901 Accuracy:0.978022038936615\n",
      "Epoch:77 Loss:0.3479044276934404\n",
      "Epoch:77 Loss:0.3479044276934404 Accuracy:0.9033390054335961 Loss:0.09669177979230881 Accuracy:0.978022038936615\n",
      "Epoch:78 Loss:0.33162755691088164\n",
      "Epoch:78 Loss:0.33162755691088164 Accuracy:0.9074598871744596 Loss:0.09462638944387436 Accuracy:0.9785714745521545\n",
      "Epoch:79 Loss:0.3367209391934531\n",
      "Epoch:79 Loss:0.3367209391934531 Accuracy:0.9072998889854976 Loss:0.09338118880987167 Accuracy:0.9777473211288452\n",
      "Epoch:80 Loss:0.33704674473175633\n",
      "Epoch:80 Loss:0.33704674473175633 Accuracy:0.9068681781108563 Loss:0.09425749629735947 Accuracy:0.978022038936615\n",
      "Epoch:81 Loss:0.32947489832128796\n",
      "Epoch:81 Loss:0.32947489832128796 Accuracy:0.9094260122094836 Loss:0.09511764347553253 Accuracy:0.9777473211288452\n",
      "Epoch:82 Loss:0.33164196567876\n",
      "Epoch:82 Loss:0.33164196567876 Accuracy:0.9076923557690212 Loss:0.09011871367692947 Accuracy:0.9791209101676941\n",
      "Epoch:83 Loss:0.33181943801733166\n",
      "Epoch:83 Loss:0.33181943801733166 Accuracy:0.9077346095672021 Loss:0.0862387865781784 Accuracy:0.9807692766189575\n",
      "Epoch:84 Loss:0.39096052646636964\n",
      "Epoch:84 Loss:0.39096052646636964 Accuracy:0.9040420730908711 Loss:0.08764965087175369 Accuracy:0.9807692766189575\n",
      "Epoch:85 Loss:0.3378049891728621\n",
      "Epoch:85 Loss:0.3378049891728621 Accuracy:0.9050507591320918 Loss:0.08896808326244354 Accuracy:0.9818681478500366\n",
      "Epoch:86 Loss:0.3244690321958982\n",
      "Epoch:86 Loss:0.3244690321958982 Accuracy:0.9079670722667987 Loss:0.08705081045627594 Accuracy:0.9807692766189575\n",
      "Epoch:87 Loss:0.3191169821299039\n",
      "Epoch:87 Loss:0.3191169821299039 Accuracy:0.9103550681701074 Loss:0.08428464084863663 Accuracy:0.9815934300422668\n",
      "Epoch:88 Loss:0.3239635618833395\n",
      "Epoch:88 Loss:0.3239635618833395 Accuracy:0.9115173679131728 Loss:0.08954552561044693 Accuracy:0.9813187122344971\n",
      "Epoch:89 Loss:0.31028406895123994\n",
      "Epoch:89 Loss:0.31028406895123994 Accuracy:0.9133347914769099 Loss:0.084260493516922 Accuracy:0.9804945588111877\n",
      "Epoch:90 Loss:0.2987752831899203\n",
      "Epoch:90 Loss:0.2987752831899203 Accuracy:0.9156805185171274 Loss:0.07974571734666824 Accuracy:0.9829670786857605\n",
      "Epoch:91 Loss:0.2987284820813399\n",
      "Epoch:91 Loss:0.2987284820813399 Accuracy:0.9152367252569932 Loss:0.07768414914608002 Accuracy:0.9818681478500366\n",
      "Epoch:92 Loss:0.2922545327590062\n",
      "Epoch:92 Loss:0.2922545327590062 Accuracy:0.9185334260647113 Loss:0.07676754891872406 Accuracy:0.9821429252624512\n",
      "Epoch:93 Loss:0.29939992840473467\n",
      "Epoch:93 Loss:0.29939992840473467 Accuracy:0.914961998279278 Loss:0.07545112073421478 Accuracy:0.9835165143013\n",
      "Epoch:94 Loss:0.28601351036475253\n",
      "Epoch:94 Loss:0.28601351036475253 Accuracy:0.9186179592059209 Loss:0.07412726432085037 Accuracy:0.9832417964935303\n",
      "Epoch:95 Loss:0.28741980131183353\n",
      "Epoch:95 Loss:0.28741980131183353 Accuracy:0.9196087632860456 Loss:0.0738145187497139 Accuracy:0.9835165143013\n",
      "Epoch:96 Loss:0.2979913904116704\n",
      "Epoch:96 Loss:0.2979913904116704 Accuracy:0.916039778636052 Loss:0.07439551502466202 Accuracy:0.9840659499168396\n",
      "Epoch:97 Loss:0.27348631964280057\n",
      "Epoch:97 Loss:0.27348631964280057 Accuracy:0.9227388409467844 Loss:0.07173535972833633 Accuracy:0.9837912321090698\n",
      "Epoch:98 Loss:0.28782472014427185\n",
      "Epoch:98 Loss:0.28782472014427185 Accuracy:0.9193787391369159 Loss:0.07130923122167587 Accuracy:0.9851648807525635\n",
      "Epoch:99 Loss:0.2737015669162457\n",
      "Epoch:99 Loss:0.2737015669162457 Accuracy:0.9241675963768592 Loss:0.0718485414981842 Accuracy:0.9846154451370239\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(train_tokens,valid_tokens,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.314820296976801, 1.0735812258683575)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(0.2737),np.exp(0.071)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),f'{DATAPATH}/inter/varybptt_model_state_dict_unfreeze')\n",
    "torch.save(optimizer.state_dict(),f'{DATAPATH}/inter/varybptt_learner_state_dict_unfreeze')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save (model,f'{DATAPATH}/inter/varybptt_model_awd_lstm')\n",
    "torch.save (optimizer,f'{DATAPATH}/inter/varybptt_optimizer_awd_lstm')\n",
    "torch.save (learner,f'{DATAPATH}/inter/varybptt_learner_awd_lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_lm_weights=model.encoder.weight.data.cpu().numpy()\n",
    "import pickle\n",
    "pickle.dump(pretrained_lm_weights,open(f'{DATAPATH}/inter/varybpttpretrained_lm_weights','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
