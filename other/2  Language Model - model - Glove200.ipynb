{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import os \n",
    "import fastai\n",
    "import fastai\n",
    "from fastai.text import *\n",
    "PATH=\"/home/kirana/Documents/phd/final\"\n",
    "DATAPATH=\"/home/kirana/Documents/phd/data/aclImdb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "[df_train,df_valid,df_test,itos, train_tokens, valid_tokens, test_tokens, trn_lm, val_lm, test_lm]=pickle.load(open(f'{PATH}/inter/dfs_tokens_fastai.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=52 # 52 - Jeremey, 20 - default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt= 70 #70 - Jeremey, 35 - default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_lm(tokens,bs):\n",
    "    import itertools\n",
    "    # Collapse into a single large array\n",
    "    tokens=np.asarray(list (itertools.chain(*tokens)))\n",
    "    # How many batches\n",
    "    n_batch=len(tokens)//bs\n",
    "    # Truncate to exclude the ones at the end\n",
    "    tokens=tokens[:bs*n_batch]\n",
    "    # Reshape\n",
    "    tokens=tokens.reshape(bs,-1)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52, 396650), (52, 43895), (52, 143259))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens=create_data_lm(df_train['tokens'],bs)\n",
    "valid_tokens=create_data_lm(df_valid['tokens'],bs)\n",
    "test_tokens=create_data_lm(df_test['tokens'],bs)\n",
    "train_tokens.shape, valid_tokens.shape, test_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i):\n",
    "    seq_len = min(args.bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52, 70) (52, 70)\n"
     ]
    }
   ],
   "source": [
    "n_batch=train_tokens.shape[1]\n",
    "for i in range(0,n_batch,bptt):\n",
    "    seq_len=min(bptt,n_batch-1-i)\n",
    "    x=train_tokens[:,i:i+seq_len]\n",
    "    y=train_tokens[:,i+1:i+1+seq_len]\n",
    "    print (x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52, 70), (52, 70))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   41,    42,    43,    40, ...,     2,  1896,   463,     4],\n",
       "        [ 1071,    23,    67,    39, ...,     2,    98,    11,    63],\n",
       "        [   54,    34,     7,   190, ...,     2,    87,    34,  5998],\n",
       "        [    7,  1213,     8,   453, ...,   406,     4,     2,    46],\n",
       "        ...,\n",
       "        [   60,   126,   931,     6, ...,   114,    39,   599,    63],\n",
       "        [    2, 15094,   132,    65, ...,    56,  1350,   209,   281],\n",
       "        [11783,   658,   318,     3, ...,     6,    64,    79,  1268],\n",
       "        [   42,    43,    40,     2, ...,   708,   111,     4,     2]]),\n",
       " array([[   42,    43,    40,    13, ...,  1896,   463,     4,     2],\n",
       "        [   23,    67,    39,    18, ...,    98,    11,    63,    28],\n",
       "        [   34,     7,   190,  1732, ...,    87,    34,  5998,    47],\n",
       "        [ 1213,     8,   453,    21, ...,     4,     2,    46,     7],\n",
       "        ...,\n",
       "        [  126,   931,     6,   126, ...,    39,   599,    63,   314],\n",
       "        [15094,   132,    65,   411, ...,  1350,   209,   281,     9],\n",
       "        [  658,   318,     3,    25, ...,    64,    79,  1268,    85],\n",
       "        [   43,    40,     2,    14, ...,   111,     4,     2, 13447]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52, 70), (52, 70))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 396650)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-trained GLOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_GLOVE='/home/kirana/Documents/phd/data/pre-trained/glove/glove.6B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;31mglove_50_glove_100.tgz\u001b[0m  glove.6B.300d.txt  wordvectors_glove50\r\n",
      "glove.6B.100d.txt       glove.6B.50d.txt\r\n",
      "glove.6B.200d.txt       word2idx_glove50\r\n"
     ]
    }
   ],
   "source": [
    "ls {PATH_GLOVE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vectors(path=f'{PATH_GLOVE}/glove.6B.50d.txt'):\n",
    "    f=open(f'{path}','r')\n",
    "    glovelines=f.readlines()\n",
    "    word2idx={}\n",
    "    idx=0\n",
    "    wordvectors={}\n",
    "    for line in glovelines:\n",
    "        line=line.split(' ')\n",
    "        vector=np.asarray(line[1:]).astype(np.float32)\n",
    "        word2idx[line[0]]=idx\n",
    "        idx=idx+1\n",
    "        wordvectors[line[0]]=vector\n",
    "    return word2idx,wordvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx_glove,wordvectors_glove=load_glove_vectors(f'{PATH_GLOVE}/glove.6B.200d.txt')\n",
    "import pickle\n",
    "pickle.dump(word2idx_glove,open(f'{PATH_GLOVE}/word2idx_glove200','wb'))\n",
    "pickle.dump(wordvectors_glove,open(f'{PATH_GLOVE}/wordvectors_glove200','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx_glove=pickle.load(open(f'{PATH_GLOVE}/word2idx_glove200','rb'))\n",
    "wordvectors_glove=pickle.load(open(f'{PATH_GLOVE}/wordvectors_glove200','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2idx_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', ',', '.', 'of', 'to']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[*word2idx_glove.keys()][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors_glove['the'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_mtx=np.array([wordvectors_glove[key] for key in [*word2idx_glove.keys()]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 200)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_mtx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5072"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In our list but not in Glove\n",
    "len(set(itos).difference(set([*word2idx_glove.keys()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0845728006402988"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(itos).difference(set([*word2idx_glove.keys()])))/len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_mtx=np.zeros((len(itos),glove_mtx.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.071549,  0.093459,  0.023738, -0.090339, ..., -0.008877,  0.33617 ,  0.030591,  0.25577 ], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors_glove['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_wgts = glove_mtx # converts np.ndarray from torch.FloatTensor.output shape: (238462, 400)\n",
    "row_m = enc_wgts.mean(0) # returns the average of the array elements along axis 0. output shape: (400,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,word in enumerate(itos):\n",
    "    try:\n",
    "        glove_mtx[i]=wordvectors_glove[word]\n",
    "    except:\n",
    "        glove_mtx[i]=row_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.071549,  0.093459,  0.023738, -0.090339, ..., -0.008877,  0.33617 ,  0.030591,  0.25577 ])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_mtx[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59972, 200), 59972)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_wgts.shape, len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_emb=enc_wgts.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_w=enc_wgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59972, 200)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from adaptive import *\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inp=len(itos)\n",
    "n_emb=200 #650\n",
    "n_hidden=200 #650\n",
    "n_layers=2\n",
    "dropout=0.5\n",
    "wd=1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class language_model (nn.Module):\n",
    "    def __init__(self,n_inp,n_emb,n_hidden,n_layers,bidirectional,bs,device,dropout_e=0.05,dropout=0.5,\\\n",
    "                 dropout_o=0.5,pretrain_mtx=None,adaptive_log_softmax=True,tie_weights=True):\n",
    "        super().__init__()\n",
    "        self.n_inp,self.n_emb,self.n_hidden,self.n_layers,self.bidirectional,self.bs,self.device,self.pretrain_mtx=\\\n",
    "                            n_inp,n_emb,n_hidden,n_layers,bidirectional,bs,device,pretrain_mtx\n",
    "        self.adaptive_log_softmax,self.tie_weights=adaptive_log_softmax,tie_weights\n",
    "        self.dropout_e,self.dropout,self.dropout_o=dropout_e,dropout,dropout_o\n",
    "        self.gen_hidden()\n",
    "        self.create_architecture()\n",
    "        if pretrain_mtx is not None:\n",
    "            print (\"initializing\")\n",
    "            self.initialize_glove()\n",
    "            \n",
    "        if self.adaptive_log_softmax is False:\n",
    "            self.criterion=nn.CrossEntropyLoss()\n",
    "        \n",
    "    def create_architecture(self):\n",
    "        # Dropout layer\n",
    "        self.dropout_enc=nn.Dropout(self.dropout_e)\n",
    "        # Embedding Layer\n",
    "        self.encoder=nn.Embedding(self.n_inp,self.n_emb)\n",
    "        # LSTM Layer\n",
    "        self.lstm=nn.LSTM(self.n_emb,self.n_hidden,self.n_layers,batch_first=True,dropout=self.dropout,\\\n",
    "                          bidirectional=False)\n",
    "        self.dropout_op=nn.Dropout(self.dropout_o)\n",
    "        \n",
    "        if self.adaptive_log_softmax:\n",
    "            # Adaptive Log Softmax Loss\n",
    "            self.adaptive_softmax=AdaptiveLogSoftmaxWithLoss(self.n_hidden,\n",
    "                                    self.n_inp,\n",
    "                                    cutoffs=[round(self.n_inp/15),3*round(self.n_inp/15)],\n",
    "                                    div_value=4,\n",
    "                                    get_full_prob=True)\n",
    "        else:\n",
    "            self.decoder=nn.Linear(self.n_hidden,self.n_inp)\n",
    "    \n",
    "    def freeze_embedding(self):\n",
    "        self.encoder.weight.requires_grad=False\n",
    "        if self.tie_weights:\n",
    "            self.decoder.weight.requires_grad=False\n",
    "        \n",
    "    \n",
    "    def unfreeze_embedding(self):\n",
    "        self.encoder.weight.requires_grad=True\n",
    "        if self.tie_weights:\n",
    "            self.decoder.weight.requires_grad=True\n",
    "        \n",
    "    def initialize_glove(self):\n",
    "        self.encoder.weight.data=torch.Tensor(self.pretrain_mtx)\n",
    "        if self.tie_weights:\n",
    "            self.decoder.weight=self.encoder.weight\n",
    "    \n",
    "    def gen_hidden(self):\n",
    "        # Initialize hidden\n",
    "        self.hidden=(Variable(torch.zeros(self.n_layers,self.bs,self.n_hidden,requires_grad=False).to(self.device)),\n",
    "                     Variable(torch.zeros(self.n_layers,self.bs,self.n_hidden,requires_grad=False).to(self.device)))\n",
    "    \n",
    "        \n",
    "    def forward(self,Xb,Yb):\n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        embs=self.dropout_enc(self.encoder(Xb))\n",
    "        if Xb.size(0) < self.bs:\n",
    "            self.hidden=(self.hidden[0][:,:Xb.size(0),:].contiguous(),\n",
    "            self.hidden[1][:,:Xb.size(0),:].contiguous())\n",
    "        out,new_hidden=self.lstm(embs,self.hidden)\n",
    "        out=self.dropout_op(out)\n",
    "         # Wrap the hidden state in a new tensor without the gradients\n",
    "        self.hidden=(Variable(new_hidden[0].data,requires_grad=False).to(self.device),\\\n",
    "                     Variable(new_hidden[1].data,requires_grad=False).to(self.device))\n",
    "        if self.adaptive_log_softmax:\n",
    "            out=out.reshape(out.size(0)*out.size(1),out.size(2))        # output is of shape n_batch * n_seq * n_hidden\n",
    "      \n",
    "            out=self.adaptive_softmax(out,Yb.view(-1))\n",
    "            loss=out.loss\n",
    "            preds=out.output_full\n",
    "        else:\n",
    "            #import pdb\n",
    "            #pdb.set_trace()\n",
    "            preds=self.decoder(out.contiguous().view(out.size(0)*out.size(1), out.size(2)))\n",
    "            loss=self.criterion(preds,Yb.contiguous().view(-1))\n",
    "\n",
    "        return preds, loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_multinomial(preds,actual):\n",
    "    preds=preds.max(1)[1]\n",
    "    correct=preds==actual\n",
    "    return correct.float().sum()/len(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing\n"
     ]
    }
   ],
   "source": [
    "device=\"cuda:1\"\n",
    "model=language_model(n_inp,n_emb,n_hidden,n_layers,False,bs,device,0.05,0.5,0.5,new_w,False,True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59972, 200)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([59972, 200]), torch.Size([59972, 200]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.weight.data.shape,model.decoder.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([59972, 200]), 200, 59972)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Linear(n_hidden,n_inp).weight.data.shape, n_hidden, n_inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test if model forward works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([59972, 200]), torch.Size([59972, 200]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder.weight.shape,model.encoder.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 70)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1==0:\n",
    "    model.forward(torch.LongTensor(x),torch.LongTensor(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self,model,optimizer,metric_fn,device,bptt=12,print_every=5,clip_val=None):\n",
    "        self.model,self.optimizer,self.metric_fn,self.device,self.print_every,self.bptt,self.losses,self.clip_val=\\\n",
    "            model,optimizer,metric_fn,device,print_every,bptt,[],clip_val\n",
    "        self.n_epochs=1\n",
    "  \n",
    "        \n",
    "    \n",
    "    def fit (self,Xb,Yb,mode_train=True):\n",
    "        if mode_train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            \n",
    "        preds,loss=self.model(Xb,Yb)\n",
    "        \n",
    "       \n",
    "            \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            acc=self.metric_fn(preds,Yb.view(-1))\n",
    "            acc=acc.item()\n",
    "            del preds\n",
    "        \n",
    "        if mode_train:\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        myloss=loss.item()\n",
    "        del loss\n",
    "        \n",
    "        if self.clip_val is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.25)\n",
    "            if 1==0:\n",
    "                for p in self.model.parameters():\n",
    "                    p.data.add_(self.lr, p.grad.data)\n",
    "        \n",
    "        return myloss, acc\n",
    "    \n",
    "    def lr_find (self,start_lr,end_lr,iterator,n_batch):\n",
    "        losses,lrs=[],[]\n",
    "        ratio=end_lr/start_lr\n",
    "        num_steps=n_batch\n",
    "        lr=start_lr\n",
    "        for i in range(num_steps):            \n",
    "            lr=lr*(end_lr/start_lr)**(1/num_steps)\n",
    "            lrs.append(lr)\n",
    "        self.lrs=lrs\n",
    "        self.run_epoch(iterator,mode_train=True,lrs=lrs)\n",
    "    \n",
    "    def run_epoch(self,iterator,mode_train,lrs=None):\n",
    "        n_batch=iterator.shape[1]\n",
    "        epoch_loss,epoch_acc,i=0,0,0\n",
    "        self.model.gen_hidden()\n",
    "        for k,i in enumerate(range(0,n_batch,self.bptt)):\n",
    "            seq_len=min(bptt,n_batch-1-i)\n",
    "            Xb=train_tokens[:,i:i+seq_len]\n",
    "            Yb=train_tokens[:,i+1:i+1+seq_len]\n",
    "            Xb=torch.LongTensor(Xb)\n",
    "            Yb=torch.LongTensor(Yb)\n",
    "            Xb=Xb.to(self.device)\n",
    "            Yb=Yb.to(self.device)\n",
    "            \n",
    "            if lrs is not None:\n",
    "                lr=lrs[k]\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr']=lr \n",
    "            \n",
    "\n",
    "            loss,acc=self.fit(Xb,Yb,mode_train)\n",
    "            \n",
    "            if lrs is not None:\n",
    "                self.losses.append(loss)\n",
    "            \n",
    "            \n",
    "            epoch_loss+=loss\n",
    "            epoch_acc+=acc\n",
    "            if k%self.print_every == 0:\n",
    "                if k:\n",
    "                    print (f'Batch:{k} {epoch_loss/(k)}  {epoch_acc/(k)}')  \n",
    "                    torch.cuda.empty_cache()\n",
    "        epoch_loss=epoch_loss/k\n",
    "        epoch_acc=epoch_acc/k\n",
    "        \n",
    "        if 1==0:\n",
    "            lr /= 4.0\n",
    "            # Freeze all the layers initially\n",
    "            for param in resnet.parameters():\n",
    "                param.requires_grad=False\n",
    "            torch.save(resnet,'resnet')\n",
    "            torch.save(resnet.state_dict(),'resnet_state_dict')\n",
    "            resnet.load_state_dict(torch.load('resnet_state_dict'))\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr']=lr  \n",
    "            \n",
    "        return epoch_loss,epoch_acc\n",
    "    \n",
    "    def plot_lrs(self, n_roll=1):\n",
    "        import seaborn as sns\n",
    "        ax=sns.lineplot(x=self.lrs,y=pd.Series(self.losses).rolling(n_roll).mean())\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.set_xlabel('Learning Rate')\n",
    "\n",
    "    \n",
    "    def run_epochs(self,dltrain,dlvalid,n_epochs=1):\n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            loss,acc=self.run_epoch(dltrain,True)\n",
    "            print (f'Epoch:{epoch} Loss:{loss}')\n",
    "            lossv,accv=self.run_epoch(dlvalid,mode_train=False)\n",
    "            print (f'Epoch:{epoch} Loss:{loss} Accuracy:{acc} Loss:{lossv} Accuracy:{accv}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(model.parameters(),lr=1e-3,betas=(0.9,0.999), weight_decay=wd)\n",
    "metric_fn=accuracy_multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5667"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_batch=np.int(np.ceil(train_tokens.shape[1]/bptt))\n",
    "n_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.weight.requires_grad, model.decoder.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner=Learner(model,optimizer,accuracy_multinomial,device,bptt,500,0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59972"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:500 7.8526061296463014  0.048273628686089066\n",
      "Batch:1000 7.568344240665436  0.0656211562550161\n",
      "Batch:1500 7.385362540880839  0.07556172394395495\n",
      "Batch:2000 7.243246864318848  0.08274395888776052\n",
      "Batch:2500 7.12381884689331  0.08824395966613666\n",
      "Batch:3000 7.020513757864634  0.09280357567539128\n",
      "Batch:3500 6.928043052945818  0.09667009880985798\n",
      "Batch:4000 6.844364072918892  0.10002301305398578\n",
      "Batch:4500 6.768656057569716  0.10318645177000306\n",
      "Batch:5000 6.699413502311707  0.10720423592370935\n",
      "Batch:5500 6.634127405773509  0.11119016533315351\n",
      "Epoch:0 Loss:6.6136987492089725\n",
      "Batch:500 5.621094839096069  0.17891704100370406\n",
      "Epoch:0 Loss:6.6136987492089725 Accuracy:0.11241941579562167 Loss:5.619333422355104 Accuracy:0.17869302549620755\n",
      "Batch:500 5.8941029958724975  0.15902033844590188\n",
      "Batch:1000 5.858174221038818  0.16109726160764695\n",
      "Batch:1500 5.829671250979105  0.16322107116381326\n",
      "Batch:2000 5.802444364070892  0.16539520118385553\n",
      "Batch:2500 5.778238586616516  0.16717209668159486\n",
      "Batch:3000 5.756934444904328  0.16867171196639538\n",
      "Batch:3500 5.737902788298471  0.1699759112426213\n",
      "Batch:4000 5.721627427220344  0.17095625849440693\n",
      "Batch:4500 5.707674700948927  0.17175367147392698\n",
      "Batch:5000 5.696355530643463  0.17234325019419194\n",
      "Batch:5500 5.685530253583734  0.17290874964540656\n",
      "Epoch:1 Loss:5.682396058097056\n",
      "Batch:500 5.290620901107788  0.2037390188872814\n",
      "Epoch:1 Loss:5.682396058097056 Accuracy:0.1730769364668883 Loss:5.28953874624517 Accuracy:0.20351469492988344\n",
      "Batch:500 5.582786861419677  0.17924121701717377\n",
      "Batch:1000 5.5759108853340145  0.17894423878192903\n",
      "Batch:1500 5.574851783116658  0.17877949505050977\n",
      "Batch:2000 5.571453500747681  0.17900096942484378\n",
      "Batch:2500 5.56864702091217  0.17911374419927598\n",
      "Batch:3000 5.566207444985707  0.17925788337488968\n",
      "Batch:3500 5.563358136040824  0.17948760601878166\n",
      "Batch:4000 5.561034183263779  0.1796570133678615\n",
      "Batch:4500 5.558715523295932  0.17986533143785266\n",
      "Batch:5000 5.55725411567688  0.18001215073168278\n",
      "Batch:5500 5.554831739165566  0.18024636151032014\n",
      "Epoch:2 Loss:5.554220031185534\n",
      "Batch:500 5.247057577133178  0.20861649253964423\n",
      "Epoch:2 Loss:5.554220031185534 Accuracy:0.1802993262330635 Loss:5.246158192982894 Accuracy:0.20842250746688204\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(train_tokens,valid_tokens,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:500 5.53926357460022  0.18324890884757042\n",
      "Batch:1000 5.534780735969544  0.18300989787280558\n",
      "Batch:1500 5.535018832524617  0.1827939638396104\n",
      "Batch:2000 5.532855356216431  0.18289176600426435\n",
      "Batch:2500 5.530926789855957  0.18288506276011468\n",
      "Batch:3000 5.529588640054067  0.18288892726103465\n",
      "Batch:3500 5.527603641646249  0.183052362625088\n",
      "Batch:4000 5.526070582270623  0.18319210946559905\n",
      "Batch:4500 5.524496164957682  0.18330592966079712\n",
      "Batch:5000 5.523850279712677  0.18339110670089723\n",
      "Batch:5500 5.5221725163026285  0.18357428349419075\n",
      "Epoch:0 Loss:5.521768107500086\n",
      "Batch:500 5.22363712978363  0.21065495431423187\n",
      "Epoch:0 Loss:5.521768107500086 Accuracy:0.1836117340710068 Loss:5.222834784828685 Accuracy:0.21049149149057397\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(train_tokens,valid_tokens,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),f'{PATH}/inter/glove200_model_state_dict')\n",
    "torch.save(optimizer.state_dict(),f'{PATH}/inter/glove200_learner_state_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.unfreeze_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.weight.requires_grad, model.decoder.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:500 5.312142787933349  0.20092143654823302\n",
      "Batch:1000 5.220643448829651  0.20611649253964423\n",
      "Batch:1500 5.167726959228515  0.208644331852595\n",
      "Batch:2000 5.126642462968826  0.210777894616127\n",
      "Batch:2500 5.093956485748291  0.21232418619990348\n",
      "Batch:3000 5.067064871470134  0.21367510228852432\n",
      "Batch:3500 5.043712053571428  0.21492316636017392\n",
      "Batch:4000 5.023481311321259  0.21595509352535008\n",
      "Batch:4500 5.005813073158264  0.21689677559667164\n",
      "Batch:5000 4.990340952396393  0.2177352861136198\n",
      "Batch:5500 4.9752811776074495  0.21863832317699086\n",
      "Epoch:0 Loss:4.970837374689551\n",
      "Batch:500 4.539077677726746  0.2520000110864639\n",
      "Epoch:0 Loss:4.970837374689551 Accuracy:0.21889011698889724 Loss:4.538255211649139 Accuracy:0.2518512300041874\n",
      "Batch:500 4.814705096244812  0.22838078156113625\n",
      "Batch:1000 4.804414977550507  0.22867336396872998\n",
      "Batch:1500 4.8006322940190636  0.228641953676939\n",
      "Batch:2000 4.794489390134811  0.22892899580299855\n",
      "Batch:2500 4.788668313407898  0.22904748484492302\n",
      "Batch:3000 4.78379782485962  0.22925486580530802\n",
      "Batch:3500 4.778448377881731  0.2296226182750293\n",
      "Batch:4000 4.773324790835381  0.2299180754683912\n",
      "Batch:4500 4.768548707538181  0.23021283276875815\n",
      "Batch:5000 4.7646213064193725  0.2304950672060251\n",
      "Batch:5500 4.759703346165744  0.2308722899420695\n",
      "Epoch:1 Loss:4.758384662342711\n",
      "Batch:500 4.431983956336975  0.2589423184096813\n",
      "Epoch:1 Loss:4.758384662342711 Accuracy:0.23095493007055493 Loss:4.431099322804234 Accuracy:0.25874455551211345\n",
      "Batch:500 4.714799681663513  0.23487638565897942\n",
      "Batch:1000 4.707588285923004  0.23482226476073265\n",
      "Batch:1500 4.706583679199219  0.23459250287214914\n",
      "Batch:2000 4.703127913951874  0.23478558888286352\n",
      "Batch:2500 4.699644561195374  0.23475275920033456\n",
      "Batch:3000 4.696897638638815  0.23487904121975103\n",
      "Batch:3500 4.693732672418867  0.23509153465288027\n",
      "Batch:4000 4.690560976147652  0.23526436627656222\n",
      "Batch:4500 4.6878159742355345  0.2354646638929844\n",
      "Batch:5000 4.685712371730804  0.23562819869816304\n",
      "Batch:5500 4.682495216369629  0.23589117072387175\n",
      "Epoch:2 Loss:4.681713277360301\n",
      "Batch:500 4.368095535278321  0.26396099901199344\n",
      "Epoch:2 Loss:4.681713277360301 Accuracy:0.2359469949783599 Loss:4.367342884270578 Accuracy:0.26372334781731144\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(train_tokens,valid_tokens,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),f'{PATH}/inter/glove200_model_state_dict')\n",
    "torch.save(optimizer.state_dict(),f'{PATH}/inter/glove200_learner_state_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:500 4.659603902816772  0.23899616533517837\n",
      "Batch:1000 4.6538007450103756  0.23880990178883077\n",
      "Batch:1500 4.653609954516093  0.23856576255957285\n",
      "Batch:2000 4.651165057182312  0.2385523467734456\n",
      "Batch:2500 4.648830134391785  0.2384993522465229\n",
      "Batch:3000 4.646925424098969  0.2385403046409289\n",
      "Batch:3500 4.644628941808428  0.23870409328171185\n",
      "Batch:4000 4.642290043950081  0.23883792372420432\n",
      "Batch:4500 4.640284406873915  0.23893133036295572\n",
      "Batch:5000 4.638935650253296  0.23903336329758168\n",
      "Batch:5500 4.636414554595947  0.23926025142723864\n",
      "Epoch:0 Loss:4.635876988927004\n",
      "Batch:500 4.328870507240295  0.26718572479486463\n",
      "Epoch:0 Loss:4.635876988927004 Accuracy:0.23929453880236504 Loss:4.328198654229561 Accuracy:0.26693066759733114\n",
      "Batch:500 4.621987651824951  0.24143572640419006\n",
      "Batch:1000 4.6171903052330014  0.24126347355544567\n",
      "Batch:1500 4.617815558115641  0.24099744781851767\n",
      "Batch:2000 4.616146491289139  0.24096470968425274\n",
      "Batch:2500 4.614309655380249  0.24084330854415895\n",
      "Batch:3000 4.612687684218089  0.2408733635097742\n",
      "Batch:3500 4.610920642852784  0.24106500391449248\n",
      "Batch:4000 4.609070534706116  0.24121244306862355\n",
      "Batch:4500 4.607613447931078  0.24130159904228315\n",
      "Batch:5000 4.606657708549499  0.24140336338281632\n",
      "Batch:5500 4.604679615887728  0.24159626547585833\n",
      "Epoch:1 Loss:4.604324482289297\n",
      "Batch:500 4.298688532829285  0.26952968111634257\n",
      "Epoch:1 Loss:4.604324482289297 Accuracy:0.24162143590885382 Loss:4.298095278002238 Accuracy:0.2693076765660844\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(train_tokens,valid_tokens,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49.40244910553017, 72.24044000732538)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(3.9), np.exp(4.28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),f'{PATH}/inter/glove200_model_state_dict')\n",
    "torch.save(optimizer.state_dict(),f'{PATH}/inter/glove200_learner_state_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:500 7.944759424209595  0.03532285663112998\n",
      "Batch:1000 7.877773783683777  0.03720142808556557\n",
      "Batch:1500 7.802010853131613  0.03968857092410326\n",
      "Batch:2000 7.722838351964951  0.04160571377538145\n",
      "Batch:2500 7.658665308570862  0.043098285208642485\n"
     ]
    }
   ],
   "source": [
    "if 1==0:\n",
    "    learner.lr_find(1e-4,1e-1,valid_tokens,n_batch)\n",
    "    learner.plot_lrs(50)\n",
    "    len(learner.lrs)\n",
    "    len(learner.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning to predict RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
