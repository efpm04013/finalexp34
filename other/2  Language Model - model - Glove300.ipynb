{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import os \n",
    "import fastai\n",
    "import fastai\n",
    "from fastai.text import *\n",
    "PATH=\"/home/kirana/Documents/phd/final\"\n",
    "DATAPATH=\"/home/kirana/Documents/phd/data/aclImdb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "[df_train,df_valid,df_test,itos, train_tokens, valid_tokens, test_tokens, trn_lm, val_lm, test_lm]=pickle.load(open(f'{PATH}/inter/dfs_tokens_fastai.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=52 # 52 - Jeremey, 20 - default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt= 70 #70 - Jeremey, 35 - default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_lm(tokens,bs):\n",
    "    import itertools\n",
    "    # Collapse into a single large array\n",
    "    tokens=np.asarray(list (itertools.chain(*tokens)))\n",
    "    # How many batches\n",
    "    n_batch=len(tokens)//bs\n",
    "    # Truncate to exclude the ones at the end\n",
    "    tokens=tokens[:bs*n_batch]\n",
    "    # Reshape\n",
    "    tokens=tokens.reshape(bs,-1)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52, 396650), (52, 43895), (52, 143259))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens=create_data_lm(df_train['tokens'],bs)\n",
    "valid_tokens=create_data_lm(df_valid['tokens'],bs)\n",
    "test_tokens=create_data_lm(df_test['tokens'],bs)\n",
    "train_tokens.shape, valid_tokens.shape, test_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i):\n",
    "    seq_len = min(args.bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52, 70) (52, 70)\n"
     ]
    }
   ],
   "source": [
    "n_batch=train_tokens.shape[1]\n",
    "for i in range(0,n_batch,bptt):\n",
    "    seq_len=min(bptt,n_batch-1-i)\n",
    "    x=train_tokens[:,i:i+seq_len]\n",
    "    y=train_tokens[:,i+1:i+1+seq_len]\n",
    "    print (x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52, 70), (52, 70))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   41,    42,    43,    40, ...,     2,  1896,   463,     4],\n",
       "        [ 1071,    23,    67,    39, ...,     2,    98,    11,    63],\n",
       "        [   54,    34,     7,   190, ...,     2,    87,    34,  5998],\n",
       "        [    7,  1213,     8,   453, ...,   406,     4,     2,    46],\n",
       "        ...,\n",
       "        [   60,   126,   931,     6, ...,   114,    39,   599,    63],\n",
       "        [    2, 15094,   132,    65, ...,    56,  1350,   209,   281],\n",
       "        [11783,   658,   318,     3, ...,     6,    64,    79,  1268],\n",
       "        [   42,    43,    40,     2, ...,   708,   111,     4,     2]]),\n",
       " array([[   42,    43,    40,    13, ...,  1896,   463,     4,     2],\n",
       "        [   23,    67,    39,    18, ...,    98,    11,    63,    28],\n",
       "        [   34,     7,   190,  1732, ...,    87,    34,  5998,    47],\n",
       "        [ 1213,     8,   453,    21, ...,     4,     2,    46,     7],\n",
       "        ...,\n",
       "        [  126,   931,     6,   126, ...,    39,   599,    63,   314],\n",
       "        [15094,   132,    65,   411, ...,  1350,   209,   281,     9],\n",
       "        [  658,   318,     3,    25, ...,    64,    79,  1268,    85],\n",
       "        [   43,    40,     2,    14, ...,   111,     4,     2, 13447]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52, 70), (52, 70))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 396650)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-trained GLOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_GLOVE='/home/kirana/Documents/phd/data/pre-trained/glove/glove.6B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;31mglove_50_glove_100.tgz\u001b[0m  glove.6B.300d.txt  word2idx_glove50\r\n",
      "glove.6B.100d.txt       glove.6B.50d.txt   wordvectors_glove200\r\n",
      "glove.6B.200d.txt       word2idx_glove200  wordvectors_glove50\r\n"
     ]
    }
   ],
   "source": [
    "ls {PATH_GLOVE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vectors(path=f'{PATH_GLOVE}/glove.6B.50d.txt'):\n",
    "    f=open(f'{path}','r')\n",
    "    glovelines=f.readlines()\n",
    "    word2idx={}\n",
    "    idx=0\n",
    "    wordvectors={}\n",
    "    for line in glovelines:\n",
    "        line=line.split(' ')\n",
    "        vector=np.asarray(line[1:]).astype(np.float32)\n",
    "        word2idx[line[0]]=idx\n",
    "        idx=idx+1\n",
    "        wordvectors[line[0]]=vector\n",
    "    return word2idx,wordvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx_glove,wordvectors_glove=load_glove_vectors(f'{PATH_GLOVE}/glove.6B.300d.txt')\n",
    "import pickle\n",
    "pickle.dump(word2idx_glove,open(f'{PATH_GLOVE}/word2idx_glove300','wb'))\n",
    "pickle.dump(wordvectors_glove,open(f'{PATH_GLOVE}/wordvectors_glove300','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx_glove=pickle.load(open(f'{PATH_GLOVE}/word2idx_glove300','rb'))\n",
    "wordvectors_glove=pickle.load(open(f'{PATH_GLOVE}/wordvectors_glove300','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2idx_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', ',', '.', 'of', 'to']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[*word2idx_glove.keys()][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors_glove['the'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_mtx=np.array([wordvectors_glove[key] for key in [*word2idx_glove.keys()]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 300)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_mtx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5072"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In our list but not in Glove\n",
    "len(set(itos).difference(set([*word2idx_glove.keys()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0845728006402988"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(itos).difference(set([*word2idx_glove.keys()])))/len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_mtx=np.zeros((len(itos),glove_mtx.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04656 ,  0.21318 , -0.007436, -0.45854 , ..., -0.087461,  0.009061, -0.20989 ,  0.053913], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors_glove['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_wgts = glove_mtx # converts np.ndarray from torch.FloatTensor.output shape: (238462, 400)\n",
    "row_m = enc_wgts.mean(0) # returns the average of the array elements along axis 0. output shape: (400,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,word in enumerate(itos):\n",
    "    try:\n",
    "        glove_mtx[i]=wordvectors_glove[word]\n",
    "    except:\n",
    "        glove_mtx[i]=row_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04656 ,  0.21318 , -0.007436, -0.45854 , ..., -0.087461,  0.009061, -0.20989 ,  0.053913])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_mtx[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59972, 300), 59972)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_wgts.shape, len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_emb=enc_wgts.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_w=enc_wgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59972, 300)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from adaptive import *\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inp=len(itos)\n",
    "n_emb=300 #650\n",
    "n_hidden=300 #650\n",
    "n_layers=2\n",
    "dropout=0.5\n",
    "wd=1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class language_model (nn.Module):\n",
    "    def __init__(self,n_inp,n_emb,n_hidden,n_layers,bidirectional,bs,device,dropout_e=0.05,dropout=0.5,\\\n",
    "                 dropout_o=0.5,pretrain_mtx=None,adaptive_log_softmax=True,tie_weights=True):\n",
    "        super().__init__()\n",
    "        self.n_inp,self.n_emb,self.n_hidden,self.n_layers,self.bidirectional,self.bs,self.device,self.pretrain_mtx=\\\n",
    "                            n_inp,n_emb,n_hidden,n_layers,bidirectional,bs,device,pretrain_mtx\n",
    "        self.adaptive_log_softmax,self.tie_weights=adaptive_log_softmax,tie_weights\n",
    "        self.dropout_e,self.dropout,self.dropout_o=dropout_e,dropout,dropout_o\n",
    "        self.gen_hidden()\n",
    "        self.create_architecture()\n",
    "        if pretrain_mtx is not None:\n",
    "            print (\"initializing\")\n",
    "            self.initialize_glove()\n",
    "            \n",
    "        if self.adaptive_log_softmax is False:\n",
    "            self.criterion=nn.CrossEntropyLoss()\n",
    "        \n",
    "    def create_architecture(self):\n",
    "        # Dropout layer\n",
    "        self.dropout_enc=nn.Dropout(self.dropout_e)\n",
    "        # Embedding Layer\n",
    "        self.encoder=nn.Embedding(self.n_inp,self.n_emb)\n",
    "        # LSTM Layer\n",
    "        self.lstm=nn.LSTM(self.n_emb,self.n_hidden,self.n_layers,batch_first=True,dropout=self.dropout,\\\n",
    "                          bidirectional=False)\n",
    "        self.dropout_op=nn.Dropout(self.dropout_o)\n",
    "        \n",
    "        if self.adaptive_log_softmax:\n",
    "            # Adaptive Log Softmax Loss\n",
    "            self.adaptive_softmax=AdaptiveLogSoftmaxWithLoss(self.n_hidden,\n",
    "                                    self.n_inp,\n",
    "                                    cutoffs=[round(self.n_inp/15),3*round(self.n_inp/15)],\n",
    "                                    div_value=4,\n",
    "                                    get_full_prob=True)\n",
    "        else:\n",
    "            self.decoder=nn.Linear(self.n_hidden,self.n_inp)\n",
    "    \n",
    "    def freeze_embedding(self):\n",
    "        self.encoder.weight.requires_grad=False\n",
    "        if self.tie_weights:\n",
    "            self.decoder.weight.requires_grad=False\n",
    "        \n",
    "    \n",
    "    def unfreeze_embedding(self):\n",
    "        self.encoder.weight.requires_grad=True\n",
    "        if self.tie_weights:\n",
    "            self.decoder.weight.requires_grad=True\n",
    "        \n",
    "    def initialize_glove(self):\n",
    "        self.encoder.weight.data=torch.Tensor(self.pretrain_mtx)\n",
    "        if self.tie_weights:\n",
    "            self.decoder.weight=self.encoder.weight\n",
    "    \n",
    "    def gen_hidden(self):\n",
    "        # Initialize hidden\n",
    "        self.hidden=(Variable(torch.zeros(self.n_layers,self.bs,self.n_hidden,requires_grad=False).to(self.device)),\n",
    "                     Variable(torch.zeros(self.n_layers,self.bs,self.n_hidden,requires_grad=False).to(self.device)))\n",
    "    \n",
    "        \n",
    "    def forward(self,Xb,Yb):\n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        embs=self.dropout_enc(self.encoder(Xb))\n",
    "        if Xb.size(0) < self.bs:\n",
    "            self.hidden=(self.hidden[0][:,:Xb.size(0),:].contiguous(),\n",
    "            self.hidden[1][:,:Xb.size(0),:].contiguous())\n",
    "        out,new_hidden=self.lstm(embs,self.hidden)\n",
    "        out=self.dropout_op(out)\n",
    "         # Wrap the hidden state in a new tensor without the gradients\n",
    "        self.hidden=(Variable(new_hidden[0].data,requires_grad=False).to(self.device),\\\n",
    "                     Variable(new_hidden[1].data,requires_grad=False).to(self.device))\n",
    "        if self.adaptive_log_softmax:\n",
    "            out=out.reshape(out.size(0)*out.size(1),out.size(2))        # output is of shape n_batch * n_seq * n_hidden\n",
    "      \n",
    "            out=self.adaptive_softmax(out,Yb.view(-1))\n",
    "            loss=out.loss\n",
    "            preds=out.output_full\n",
    "        else:\n",
    "            #import pdb\n",
    "            #pdb.set_trace()\n",
    "            preds=self.decoder(out.contiguous().view(out.size(0)*out.size(1), out.size(2)))\n",
    "            loss=self.criterion(preds,Yb.contiguous().view(-1))\n",
    "\n",
    "        return preds, loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_multinomial(preds,actual):\n",
    "    preds=preds.max(1)[1]\n",
    "    correct=preds==actual\n",
    "    return correct.float().sum()/len(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing\n"
     ]
    }
   ],
   "source": [
    "device=\"cuda:0\"\n",
    "model=language_model(n_inp,n_emb,n_hidden,n_layers,False,bs,device,0.05,0.5,0.5,new_w,False,True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59972, 300)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([59972, 300]), torch.Size([59972, 300]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.weight.data.shape,model.decoder.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([59972, 300]), 300, 59972)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Linear(n_hidden,n_inp).weight.data.shape, n_hidden, n_inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test if model forward works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([59972, 300]), torch.Size([59972, 300]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder.weight.shape,model.encoder.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 70)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1==0:\n",
    "    model.forward(torch.LongTensor(x),torch.LongTensor(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self,model,optimizer,metric_fn,device,bptt=12,print_every=5,clip_val=None):\n",
    "        self.model,self.optimizer,self.metric_fn,self.device,self.print_every,self.bptt,self.losses,self.clip_val=\\\n",
    "            model,optimizer,metric_fn,device,print_every,bptt,[],clip_val\n",
    "        self.n_epochs=1\n",
    "  \n",
    "        \n",
    "    \n",
    "    def fit (self,Xb,Yb,mode_train=True):\n",
    "        if mode_train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            \n",
    "        preds,loss=self.model(Xb,Yb)\n",
    "        \n",
    "       \n",
    "            \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            acc=self.metric_fn(preds,Yb.view(-1))\n",
    "            acc=acc.item()\n",
    "            del preds\n",
    "        \n",
    "        if mode_train:\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        myloss=loss.item()\n",
    "        del loss\n",
    "        \n",
    "        if self.clip_val is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.25)\n",
    "            if 1==0:\n",
    "                for p in self.model.parameters():\n",
    "                    p.data.add_(self.lr, p.grad.data)\n",
    "        \n",
    "        return myloss, acc\n",
    "    \n",
    "    def lr_find (self,start_lr,end_lr,iterator,n_batch):\n",
    "        losses,lrs=[],[]\n",
    "        ratio=end_lr/start_lr\n",
    "        num_steps=n_batch\n",
    "        lr=start_lr\n",
    "        for i in range(num_steps):            \n",
    "            lr=lr*(end_lr/start_lr)**(1/num_steps)\n",
    "            lrs.append(lr)\n",
    "        self.lrs=lrs\n",
    "        self.run_epoch(iterator,mode_train=True,lrs=lrs)\n",
    "    \n",
    "    def run_epoch(self,iterator,mode_train,lrs=None):\n",
    "        n_batch=iterator.shape[1]\n",
    "        epoch_loss,epoch_acc,i=0,0,0\n",
    "        self.model.gen_hidden()\n",
    "        for k,i in enumerate(range(0,n_batch,self.bptt)):\n",
    "            seq_len=min(bptt,n_batch-1-i)\n",
    "            Xb=train_tokens[:,i:i+seq_len]\n",
    "            Yb=train_tokens[:,i+1:i+1+seq_len]\n",
    "            Xb=torch.LongTensor(Xb)\n",
    "            Yb=torch.LongTensor(Yb)\n",
    "            Xb=Xb.to(self.device)\n",
    "            Yb=Yb.to(self.device)\n",
    "            \n",
    "            if lrs is not None:\n",
    "                lr=lrs[k]\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr']=lr \n",
    "            \n",
    "\n",
    "            loss,acc=self.fit(Xb,Yb,mode_train)\n",
    "            \n",
    "            if lrs is not None:\n",
    "                self.losses.append(loss)\n",
    "            \n",
    "            \n",
    "            epoch_loss+=loss\n",
    "            epoch_acc+=acc\n",
    "            if k%self.print_every == 0:\n",
    "                if k:\n",
    "                    print (f'Batch:{k} {epoch_loss/(k)}  {epoch_acc/(k)}')  \n",
    "                    torch.cuda.empty_cache()\n",
    "        epoch_loss=epoch_loss/k\n",
    "        epoch_acc=epoch_acc/k\n",
    "        \n",
    "        if 1==0:\n",
    "            lr /= 4.0\n",
    "            # Freeze all the layers initially\n",
    "            for param in resnet.parameters():\n",
    "                param.requires_grad=False\n",
    "            torch.save(resnet,'resnet')\n",
    "            torch.save(resnet.state_dict(),'resnet_state_dict')\n",
    "            resnet.load_state_dict(torch.load('resnet_state_dict'))\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr']=lr  \n",
    "            \n",
    "        return epoch_loss,epoch_acc\n",
    "    \n",
    "    def plot_lrs(self, n_roll=1):\n",
    "        import seaborn as sns\n",
    "        ax=sns.lineplot(x=self.lrs,y=pd.Series(self.losses).rolling(n_roll).mean())\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.set_xlabel('Learning Rate')\n",
    "\n",
    "    \n",
    "    def run_epochs(self,dltrain,dlvalid,n_epochs=1):\n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            loss,acc=self.run_epoch(dltrain,True)\n",
    "            print (f'Epoch:{epoch} Loss:{loss}')\n",
    "            lossv,accv=self.run_epoch(dlvalid,mode_train=False)\n",
    "            print (f'Epoch:{epoch} Loss:{loss} Accuracy:{acc} Loss:{lossv} Accuracy:{accv}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(model.parameters(),lr=1e-3,betas=(0.9,0.999), weight_decay=wd)\n",
    "metric_fn=accuracy_multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5667"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_batch=np.int(np.ceil(train_tokens.shape[1]/bptt))\n",
    "n_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.weight.requires_grad, model.decoder.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner=Learner(model,optimizer,accuracy_multinomial,device,bptt,500,0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59972"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:500 7.650373255729675  0.0643522001367528\n",
      "Batch:1000 7.355331929206848  0.08340714605210815\n",
      "Batch:1500 7.16919517103831  0.09369890569189253\n",
      "Batch:2000 7.026159649848938  0.10086923582880991\n",
      "Batch:2500 6.907291049194336  0.10625385119938291\n",
      "Batch:3000 6.804987421353658  0.11059451041638385\n",
      "Batch:3500 6.713873579570225  0.11428846648352088\n",
      "Batch:4000 6.631647217273712  0.11763661232418962\n",
      "Batch:4500 6.557406237072414  0.12086105564084007\n",
      "Batch:5000 6.489825654697418  0.12414945648175199\n",
      "Batch:5500 6.426086421272972  0.12767707912905396\n",
      "Epoch:0 Loss:6.406246617452188\n",
      "Batch:500 5.427806157112122  0.19353791946172713\n",
      "Epoch:0 Loss:6.406246617452188 Accuracy:0.12880557513698568 Loss:5.425855957530142 Accuracy:0.19330056650977007\n",
      "Batch:500 5.706937162399292  0.17305165627598762\n",
      "Batch:1000 5.671671550750732  0.17569011788070202\n",
      "Batch:1500 5.645417999903361  0.1780686892469724\n",
      "Batch:2000 5.619542998313904  0.18057445825636387\n",
      "Batch:2500 5.5962138168334965  0.1825322054207325\n",
      "Batch:3000 5.5761413615544635  0.1842207035223643\n",
      "Batch:3500 5.557922484942845  0.1856954549082688\n",
      "Batch:4000 5.54235468018055  0.1867853783145547\n",
      "Batch:4500 5.529081446223789  0.18764109408524302\n",
      "Batch:5000 5.518484254741669  0.18835105130970478\n",
      "Batch:5500 5.507983255993236  0.18906119615652345\n",
      "Epoch:1 Loss:5.50505294759409\n",
      "Batch:500 5.100656890869141  0.22169451814889907\n",
      "Epoch:1 Loss:5.50505294759409 Accuracy:0.18923245822735318 Loss:5.099391175989519 Accuracy:0.22137688140739856\n",
      "Batch:500 5.408858603477478  0.19664176550507545\n",
      "Batch:1000 5.401922072410583  0.19631319405138492\n",
      "Batch:1500 5.400262317975362  0.19609780938426655\n",
      "Batch:2000 5.396831729173661  0.19635605112463236\n",
      "Batch:2500 5.3939119752883915  0.19639934786558153\n",
      "Batch:3000 5.3915196461677555  0.1965175895790259\n",
      "Batch:3500 5.388526120185852  0.1967619380780629\n",
      "Batch:4000 5.385769357800483  0.19694602362066507\n",
      "Batch:4500 5.383267882559035  0.19711563596129417\n",
      "Batch:5000 5.381774801158905  0.19726802913844585\n",
      "Batch:5500 5.379246633529663  0.19750869844718413\n",
      "Epoch:2 Loss:5.378631846626193\n",
      "Batch:500 5.05398635673523  0.2263610011637211\n",
      "Epoch:2 Loss:5.378631846626193 Accuracy:0.19757107416469638 Loss:5.052741241607179 Accuracy:0.2260743764123658\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(train_tokens,valid_tokens,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:500 5.362048350334168  0.20057363364100456\n",
      "Batch:1000 5.356271614551544  0.20035824891924858\n",
      "Batch:1500 5.3562648229599  0.20016923805077871\n",
      "Batch:2000 5.354249463319778  0.20030934796482325\n",
      "Batch:2500 5.352171015357971  0.2003825347840786\n",
      "Batch:3000 5.350640781402588  0.20040843222041924\n",
      "Batch:3500 5.348602713312421  0.20052802929282187\n",
      "Batch:4000 5.346792228221894  0.20064732870459556\n",
      "Batch:4500 5.345160547150506  0.20077168007029428\n",
      "Batch:5000 5.344375391769409  0.20087138094305992\n",
      "Batch:5500 5.342533753828569  0.20105749984762886\n",
      "Epoch:0 Loss:5.342134636533248\n",
      "Batch:500 5.02782037448883  0.22905990239977836\n",
      "Epoch:0 Loss:5.342134636533248 Accuracy:0.20107552953303764 Loss:5.026465336101477 Accuracy:0.22885690092375024\n",
      "Batch:500 5.334267107963562  0.20334726029634476\n",
      "Batch:1000 5.3293809261322025  0.20306566692888736\n",
      "Batch:1500 5.329662696520487  0.20273114311695098\n",
      "Batch:2000 5.328221499919891  0.20278723277151584\n",
      "Batch:2500 5.326817391586304  0.20271758991479874\n",
      "Batch:3000 5.325701810836792  0.20270092324912548\n",
      "Batch:3500 5.324189166886466  0.20281272340672357\n",
      "Batch:4000 5.322712271213532  0.2028798839226365\n",
      "Batch:4500 5.321520169999864  0.20298785864313443\n",
      "Batch:5000 5.321100407505035  0.20306083180308343\n",
      "Batch:5500 5.319547538757324  0.20325320448658682\n",
      "Epoch:1 Loss:5.319290229048024\n",
      "Batch:500 5.008722056388855  0.23110715502500534\n",
      "Epoch:1 Loss:5.319290229048024 Accuracy:0.20328392723277394 Loss:5.007352806924823 Accuracy:0.23090244294924028\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(train_tokens,valid_tokens,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),f'{PATH}/inter/glove300_model_state_dict')\n",
    "torch.save(optimizer.state_dict(),f'{PATH}/inter/glove300_learner_state_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.unfreeze_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.weight.requires_grad, model.decoder.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:500 5.109595203399659  0.22029616543650626\n",
      "Batch:1000 5.024958127498627  0.22427748450636864\n",
      "Batch:1500 4.976035773277283  0.22652272260189057\n",
      "Batch:2000 4.937592483520508  0.2284240504875779\n",
      "Batch:2500 4.90689591217041  0.22983078132867812\n",
      "Batch:3000 4.881629475434621  0.2310493710587422\n",
      "Batch:3500 4.8595305418287005  0.23218172320723535\n",
      "Batch:4000 4.840361280798912  0.23312686644122005\n",
      "Batch:4500 4.823488499323527  0.23402217317620913\n",
      "Batch:5000 4.808894626903534  0.23476704494059086\n",
      "Batch:5500 4.794439581437544  0.23558677515387536\n",
      "Epoch:0 Loss:4.790255012505474\n",
      "Batch:500 4.369141964912415  0.26816264814138413\n",
      "Epoch:0 Loss:4.790255012505474 Accuracy:0.23581459306172586 Loss:4.368037181227591 Accuracy:0.26794916671809205\n",
      "Batch:500 4.637830147743225  0.24487913247942925\n",
      "Batch:1000 4.627820488929749  0.24505742911994458\n",
      "Batch:1500 4.624270217259725  0.24494964515169462\n",
      "Batch:2000 4.618249243497848  0.24523284111917018\n",
      "Batch:2500 4.612601367759704  0.24534979175329208\n",
      "Batch:3000 4.607347940603892  0.24560312507053217\n",
      "Batch:3500 4.6020822995049615  0.24594695020147733\n",
      "Batch:4000 4.596738917112351  0.24628304709494114\n",
      "Batch:4500 4.592148843023512  0.24654689781533348\n",
      "Batch:5000 4.588271742630005  0.24679501132667064\n",
      "Batch:5500 4.583393721060319  0.24715495631369677\n",
      "Epoch:1 Loss:4.582179441327473\n",
      "Batch:500 4.261753575325012  0.2755428685247898\n",
      "Epoch:1 Loss:4.582179441327473 Accuracy:0.2472301135599045 Loss:4.260612500721568 Accuracy:0.27529730168921335\n",
      "Batch:500 4.537899973869323  0.25159946155548096\n",
      "Batch:1000 4.529589542865753  0.25135083523392676\n",
      "Batch:1500 4.528328512509664  0.2511426848769188\n",
      "Batch:2000 4.5248366091251375  0.2512288571074605\n",
      "Batch:2500 4.521664792633056  0.25120165929794314\n",
      "Batch:3000 4.518870270570119  0.2513005603949229\n",
      "Batch:3500 4.515752527509417  0.251469555663211\n",
      "Batch:4000 4.512454617857933  0.25166821148991586\n",
      "Batch:4500 4.509708106888665  0.25185190352135234\n",
      "Batch:5000 4.507599619293213  0.2520146263539791\n",
      "Batch:5500 4.504414307420904  0.25228147957541724\n",
      "Epoch:2 Loss:4.503653407728045\n",
      "Batch:500 4.198656305789948  0.28000550734996793\n",
      "Epoch:2 Loss:4.503653407728045 Accuracy:0.25233567244660254 Loss:4.197566623322701 Accuracy:0.2797691472219318\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(train_tokens,valid_tokens,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),f'{PATH}/inter/glove300_model_state_dict')\n",
    "torch.save(optimizer.state_dict(),f'{PATH}/inter/glove300_learner_state_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:500 4.4805640687942505  0.25539451655745504\n",
      "Batch:1000 4.474158131122589  0.2550252858698368\n",
      "Batch:1500 4.473895053227743  0.2547846265236537\n",
      "Batch:2000 4.471721058607101  0.25485619227588174\n",
      "Batch:2500 4.469276383209229  0.25479858236908914\n",
      "Batch:3000 4.46733182032903  0.254842776487271\n",
      "Batch:3500 4.465054332051959  0.2549701836151736\n",
      "Batch:4000 4.462561264872551  0.2551188982762396\n",
      "Batch:4500 4.460545792579651  0.2552862135635482\n",
      "Batch:5000 4.459197494506836  0.2553994614362717\n",
      "Batch:5500 4.456690646691756  0.25562358725883744\n",
      "Epoch:0 Loss:4.456210838288247\n",
      "Batch:500 4.1550737524032595  0.28319616737961767\n",
      "Epoch:0 Loss:4.456210838288247 Accuracy:0.2556529957110748 Loss:4.154098665124872 Accuracy:0.28289825560753806\n",
      "Batch:500 4.441973182678223  0.2583093514740467\n",
      "Batch:1000 4.43620744562149  0.2580338017642498\n",
      "Batch:1500 4.436528824806214  0.25765349028507867\n",
      "Batch:2000 4.434580546855926  0.25761292251199486\n",
      "Batch:2500 4.432978987693787  0.25742198864817617\n",
      "Batch:3000 4.431627547899882  0.2574296809285879\n",
      "Batch:3500 4.429962221826826  0.2575103716594832\n",
      "Batch:4000 4.427977996706963  0.2576080463267863\n",
      "Batch:4500 4.426498258272807  0.2577187529702981\n",
      "Batch:5000 4.4256556056976315  0.2577689666360617\n",
      "Batch:5500 4.4235888145620175  0.2579916189854795\n",
      "Epoch:1 Loss:4.423207478097416\n",
      "Batch:500 4.124368309020996  0.2857170466184616\n",
      "Epoch:1 Loss:4.423207478097416 Accuracy:0.2580111697368768 Loss:4.123569920112452 Accuracy:0.2853473415405176\n",
      "Batch:500 4.413873003959655  0.2602236373126507\n",
      "Batch:1000 4.408622694015503  0.2599895708709955\n",
      "Batch:1500 4.409398337682088  0.25961502866943675\n",
      "Batch:2000 4.40816685461998  0.25945715326815844\n",
      "Batch:2500 4.406737294578552  0.2593380323767662\n",
      "Batch:3000 4.40567233133316  0.2593734537363052\n",
      "Batch:3500 4.404402483940125  0.2594475772551128\n",
      "Batch:4000 4.4027214111089705  0.25954231825098395\n",
      "Batch:4500 4.401318680021498  0.25961045014527107\n",
      "Batch:5000 4.400759583282471  0.259659625890851\n",
      "Batch:5500 4.398996559056369  0.25984801253134554\n",
      "Epoch:2 Loss:4.398811217630199\n",
      "Batch:500 4.100456211566925  0.28757089364528654\n",
      "Epoch:2 Loss:4.398811217630199 Accuracy:0.2598760041601869 Loss:4.099743381071319 Accuracy:0.28728728854294977\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(train_tokens,valid_tokens,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),f'{PATH}/inter/glove50_model_state_dict')\n",
    "torch.save(optimizer.state_dict(),f'{PATH}/inter/glove50_learner_state_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:500 4.394060876846313  0.2616000111401081\n",
      "Batch:1000 4.3888170924186705  0.2612609998434782\n",
      "Batch:1500 4.3895150696436565  0.2608487287759781\n",
      "Batch:2000 4.388436628103256  0.260814021833241\n",
      "Batch:2500 4.386969807815552  0.2607619886636734\n",
      "Batch:3000 4.38608130645752  0.26082647589345775\n",
      "Batch:3500 4.384810282434736  0.2608701048578535\n",
      "Batch:4000 4.383412978053093  0.26090749687701464\n",
      "Batch:4500 4.382347243203057  0.2609957371254762\n",
      "Batch:5000 4.3818060770988465  0.26106951603591444\n",
      "Batch:5500 4.380182161331176  0.26122533525391056\n",
      "Epoch:0 Loss:4.3799635115925435\n",
      "Batch:500 4.081819926738739  0.28890056455135343\n",
      "Epoch:0 Loss:4.3799635115925435 Accuracy:0.2612428658270726 Loss:4.081071397714448 Accuracy:0.2885811706526618\n",
      "Batch:500 4.376593412399292  0.26288462603092194\n",
      "Batch:1000 4.371794269084931  0.2627299554347992\n",
      "Batch:1500 4.373024185498555  0.262359717498223\n",
      "Batch:2000 4.3721199862957  0.26224218074977396\n",
      "Batch:2500 4.370902025222779  0.26204902150034903\n",
      "Batch:3000 4.37020948712031  0.2620467137595018\n",
      "Batch:3500 4.369172259466989  0.2620836838526385\n",
      "Batch:4000 4.3677815381288525  0.26213936471939087\n",
      "Batch:4500 4.366886200375027  0.2622075805399153\n",
      "Batch:5000 4.366561845970153  0.26225550484359267\n",
      "Batch:5500 4.3651036858125165  0.26241274760257116\n",
      "Epoch:1 Loss:4.364951656599412\n",
      "Batch:500 4.066004956245423  0.2902159493565559\n",
      "Epoch:1 Loss:4.364951656599412 Accuracy:0.26243400499829544 Loss:4.065498522784341 Accuracy:0.2898575259072549\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(train_tokens,valid_tokens,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49.40244910553017, 59.739891704145194)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(3.9), np.exp(4.09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1==0:\n",
    "    learner.lr_find(1e-4,1e-1,valid_tokens,n_batch)\n",
    "    learner.plot_lrs(50)\n",
    "    len(learner.lrs)\n",
    "    len(learner.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),f'{PATH}/inter/glove300_model_state_dict')\n",
    "torch.save(optimizer.state_dict(),f'{PATH}/inter/glove300_learner_state_dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning to predict RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
