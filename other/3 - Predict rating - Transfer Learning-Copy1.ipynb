{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import os \n",
    "import fastai\n",
    "import fastai\n",
    "from fastai.text import *\n",
    "PATH=\"/home/kirana/Documents/phd/final\"\n",
    "DATAPATH=\"/home/kirana/Documents/phd/data/aclImdb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "[df_train,df_valid,df_test,itos, train_tokens, valid_tokens, test_tokens, trn_lm, val_lm, test_lm]=pickle.load(open(f'{PATH}/inter/dfs_tokens_fastai.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=52 # 52 - Jeremey, 20 - default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt= 70 #70 - Jeremey, 35 - default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18413</th>\n",
       "      <td>2</td>\n",
       "      <td>I've just got back from watching this delightf...</td>\n",
       "      <td>[ \\n , xxbos, xxfld, 1, i, 've, just, got, bac...</td>\n",
       "      <td>[41, 42, 43, 40, 13, 161, 57, 210, 162, 51, 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43883</th>\n",
       "      <td>2</td>\n",
       "      <td>For a first film, this shows a great deal of v...</td>\n",
       "      <td>[ \\n , xxbos, xxfld, 1, xxmaj, for, a, first, ...</td>\n",
       "      <td>[41, 42, 43, 40, 2, 24, 7, 106, 27, 5, 14, 298...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45342</th>\n",
       "      <td>2</td>\n",
       "      <td>I am fully aware that no film adaptation of a ...</td>\n",
       "      <td>[ \\n , xxbos, xxfld, 1, i, am, fully, aware, t...</td>\n",
       "      <td>[41, 42, 43, 40, 13, 264, 1390, 1811, 15, 74, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6454</th>\n",
       "      <td>2</td>\n",
       "      <td>I'm always very cautious when people refer to ...</td>\n",
       "      <td>[ \\n , xxbos, xxfld, 1, i, 'm, always, very, c...</td>\n",
       "      <td>[41, 42, 43, 40, 13, 167, 236, 70, 15406, 69, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13703</th>\n",
       "      <td>2</td>\n",
       "      <td>I started to watch this movie but gave up beca...</td>\n",
       "      <td>[ \\n , xxbos, xxfld, 1, i, started, to, watch,...</td>\n",
       "      <td>[41, 42, 43, 40, 13, 677, 9, 127, 14, 25, 26, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                               text  \\\n",
       "18413      2  I've just got back from watching this delightf...   \n",
       "43883      2  For a first film, this shows a great deal of v...   \n",
       "45342      2  I am fully aware that no film adaptation of a ...   \n",
       "6454       2  I'm always very cautious when people refer to ...   \n",
       "13703      2  I started to watch this movie but gave up beca...   \n",
       "\n",
       "                                                   words  \\\n",
       "18413  [ \\n , xxbos, xxfld, 1, i, 've, just, got, bac...   \n",
       "43883  [ \\n , xxbos, xxfld, 1, xxmaj, for, a, first, ...   \n",
       "45342  [ \\n , xxbos, xxfld, 1, i, am, fully, aware, t...   \n",
       "6454   [ \\n , xxbos, xxfld, 1, i, 'm, always, very, c...   \n",
       "13703  [ \\n , xxbos, xxfld, 1, i, started, to, watch,...   \n",
       "\n",
       "                                                  tokens  \n",
       "18413  [41, 42, 43, 40, 13, 161, 57, 210, 162, 51, 16...  \n",
       "43883  [41, 42, 43, 40, 2, 24, 7, 106, 27, 5, 14, 298...  \n",
       "45342  [41, 42, 43, 40, 13, 264, 1390, 1811, 15, 74, ...  \n",
       "6454   [41, 42, 43, 40, 13, 167, 236, 70, 15406, 69, ...  \n",
       "13703  [41, 42, 43, 40, 13, 677, 9, 127, 14, 25, 26, ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_unk_', '_pad_', 'xxmaj', 'the', '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch=df_train.shape[0]//bs\n",
    "padlen=640\n",
    "padding_idx=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['n_tok']=df_train['tokens'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    67500.000000\n",
       "mean       305.567867\n",
       "std        225.163517\n",
       "min         15.000000\n",
       "25%        166.000000\n",
       "50%        229.000000\n",
       "75%        371.000000\n",
       "max       3455.000000\n",
       "Name: n_tok, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['n_tok'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.sort_values(by='n_tok', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid['n_tok']=df_valid['tokens'].apply(len)\n",
    "df_valid.sort_values(by='n_tok', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['n_tok']=df_test['tokens'].apply(len)\n",
    "df_test.sort_values(by='n_tok', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41. 42. 43. 40. ...  1.  1.  1.  1.]\n",
      "2\n",
      "640\n",
      "[41. 42. 43. 40. ...  1.  1.  1.  1.]\n",
      "2\n",
      "640\n",
      "[41. 42. 43. 40. ...  1.  1.  1.  1.]\n",
      "0\n",
      "640\n",
      "[41. 42. 43. 40. ...  1.  1.  1.  1.]\n",
      "0\n",
      "640\n",
      "[41. 42. 43. 40. ...  1.  1.  1.  1.]\n",
      "2\n",
      "640\n",
      "[41. 42. 43. 40. ...  1.  1.  1.  1.]\n",
      "2\n",
      "640\n",
      "[41. 42. 43. 40. ...  1.  1.  1.  1.]\n",
      "0\n",
      "640\n",
      "[41. 42. 43. 40. ...  1.  1.  1.  1.]\n",
      "0\n",
      "640\n",
      "[41. 42. 43. 40. ...  1.  1.  1.  1.]\n",
      "1\n",
      "640\n",
      "[41. 42. 43. 40. ...  1.  1.  1.  1.]\n",
      "2\n",
      "640\n",
      "[41. 42. 43. 40. ...  1.  1.  1.  1.]\n",
      "2\n",
      "640\n",
      "[41. 42. 43. 40. ...  1.  1.  1.  1.]\n",
      "2\n",
      "640\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,n_batch):\n",
    "    x=df_train['tokens'].values[i]\n",
    "    y=df_train['label'].values[i]    \n",
    "    out=np.ones(padlen)\n",
    "    if len(x) < padlen:\n",
    "        out[:len(x)]=x\n",
    "    else:\n",
    "        out=x[:padlen]\n",
    "    print (out)\n",
    "    print (y)\n",
    "    print (len(out))\n",
    "    if i>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67500, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2    45002\n",
       " 1    11252\n",
       " 0    11246\n",
       " Name: label, dtype: int64, 2    4998\n",
       " 0    1254\n",
       " 1    1248\n",
       " Name: label, dtype: int64, 1    12500\n",
       " 0    12500\n",
       " Name: label, dtype: int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].value_counts(), df_valid['label'].value_counts(),df_test['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_big=df_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_big=df_valid.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid=df_valid.loc[df_valid['label']<2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df_train.loc[df_train['label']<2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22498, 5), (2502, 5))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones(3).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ds_sentiment (Dataset):\n",
    "    def __init__ (self,df,bs,padlen=64,xvar='tokens',yvar='label',len_var='n_tok',padding_idx=1):\n",
    "        self.x,self.y,self.padlen,self.padding_idx,self.len_var,self.bs=\\\n",
    "            df[xvar],df[yvar],padlen,padding_idx,df[len_var],bs\n",
    "    def pad (self,x):\n",
    "        out=np.ones(self.padlen)*self.padding_idx\n",
    "        out=out.astype(int)\n",
    "        if len(x)>=self.padlen:\n",
    "            out[:]=x[:self.padlen]\n",
    "        else:\n",
    "            out[:len(x)]=x\n",
    "        return out\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        return self.pad(self.x.iloc[idx]),self.y.iloc[idx],self.len_var.iloc[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "padlen=max(df_train['n_tok'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3354"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstrain=ds_sentiment(df_train,bs,padlen)\n",
    "dsvalid=ds_sentiment(df_valid,bs,padlen)\n",
    "dstest=ds_sentiment(df_test,bs,padlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dltrain=DataLoader(dstrain,bs,True)\n",
    "dlvalid=DataLoader(dsvalid,bs,False)\n",
    "dltest=DataLoader(dstest,bs,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xb,yb,xlen in dltrain:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[41, 42, 43,  ...,  1,  1,  1],\n",
       "         [41, 42, 43,  ...,  1,  1,  1],\n",
       "         [41, 42, 43,  ...,  1,  1,  1],\n",
       "         ...,\n",
       "         [41, 42, 43,  ...,  1,  1,  1],\n",
       "         [41, 42, 43,  ...,  1,  1,  1],\n",
       "         [41, 42, 43,  ...,  1,  1,  1]]),\n",
       " tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,\n",
       "         0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "         1, 1, 0, 0]),\n",
       " tensor([ 171,  250,  223,  165,  143,  389,  447,  192,  328,  124,  304,  163,\n",
       "          259,   80,  164,  135,  283,  191, 1017,  124,  426,  146,  314,  199,\n",
       "          186,  216,  245,  154,  664,  360,  218,   85,  107,  126,  185,  267,\n",
       "           61,  436,  113,  259,  479,  195,  330,  183,   73,  513,   95,  200,\n",
       "          162,  188,   98,  178]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb, xlen"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cyclical learning rates\n",
    "\tvary learning rate by epoch between two values\n",
    "\t\n",
    "\n",
    "bptt=70\n",
    "n_emb=400\n",
    "n_hid=1150\n",
    "n_layers=3\n",
    "bs=48 # whatever is the max that can fit in memory\n",
    "\n",
    "# shuffle dataset\n",
    "# sort the data by length\n",
    "\n",
    "\n",
    "class ds_sentiment(Dataset):\n",
    "\tdef __init__(self,x,y):\n",
    "\t\tself.x,self.y=x,y\n",
    "\t\n",
    "\tdef __getitem__(self,idx):\n",
    "\t\treturn self.x[idx],self.y[idx]\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 70)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs,bptt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inp=len(itos)\n",
    "n_emb=400 #650\n",
    "n_hidden=400 #650\n",
    "n_layers= 2 # 2\n",
    "dropout= 0.5\n",
    "wd=1e-7\n",
    "bidirectional=False\n",
    "dropout_e= 0.05\n",
    "dropout_o=0.5\n",
    "n_out=1\n",
    "n_hidden2=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([52, 3354]), torch.Size([52]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape,xlen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    11252\n",
       "0    11246\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentiment_classifier (nn.Module):\n",
    "    def __init__(self,n_inp,n_emb,n_hidden,n_layers,bidirectional,bs,device,dropout_e=0.05,dropout=0.5,\\\n",
    "                 dropout_o=0.5,pretrain_mtx=None,n_out=1,padding_idx=1,n_hidden2=25):\n",
    "        super().__init__()\n",
    "        self.n_inp,self.n_emb,self.n_hidden,self.n_layers,self.bidirectional,self.bs,self.device,self.pretrain_mtx,self.padding_idx=\\\n",
    "                            n_inp,n_emb,n_hidden,n_layers,bidirectional,bs,device,pretrain_mtx,padding_idx\n",
    "        self.n_out,self.n_hidden2=n_out,n_hidden2\n",
    "        self.dropout_e,self.dropout,self.dropout_o=dropout_e,dropout,dropout_o\n",
    "        \n",
    "        self.create_architecture()\n",
    "        if pretrain_mtx is not None:\n",
    "            print (\"initializing\")\n",
    "            self.initialize_glove()\n",
    "        self.init_hidden()\n",
    "        self.criterion=nn.BCELoss()\n",
    "        \n",
    "    def create_architecture(self):\n",
    "        # Dropout layer\n",
    "        self.dropout_enc=nn.Dropout(self.dropout_e)\n",
    "        # Embedding Layer: Embedding layer just maps each word to an index. n_inp to n_emb mapping is all it does\n",
    "            # input to this is of shape n_batch * n_seq\n",
    "        self.encoder=nn.Embedding(self.n_inp,self.n_emb,padding_idx=self.padding_idx)\n",
    "             # embs are going to be of shape n_batch * n_seq * n_emb\n",
    "            # h and c are of shape n_layers * n_batch * n_hidden\n",
    "        # LSTM Layer\n",
    "        self.lstm=nn.LSTM(self.n_emb,self.n_hidden,self.n_layers,batch_first=True,dropout=self.dropout,\\\n",
    "                          bidirectional=self.bidirectional)\n",
    "        \n",
    "                # output is of shape n_batch * n_seq * n_hidden\n",
    "            # output [:,-1,:] is the same as hn[-1]\n",
    "        self.dropout_op=nn.Dropout(self.dropout_o)\n",
    "        self.fc=nn.Linear(self.n_hidden,self.n_out)\n",
    "\n",
    "        #self.log_softmax=nn.LogSoftmax()\n",
    "        self.log_softmax=nn.Sigmoid()\n",
    "    \n",
    "    def freeze_embedding(self):\n",
    "        self.encoder.weight.requires_grad=False\n",
    "         \n",
    "    def unfreeze_embedding(self):\n",
    "        self.encoder.weight.requires_grad=True\n",
    "\n",
    "    def initialize_glove(self):\n",
    "        self.encoder.weight.data=torch.Tensor(self.pretrain_mtx)\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # Initialize hidden\n",
    "        self.hidden=(Variable(torch.randn(self.n_layers,self.bs,self.n_hidden,requires_grad=False).to(self.device)),\n",
    "                     Variable(torch.randn(self.n_layers,self.bs,self.n_hidden,requires_grad=False).to(self.device)))\n",
    "    \n",
    "        \n",
    "    def forward(self,Xb,Yb,Xb_lengths):\n",
    "        embs=self.encoder(Xb)\n",
    "        #embs=self.dropout_enc(embs)\n",
    "        if Xb.size(0) < self.bs:\n",
    "            self.hidden=(self.hidden[0][:,:Xb.size(0),:].contiguous(),\n",
    "            self.hidden[1][:,:Xb.size(0),:].contiguous())\n",
    "        \n",
    "        # before lstm call\n",
    "            # pack_padded_sequence so that padded items in the sequence won't be shown to the LSTM\n",
    "        embs = pack_padded_sequence(embs,Xb_lengths,batch_first=True,enforce_sorted=False)\n",
    "        lstm_out,self.hidden=self.lstm(embs,self.hidden)\n",
    "\n",
    "        # after lstm call\n",
    "        lstm_out,_=pad_packed_sequence(lstm_out,batch_first=True)\n",
    "        #lstm_out=self.dropout_op(lstm_out)\n",
    "\n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "       \n",
    "        \n",
    "        # Wrap the hidden state in a new tensor without the gradients\n",
    "        if 1==1:\n",
    "            self.hidden=(Variable(self.hidden[0].data,requires_grad=False).to(self.device),\\\n",
    "                         Variable(self.hidden[1].data,requires_grad=False).to(self.device))\n",
    "        \n",
    "            # lstm_out has the values that are padded\n",
    "        #out=self.fc (lstm_out[:,-1,:])\n",
    "        out=self.fc(self.hidden[0][-1])\n",
    "\n",
    "        preds=self.log_softmax(out)\n",
    "        \n",
    "              \n",
    "        \n",
    "        loss=self.criterion(preds,Yb.contiguous().float().view(-1,1))\n",
    "        \n",
    "        return preds.view(-1), loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_binomial(preds,actual):\n",
    "    correct=torch.round(preds).long()==actual\n",
    "    return correct.float().sum()/len(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_lm_weights=pickle.load(open(f'{PATH}/inter/pretrained_lm_weights','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1==0:\n",
    "    model=torch.load (f'{PATH}/inter/model_awd_lstm')\n",
    "    pretrained_lm_weights=model.encoder.weight.data.cpu().numpy()\n",
    "    import pickle\n",
    "    pickle.dump(pretrained_lm_weights,open(f'{PATH}/inter/pretrained_lm_weights','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if model forward works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59972, 400, 400, 2, False, 52, 'cpu', 0.05, 0.5, 0.5, 1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_inp,n_emb,n_hidden,n_layers,bidirectional,bs,device,dropout_e,dropout_o,dropout,n_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing\n"
     ]
    }
   ],
   "source": [
    "model_sentiment=sentiment_classifier (n_inp,n_emb,n_hidden,n_layers,bidirectional,bs,device,dropout_e=dropout_e,dropout=dropout,\\\n",
    "                 dropout_o=dropout_o,pretrain_mtx=pretrained_lm_weights,n_out=1,padding_idx=1,n_hidden2=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment_classifier(\n",
       "  (dropout_enc): Dropout(p=0.05)\n",
       "  (encoder): Embedding(59972, 400, padding_idx=1)\n",
       "  (lstm): LSTM(400, 400, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout_op): Dropout(p=0.5)\n",
       "  (fc): Linear(in_features=400, out_features=1, bias=True)\n",
       "  (log_softmax): Sigmoid()\n",
       "  (criterion): BCELoss()\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5010, 0.5039, 0.5030, 0.5010, 0.5001, 0.4957, 0.5024, 0.4984, 0.4988,\n",
       "         0.4995, 0.4981, 0.5003, 0.5002, 0.4997, 0.4973, 0.4975, 0.4982, 0.4953,\n",
       "         0.4961, 0.4971, 0.5035, 0.5019, 0.5017, 0.4968, 0.4970, 0.4990, 0.5038,\n",
       "         0.4983, 0.5024, 0.5023, 0.4942, 0.4989, 0.4976, 0.5033, 0.4997, 0.4993,\n",
       "         0.4937, 0.5017, 0.4960, 0.4981, 0.5002, 0.4992, 0.5021, 0.5003, 0.4985,\n",
       "         0.5019, 0.4975, 0.4958, 0.5077, 0.5006, 0.5039, 0.5019],\n",
       "        grad_fn=<ViewBackward>),\n",
       " tensor(0.6949, grad_fn=<BinaryCrossEntropyBackward>))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sentiment.forward(xb,yb,xlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds,loss=model_sentiment.forward(xb,yb,xlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4982, 0.4975, 0.5049, 0.4989, 0.5002, 0.4945, 0.5047, 0.4988, 0.5027,\n",
       "        0.5007, 0.5002, 0.5034, 0.4985, 0.4996, 0.5016, 0.5038, 0.5022, 0.5005,\n",
       "        0.5000, 0.5028, 0.4989, 0.5015, 0.4965, 0.4985, 0.4984, 0.5038, 0.5001,\n",
       "        0.5038, 0.4998, 0.5041, 0.4971, 0.4956, 0.4950, 0.5028, 0.4977, 0.4973,\n",
       "        0.4975, 0.5020, 0.4954, 0.4986, 0.4978, 0.5010, 0.4990, 0.4997, 0.4982,\n",
       "        0.4987, 0.4996, 0.4961, 0.5006, 0.4981, 0.5073, 0.5007],\n",
       "       grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "        1, 1, 0, 0])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([52])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([52])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5000)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_binomial(preds,yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3851851851851852"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(yb,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9.038397e-02, -1.795423e-02, -3.457930e-02,  8.671192e-02, ..., -3.003510e-02, -1.899482e-02,  1.426260e-02,\n",
       "        -4.948893e-02],\n",
       "       [-2.453323e-02, -3.018817e-02,  6.663179e-02, -5.793565e-02, ...,  1.840935e-02,  6.371137e-02,  9.835491e-03,\n",
       "        -2.131385e-03],\n",
       "       [ 7.696263e-03, -4.266643e-02, -1.533517e-01,  2.239776e-01, ...,  9.869517e-02,  3.041433e-02,  1.824751e-01,\n",
       "         1.134978e-01],\n",
       "       [ 3.305928e-02,  2.266591e-01, -4.264669e-02,  1.490862e-01, ...,  3.407921e-02, -6.422209e-03,  3.180612e-01,\n",
       "         9.373549e-02],\n",
       "       ...,\n",
       "       [ 6.027538e-02, -5.980809e-02,  1.861691e-01, -3.105092e-02, ..., -2.764457e-02,  1.962678e-02, -2.172215e-03,\n",
       "         6.297247e-02],\n",
       "       [-1.592789e-02, -2.781571e-04,  1.301994e-01,  2.851282e-02, ...,  5.064877e-02,  1.670864e-01,  2.283701e-02,\n",
       "        -8.746398e-03],\n",
       "       [ 4.669700e-02,  3.138980e-02,  1.221957e-02, -3.927753e-02, ..., -1.281436e-01,  1.121320e-01, -3.726090e-03,\n",
       "        -3.777364e-02],\n",
       "       [ 1.677964e-02,  3.287802e-02,  2.598143e-02, -1.484041e-02, ..., -2.587889e-02,  4.438318e-02, -2.407267e-02,\n",
       "        -8.544586e-02]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_lm_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self,model,optimizer,metric_fn,device,bptt=12,print_every=5,clip_val=None):\n",
    "        self.model,self.optimizer,self.metric_fn,self.device,self.print_every,self.bptt,self.losses,self.clip_val=\\\n",
    "            model,optimizer,metric_fn,device,print_every,bptt,[],clip_val\n",
    "        self.n_epochs=1\n",
    "  \n",
    "        \n",
    "    \n",
    "    def fit (self,Xb,Yb,Xlen,mode_train=True):\n",
    "        if mode_train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            \n",
    "        preds,loss=self.model(Xb,Yb,Xlen)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            acc=self.metric_fn(preds,Yb.view(-1))\n",
    "            acc=acc.item()\n",
    "            del preds\n",
    "        \n",
    "        if mode_train:\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        myloss=loss.item()\n",
    "        del loss\n",
    "        \n",
    "        if self.clip_val is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip_val)\n",
    "            if 1==0:\n",
    "                for p in self.model.parameters():\n",
    "                    p.data.add_(self.lr, p.grad.data)\n",
    "        \n",
    "        return myloss, acc\n",
    "    \n",
    "    def lr_find (self,start_lr,end_lr,iterator,n_batch):\n",
    "        losses,lrs=[],[]\n",
    "        ratio=end_lr/start_lr\n",
    "        num_steps=n_batch\n",
    "        lr=start_lr\n",
    "        for i in range(num_steps):            \n",
    "            lr=lr*(end_lr/start_lr)**(1/num_steps)\n",
    "            lrs.append(lr)\n",
    "        self.lrs=lrs\n",
    "        self.run_epoch(iterator,mode_train=True,lrs=lrs)\n",
    "    \n",
    "    def run_epoch(self,iterator,mode_train,lrs=None):\n",
    "        epoch_loss,epoch_acc,i,k=0,0,0,0\n",
    "        self.model.init_hidden()\n",
    "        for Xb,Yb,Xlen in iterator:\n",
    "            Xb=Xb.to(self.device)\n",
    "            Yb=Yb.to(self.device)\n",
    "            Xlen=Xlen.to(self.device)\n",
    "            \n",
    "            if lrs is not None:\n",
    "                lr=lrs[k]\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr']=lr \n",
    "            \n",
    "\n",
    "            loss,acc=self.fit(Xb,Yb,Xlen,mode_train)\n",
    "            \n",
    "            if lrs is not None:\n",
    "                self.losses.append(loss)\n",
    "            \n",
    "            \n",
    "            epoch_loss+=loss\n",
    "            epoch_acc+=acc\n",
    "            \n",
    "            k=k+1\n",
    "            if k%self.print_every == 0:\n",
    "                if k:\n",
    "                    print (f'Batch:{k} {epoch_loss/(k)}  {epoch_acc/(k)}')  \n",
    "                    torch.cuda.empty_cache()\n",
    "        epoch_loss=epoch_loss/k\n",
    "        epoch_acc=epoch_acc/k\n",
    "            \n",
    "        return epoch_loss,epoch_acc\n",
    "    \n",
    "    def plot_lrs(self, n_roll=1):\n",
    "        import seaborn as sns\n",
    "        ax=sns.lineplot(x=self.lrs,y=pd.Series(self.losses).rolling(n_roll).mean())\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.set_xlabel('Learning Rate')\n",
    "\n",
    "    \n",
    "    def run_epochs(self,dltrain,dlvalid,n_epochs=1):\n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            loss,acc=self.run_epoch(dltrain,True)\n",
    "            print (f'Epoch:{epoch} Loss:{loss}')\n",
    "            lossv,accv=self.run_epoch(dlvalid,mode_train=False)\n",
    "            print (f'Epoch:{epoch} Loss:{loss} Accuracy:{acc} Loss:{lossv} Accuracy:{accv}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing\n"
     ]
    }
   ],
   "source": [
    "model_sentiment=sentiment_classifier (n_inp,n_emb,n_hidden,n_layers,bidirectional,bs,device,dropout_e,dropout,\\\n",
    "                 dropout_o,pretrain_mtx=pretrained_lm_weights,n_out=1,padding_idx=1,n_hidden2=n_hidden2)\n",
    "model_sentiment=model_sentiment.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(model_sentiment.parameters(),lr=1e-3,betas=(0.9,0.999), weight_decay=wd)\n",
    "metric_fn=accuracy_binomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(433, 49)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dltrain),len(dlvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1254\n",
       "1    1248\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner=Learner(model_sentiment,optimizer,accuracy_binomial,device,bptt,50,0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sentiment.freeze_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sentiment.encoder.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:50 0.6929491329193115  0.5080769407749176\n",
      "Batch:100 0.6929148989915848  0.5069230940937995\n",
      "Batch:150 0.6926568806171417  0.5157692476113638\n",
      "Batch:200 0.6923693001270295  0.5208654017746448\n",
      "Batch:250 0.6921350879669189  0.522692324757576\n",
      "Batch:300 0.6919598573446274  0.5255769407749176\n",
      "Batch:350 0.6917337763309479  0.5302197984286717\n",
      "Batch:400 0.6915489307045937  0.5310096336156129\n",
      "Epoch:0 Loss:0.6914546627370645\n",
      "Epoch:0 Loss:0.6914546627370645 Accuracy:0.5321549301059362 Loss:0.6900792377335685 Accuracy:0.5256410447918639\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(dltrain,dlvalid,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_group in learner.optimizer.param_groups:\n",
    "    param_group['lr']=5e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "433"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dltrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:50 0.6888651859760284  0.5465384829044342\n",
      "Batch:100 0.6868347001075744  0.5592307892441749\n",
      "Batch:150 0.6865313390890757  0.5566666867335638\n",
      "Batch:200 0.6862078475952148  0.5586538667976856\n",
      "Batch:250 0.6858403780460358  0.5570769445896149\n",
      "Batch:300 0.6851726808150609  0.5599359187483788\n",
      "Batch:350 0.6844567174570901  0.5637912304060799\n",
      "Batch:400 0.6838803415000438  0.5630769443511963\n",
      "Epoch:0 Loss:0.6838188163127414\n",
      "Epoch:0 Loss:0.6838188163127414 Accuracy:0.5624680177719433 Loss:0.6815139103908928 Accuracy:0.5648875157443844\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(dltrain,dlvalid,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:50 0.6763112819194794  0.5957692557573319\n",
      "Batch:100 0.677610512971878  0.5861538690328598\n",
      "Batch:150 0.6767759549617768  0.5900000228484472\n",
      "Batch:200 0.6763600370287896  0.5888461762666702\n",
      "Batch:250 0.677184455871582  0.5840000220537186\n",
      "Batch:300 0.6770491216580073  0.5844872006773949\n",
      "Batch:350 0.6766948577335903  0.5839011204242707\n",
      "Batch:400 0.6763908997178077  0.5843269442021847\n",
      "Epoch:0 Loss:0.6763449871512539\n",
      "Epoch:0 Loss:0.6763449871512539 Accuracy:0.5839168172647992 Loss:0.6754414633828767 Accuracy:0.594584006436017\n",
      "Batch:50 0.6701778411865235  0.590384635925293\n",
      "Batch:100 0.6711819350719452  0.5898077124357224\n",
      "Batch:150 0.6716099389394125  0.5919230979681015\n",
      "Batch:200 0.6717160579562187  0.5920192520320415\n",
      "Batch:250 0.6719906995296479  0.589692329287529\n",
      "Batch:300 0.6724682903289795  0.585705149670442\n",
      "Batch:350 0.671772335256849  0.5873626582111631\n",
      "Batch:400 0.672002954185009  0.5861538670212031\n",
      "Epoch:1 Loss:0.6721237377536765\n",
      "Epoch:1 Loss:0.6721237377536765 Accuracy:0.5862994366787873 Loss:0.6697840629791727 Accuracy:0.6012559101289633\n",
      "Batch:50 0.669329571723938  0.5888461726903915\n",
      "Batch:100 0.6696430867910386  0.5936538669466972\n",
      "Batch:150 0.6712187512715657  0.5876923290888468\n",
      "Batch:200 0.6700105783343315  0.5909615603089332\n",
      "Batch:250 0.670040830373764  0.5897692530155182\n",
      "Batch:300 0.6696049888928731  0.5899359196424484\n",
      "Batch:350 0.6693784523010254  0.5896154064791543\n",
      "Batch:400 0.6698138621449471  0.589567329660058\n",
      "Epoch:2 Loss:0.6698849901866692\n",
      "Epoch:2 Loss:0.6698849901866692 Accuracy:0.5887421452145654 Loss:0.6667977121411538 Accuracy:0.6098901313178393\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(dltrain,dlvalid,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_group in learner.optimizer.param_groups:\n",
    "    param_group['lr']=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:50 0.64853853225708  0.6207692527770996\n",
      "Batch:100 0.6483927577733993  0.6265384835004807\n",
      "Batch:150 0.6484267862637838  0.6321795090039571\n",
      "Batch:200 0.6477748197317124  0.6351923295855522\n",
      "Batch:250 0.6486767339706421  0.6325384842157364\n",
      "Batch:300 0.6489897185564041  0.6337179721395174\n",
      "Batch:350 0.6487747352463858  0.6352198032821927\n",
      "Batch:400 0.6485072585940361  0.6359134846925736\n",
      "Epoch:0 Loss:0.648502684768298\n",
      "Epoch:0 Loss:0.648502684768298 Accuracy:0.6363788593999232 Loss:0.655621100445183 Accuracy:0.626373649859915\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(dltrain,dlvalid,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_group in learner.optimizer.param_groups:\n",
    "    param_group['lr']=1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sentiment.unfreeze_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:50 0.6453262722492218  0.6419231033325196\n",
      "Batch:100 0.646092454791069  0.6440384846925735\n",
      "Batch:150 0.6475411713123321  0.6400000242392222\n",
      "Batch:200 0.6472525778412819  0.6382692554593086\n",
      "Batch:250 0.6482151463031769  0.6372307926416397\n",
      "Batch:300 0.648103004693985  0.6371154076854388\n",
      "Batch:350 0.6477661645412445  0.637967055950846\n",
      "Batch:400 0.647689622193575  0.6379807917028666\n",
      "Epoch:0 Loss:0.6474101744402914\n",
      "Epoch:0 Loss:0.6474101744402914 Accuracy:0.6383487004743713 Loss:0.6549209563099608 Accuracy:0.6357928058322595\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(dltrain,dlvalid,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:50 0.6463013350963592  0.6476923274993897\n",
      "Batch:100 0.6445563387870789  0.6455769440531731\n",
      "Batch:150 0.6444319272041321  0.6460256640116374\n",
      "Batch:200 0.6456667852401733  0.642500022649765\n",
      "Batch:250 0.6460841224193573  0.6393846383094788\n",
      "Batch:300 0.6460355154673258  0.6408333570758502\n",
      "Batch:350 0.6468912330695561  0.6384066168751036\n",
      "Batch:400 0.6467960438132286  0.6388461776077747\n",
      "Epoch:0 Loss:0.6463942212540889\n",
      "Epoch:0 Loss:0.6463942212540889 Accuracy:0.6399475645798733 Loss:0.6536961599272124 Accuracy:0.6306907619748797\n",
      "Batch:50 0.6493575870990753  0.6342307943105697\n",
      "Batch:100 0.647639335989952  0.6423077157139778\n",
      "Batch:150 0.6473417774836222  0.6419231015443801\n",
      "Batch:200 0.6454289036989213  0.6459615634381771\n",
      "Batch:250 0.6455069897174835  0.6428461775779725\n",
      "Batch:300 0.6450929262240728  0.6438461778561274\n",
      "Batch:350 0.6450396980558123  0.6428571672098977\n",
      "Batch:400 0.6445977027714253  0.6433173318207264\n",
      "Epoch:1 Loss:0.6451607778771645\n",
      "Epoch:1 Loss:0.6451607778771645 Accuracy:0.6413531010888741 Loss:0.6533705859768147 Accuracy:0.6204866730436986\n",
      "Batch:50 0.6460356366634369  0.6369230961799621\n",
      "Batch:100 0.6428401345014572  0.649230791926384\n",
      "Batch:150 0.6438935959339142  0.6476923302809398\n",
      "Batch:200 0.6434746387600899  0.6465384829044342\n",
      "Batch:250 0.6440235443115234  0.6431538677215576\n",
      "Batch:300 0.6443024426698685  0.641282072464625\n",
      "Batch:350 0.6440259044510978  0.6419780443395887\n",
      "Batch:400 0.6441598464548588  0.6411538685113192\n",
      "Epoch:2 Loss:0.6442483952634604\n",
      "Epoch:2 Loss:0.6442483952634604 Accuracy:0.6410578847206768 Loss:0.6518105317135247 Accuracy:0.6338304816460123\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(dltrain,dlvalid,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.device=device\n",
    "learner.model=learner.model.to(device)\n",
    "model.device=device\n",
    "learner.model.device=device\n",
    "learner.model.hidden=(learner.model.hidden[0].cuda(1),learner.model.hidden[1].cuda(1))\n",
    "model.hidden=(model.hidden[0].cuda(1),model.hidden[1].cuda(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_gpu_recursive(var, gpu_id):\n",
    "    for key in var:\n",
    "        if isinstance(var[key], dict):\n",
    "            var[key] = set_gpu_recursive(var[key], gpu_id)\n",
    "        else:\n",
    "            try:\n",
    "                var[key] = var[key].cuda(gpu_id)\n",
    "            except:\n",
    "                pass\n",
    "    return var\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.state = set_gpu_recursive(optimizer.state, 1)\n",
    "learner.optimizer.state = set_gpu_recursive(learner.optimizer.state, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:500 4.215455349445343  0.2790599019825459\n",
      "Batch:1000 4.210862180948258  0.2788794076293707\n",
      "Batch:1500 4.212145571072896  0.2783448839088281\n",
      "Batch:2000 4.211311246871948  0.278226111009717\n",
      "Batch:2500 4.210199825286865  0.27802792423963546\n",
      "Batch:3000 4.209871447563171  0.27792235642671587\n",
      "Batch:3500 4.2090058584213255  0.27796406232459203\n",
      "Batch:4000 4.207844992935658  0.2780102457590401\n",
      "Batch:4500 4.207080701986949  0.27803181937999194\n",
      "Batch:5000 4.206981196069718  0.2780740782529116\n",
      "Batch:5500 4.205610530679876  0.27823452775315805\n",
      "Epoch:0 Loss:4.20557110817913\n",
      "Batch:500 3.911064175128937  0.30575276267528534\n",
      "Epoch:0 Loss:4.20557110817913 Accuracy:0.27822952477167406 Loss:3.910363855164207 Accuracy:0.30540491747513915\n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(train_tokens,valid_tokens,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49.40244910553017, 49.89895197340787)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(3.9), np.exp(3.91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),f'{PATH}/inter/model_awd_lstm_state_dict_cuda_1')\n",
    "torch.save(optimizer.state_dict(),f'{PATH}/inter/learner_awd_lstm_state_dict_cuda_1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save (model,f'{PATH}/inter/model_awd_lstm_cuda_1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save (optimizer,f'{PATH}/inter/optimizer_awd_lstm_cuda_1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save (learner,f'{PATH}/inter/learner_awd_lstm_cuda_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning to predict RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
